created_utc,subreddit,author,domain,url,num_comments,score,ups,downs,title,selftext,saved,id,from_kind,gilded,from,stickied,retrieved_on,over_18,thumbnail,subreddit_id,hide_score,link_flair_css_class,author_flair_css_class,archived,is_self,from_id,permalink,name,author_flair_text,quarantine,link_flair_text,distinguished
1471742246,MachineLearning,dmgna21,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yscy3/ml_journalsconferences/,8,0,0,0,ML journals/conferences,"Hello,

I'm an Electrical Engineer and am recently been getting into Machine Learning. I just wanted to know which journals/conferences you people generally follow to keep up with the latest research happening in the field. I'm interested in natural language processing. 

Thanks! 

PS: I Google stuff and read whatever I find interesting, just wanted the opinion of experienced people. ",false,4yscy3,,0,,false,1473065473,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yscy3/ml_journalsconferences/,t3_4yscy3,,false,,
1470789774,MachineLearning,DadAtH_me,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wzisw/q3_2016_gpu_benchmarks_and_priceperformance/,3,1,1,0,Q3 2016 GPU Benchmarks and Price/Performance Metrics?,"Looking to upgrade my Nvidia card that I used for GPUy stuff and would like to get a card for TensorFlow.

Right now I'm just training on my 4770k so any improvement over that would be nice. TF requires Compute Capability &gt;3.0, which is Kepler on.

The [GeForce GTX 750 TI](http://www.videocardbenchmark.net/gpu.php?gpu=GeForce+GTX+750+Ti&amp;id=2815) is at the top of the 'Videocard Value (G3D Mark / $Price )' list however I don't know how well the G3D mark translates to CUDA performance.
",false,4wzisw,,0,,false,1473032493,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wzisw/q3_2016_gpu_benchmarks_and_priceperformance/,t3_4wzisw,,false,,
1471587887,MachineLearning,calclearner,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yhcx5/geometric_intelligencewhat_the_heck_is_it/,6,3,3,0,Geometric Intelligence--what the heck is it?,"Lately there's been a ton of hype surrounding a startup called Geometric Intelligence that claims to function on principles similar to those used by the human brain, allowing its software to gain intelligence and learn features from small amounts of examples. However, they have literally released 0 information as to how they accomplish this, if they can at all (not even a whitepaper). They have been making tons of claims with absolutely no evidence and not even a single concrete hint as to what algorithms they use. I'm not trying to insult them, but I'm very curious as to why they're so secretive. Does anyone know what's going on, or have any details as to what advantages they have over other companies?",false,4yhcx5,,0,,false,1473059863,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yhcx5/geometric_intelligencewhat_the_heck_is_it/,t3_4yhcx5,,false,,
1471932606,MachineLearning,cjmcmurtrie,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z52hd/does_anyone_have_any_strong_arguments_for_or/,9,3,3,0,Does anyone have any strong arguments for or against a Python or Scala stack for ML in a production environment?,"Although I confess my weakness for Python, I think I am pragmatic believing that betting against Python in the context of data science and machine learning is not a good idea. Generally betting against Python as the paramount ""scientific"" language in the next years would be silly. Reasons:

- It is backed by Google, Facebook and others
- It has low barriers to entry - making it easy for mathematicians, physicists, economists, quants... to join the community
- It is versatile - it can stand in as OO, functional or scripting language
- Its ecosystem is diversified - there is a huge long-tail of developers, not just a couple of big tech companies
- ... and of course, almost all of the most up-to-date machine learning and data analysis libraries are written with Python in mind

However, at my work, I am hearing an increasing amount of noise about switching to a Scala stack. This is mainly because of Apache Spark and its related libraries. To me, it looks a little boring for a machine learning engineer/researcher/romantic: out-of-box classifiers, machine learning as behemoth data jobs in the cloud, therefore an assumption of many-terabyte datasets (which denies the future of ML as standalone architecture). I am somewhat familiar with Scala, but see no way for the language to progress as the main scientific language in the community, because of these reasons and:

- Higher barriers to entry (stricter functional principles, less intuitive syntax, lots of vintage Java thinking/complication)
- A lack of backers in scientific/ML research
- Reliance on the JVM, which is far from universally native

Spark is probably unmatched just now as a tool for distributing jobs safely across clusters (thankfully you can still jam Python dependencies into Spark, although it's not straightforward). This is the only aspect of the market I see in Scala's favor.

Do any researchers or developers here have insights in this conversation?",false,4z52hd,,0,,false,1473071911,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z52hd/does_anyone_have_any_strong_arguments_for_or/,t3_4z52hd,,false,,
1470515987,MachineLearning,nlpkid,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4whnqg/has_anyone_tested_numenta_on_wellknown_benchmarks/,14,3,3,0,Has anyone tested Numenta on well-known benchmarks?,"Hello everyone! 

I've seen a lot of people denouncing Numenta/HTM, primarily because it fails to test its algorithms on well-known benchmarks. Since their algorithms went open source, has anyone tested them on benchmark tests like ImageNet recognition? Just curious if those algorithms truly work or not.

Thanks!",false,4whnqg,,0,,false,1473023368,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4whnqg/has_anyone_tested_numenta_on_wellknown_benchmarks/,t3_4whnqg,,false,,
1471369117,MachineLearning,tabinop,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y0r8d/what_if_you_want_to_penalize_a_type_of_error_more/,6,2,2,0,What if you want to penalize a type of error more ?,"Suppose you have a standard training sample set X-&gt;Y. But in addition to each sample you have a weight that tells you how much you care for that error (from slightly annoying to do not make that mistake again).

Would it be enough to bias the training sample set with more of the unacceptable errors examples to achieve that goal ? ",false,4y0r8d,,0,,false,1473051473,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y0r8d/what_if_you_want_to_penalize_a_type_of_error_more/,t3_4y0r8d,,false,,
1470365408,MachineLearning,UristMcBrogrammer,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w8be7/outside_of_graduate_school_what_is_a_good/,3,2,2,0,"Outside of graduate school, what is a good resource to learn reinforcement learning techniques such that your research can be understood and reproduced. Especially for those of us interested in doing minor independent research. EX. Stanford has many excellent open courses such as cs231n (CNNs) .",,false,4w8be7,,0,,false,1473018572,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w8be7/outside_of_graduate_school_what_is_a_good/,t3_4w8be7,,false,,
1472186870,MachineLearning,AmineHorseman,github.com,https://github.com/facebookresearch/deepmask,1,24,24,0,Official Torch implementation of DeepMask and SharpMask,,false,4zmw3s,,0,,false,1473081009,false,http://b.thumbs.redditmedia.com/rWCHQ2GHQ-SyZGcfmjs45ix1gvC3RrHXGdA4O3qt41E.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zmw3s/official_torch_implementation_of_deepmask_and/,t3_4zmw3s,,false,,
1472287160,MachineLearning,ookwrd,tokyohackathon.wordpress.com,https://tokyohackathon.wordpress.com/registration/,0,0,0,0,4 more days to register to the Conscious Machine Hackathon in Tokyo next Saturday!,,false,4ztfc5,,0,,false,1473084352,false,http://b.thumbs.redditmedia.com/jrNssaixo8bncGlOsgMGooFKqpsDGvk6sO0j_CJXuWI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ztfc5/4_more_days_to_register_to_the_conscious_machine/,t3_4ztfc5,,false,,
1470834841,MachineLearning,reworksophie,re-work.co,https://re-work.co/blog/deep-learning-jack-watts-nvidia-use-cases-for-business,0,1,1,0,"How Can Computers Learn, See &amp; Simulate Our World?",,false,4x21sy,,0,,false,1473033767,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x21sy/how_can_computers_learn_see_simulate_our_world/,t3_4x21sy,,false,,
1470850696,MachineLearning,gwulfs,github.com,https://github.com/scikit-optimize/scikit-optimize,1,3,3,0,Skopt 0.1 release,,false,4x3c0i,,0,,false,1473034430,false,http://b.thumbs.redditmedia.com/eRd3J5LyqM90kcM-01Y-pTCnoLbtWE-K_5mbkyutTbM.jpg,t5_2r3gv,false,two,,false,false,,/r/MachineLearning/comments/4x3c0i/skopt_01_release/,t3_4x3c0i,,false,News,
1472662010,MachineLearning,PM_ME_YOUR_GRADIENTS,ecmlpkdd2016.org,http://ecmlpkdd2016.org/program.html#Accepted,0,10,10,0,ECML Accepted Papers List,,false,50i5pz,,0,,false,1473097252,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50i5pz/ecml_accepted_papers_list/,t3_50i5pz,,false,,
1470078144,MachineLearning,pl47,cbonnett.github.io,http://cbonnett.github.io/Insight.html,2,7,7,0,Classifying e-commerce products based on images and text using Keras,,false,4vo5xp,,0,,false,1473008065,false,http://b.thumbs.redditmedia.com/IgNi9sowlPXN-Y8R07BA4JnXmwGCDeF3uC7WDlhK0qs.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vo5xp/classifying_ecommerce_products_based_on_images/,t3_4vo5xp,,false,,
1470439902,MachineLearning,llSourcell,youtube.com,https://www.youtube.com/watch?v=hBedCdzCoWM,0,0,0,0,Build a Self Driving Car in 5 Minutes,,false,4wdbb6,,0,,false,1473021134,false,http://a.thumbs.redditmedia.com/vp5sPSXopN8luGrUb4wCK4vdWNbPrIXMem0M0ZFXD50.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wdbb6/build_a_self_driving_car_in_5_minutes/,t3_4wdbb6,,false,,
1471229861,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xrl5s/regularizer_loss_function_to_constrain_a_variable/,5,0,0,0,Regularizer loss function to constrain a variable to be away from 0,[deleted],false,4xrl5s,,0,,false,1473046827,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xrl5s/regularizer_loss_function_to_constrain_a_variable/,t3_4xrl5s,,false,,
1470362740,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w8590/handling_weighted_samples_in_nns/,1,0,0,0,Handling weighted samples in NNs,[deleted],false,4w8590,,0,,false,1473018484,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w8590/handling_weighted_samples_in_nns/,t3_4w8590,,false,,
1470077448,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vo3kq/please_help_me_with_multilayer_perceptron_for/,2,0,0,0,"Please help me with Multi-layer Perceptron for solving ""AND"" in MATLAB",[deleted],false,4vo3kq,,0,,false,1473008031,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vo3kq/please_help_me_with_multilayer_perceptron_for/,t3_4vo3kq,,false,,
1471520249,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ybs1m/numerical_optimization/,0,1,1,0,Numerical Optimization,[removed],false,4ybs1m,,0,,false,1473057054,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ybs1m/numerical_optimization/,t3_4ybs1m,,false,,
1470778112,MachineLearning,darkconfidantislife,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wymng/intel_buys_nervana/,0,0,0,0,Intel buys nervana,[removed],false,4wymng,,0,,false,1473032039,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wymng/intel_buys_nervana/,t3_4wymng,,false,,
1471234535,MachineLearning,should_be_read_it,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xrw8h/ml_for_automatic_fault_diagnosis_of_soap_and_rest/,1,0,0,0,ML for automatic fault diagnosis of SOAP and REST service,"I want to use ML for automatically finding any issues in SOAP and REST services and remediating it without any human intervention. 
At first it will be a guided learning and then it can ML can evolve and do remediation based on the fault. The services in question are simple services which are used for booking hotel rooms etc. We have a Elastic search and Kibana for some of the systems but rest are on premises and use oracle ESBs.",false,4xrw8h,,0,,false,1473046981,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xrw8h/ml_for_automatic_fault_diagnosis_of_soap_and_rest/,t3_4xrw8h,,false,,
1471938503,MachineLearning,valikund,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5ctt/setting_up_tensorflow_with_gpu_support/,5,2,2,0,Setting up Tensorflow with GPU support,"Hi there,
I am a newbee with deeplearning frameworks, but recently I have been trying to set up my Nvidia GPU to work with tensorflow.
As far as I know it is impossible to do this on Windows right now.
I have installed ubuntu 16.04, then installed nvidia driver 370, and Cuda 7.5,  but I am not sure whether I need cuDNN ,and if I do, 4.0 or 5? After this if I would like to run it on docker, is nvidia-docker and the officail GPU tensorflow container the way to go? ",false,4z5ctt,,0,,false,1473072055,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z5ctt/setting_up_tensorflow_with_gpu_support/,t3_4z5ctt,,false,,
1472489355,MachineLearning,stevofolife,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/505ynm/how_to_get_started_with_knowledge_representation/,4,5,5,0,How to get started with Knowledge Representation?,"Any high signal, low noise books and papers to recommend for getting into KR?",false,505ynm,,0,,false,1473090920,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/505ynm/how_to_get_started_with_knowledge_representation/,t3_505ynm,,false,,
1470415157,MachineLearning,devl82,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wbajz/question_for_the_dl_aficionados/,8,1,1,0,Question for the DL aficionados,"Except (color) image (patch based) classification tasks where DL is probably the best choice given a crunchy gpu and lots of data / what about:

* image data from from non-rgb modalities

* pixel level classification 

* smallish datasets (up to some thousands per class)

* abundance of non labeled data

While it is easy to find a million cats for DL, are there architectural choices for NN that tackle these kind of datasets better than 'traditional' ML (forests, svms and the likes)?

note: the points above are a very common story for biomedical imaging... Typically you have images from sources other than your common 3-dimensional rgb (e.g. mri, spectral) providing a lot of pixels but with no/little information available. Also obtaining a proper training set is difficult as setting a bunch of different classes you need the consensus of a team of experts. I have seen some work trying to adapt pretrained data (which requires segmentation, also not always easy) but i am more versed on the not-so-cool-methods and would like to hear your expert opinions",false,4wbajz,,0,,false,1473020094,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wbajz/question_for_the_dl_aficionados/,t3_4wbajz,,false,,
1472236501,MachineLearning,MagnesiumCarbonate,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zq4er/project_structuring_best_practices_for_sharing/,4,1,1,0,Project Structuring best practices for sharing code and reproducible research?,"I found [this presentation](ftp://ftp.dcs.shef.ac.uk/home/neil/reproducible_berlin10.pdf) by Neil Lawrence of InverseProbability.com on the practices they use when packaging software to be released alongside research papers. I think this is a interesting subject, and I wanted to start a discussion here.

As far as I can tell the take-aways from this presentation are:

* Subdivide your software into independent toolboxes.

* Create a new directory for each project you work on with fixed subdirectories, e.g. `html`, `tex`, `python`, `matlab`. Each subdirectory has the related parts of your project.

* Each project uses a fixed version of the toolboxes, e.g. by downloading a certain version from your repositories. This is so other researchers can reproduce your research exactly. But you also provide the ability to use the most up-to-date toolboxes.

Personally, I have a little bit of downtime which I'd like to use to free myself from matlab (moving to python+numpy+scipy), as well as to organize my code libraries sensibly.

What best practices do you use for organizing a codebase or a library? How do you make your research reproducible? The presentation I found is from 2010, are you aware of more recent resources on this topic?",false,4zq4er,,0,,false,1473082660,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zq4er/project_structuring_best_practices_for_sharing/,t3_4zq4er,,false,,
1471030429,MachineLearning,rulerofthehell,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xfdio/how_to_classify_have_a_sequencetolabel_modelling/,5,0,0,0,How to classify have a sequence-to-label modelling instead of sequence-to-sequence modelling in LSTM? (time-series classification),"For example, if I want to classify a given time series into some categories?

 How do I model a network for such task? 

Are there any good research paper/GitHub explaining it?

Also, are there any better ways (instead of using LSTM) to do so?",false,4xfdio,,0,,false,1473040606,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xfdio/how_to_classify_have_a_sequencetolabel_modelling/,t3_4xfdio,,false,,
1471573264,MachineLearning,abhishkk65,arxiv.org,https://arxiv.org/abs/1608.05343,44,55,55,0,[1608.05343] Decoupled Neural Interfaces using Synthetic Gradients,,false,4ygccq,,0,,false,1473059352,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4ygccq/160805343_decoupled_neural_interfaces_using/,t3_4ygccq,,false,Research,
1471494725,MachineLearning,crazyappl3,techcrunch.com,https://techcrunch.com/2016/08/17/courseras-co-founder-daphne-koller-set-to-start-anew-at-calico/,0,0,0,0,Daphne Koller brings even more machine learning kung fu to the Google universe,,false,4yab79,,0,,false,1473056302,false,http://b.thumbs.redditmedia.com/QZKM5Ai6uKss0ZUqqgAB0oVyZ-OEwfIVKZi5NHVn_UY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yab79/daphne_koller_brings_even_more_machine_learning/,t3_4yab79,,false,,
1472027088,MachineLearning,uwtech,nlpbots.com,http://www.nlpbots.com/,0,1,1,0,INTELLIGENT INTERACTIONS FOR PRODUCT AND SERVICE.,,false,4zbj3w,,0,,false,1473075215,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zbj3w/intelligent_interactions_for_product_and_service/,t3_4zbj3w,,false,,
1470914055,MachineLearning,metacurse,people.idsia.ch,http://people.idsia.ch/~juergen/nnpioneeraward.html,29,74,74,0,Juergen Schmidhuber's Acceptance Speech: IEEE Neural Networks Pioneer Award,,false,4x7hfa,,0,,false,1473036545,false,http://b.thumbs.redditmedia.com/585DT3LrPHetIRcNxzHofetYVTuWm6NgWNKQQznZThU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x7hfa/juergen_schmidhubers_acceptance_speech_ieee/,t3_4x7hfa,,false,,
1472490238,MachineLearning,alxndrkalinin,tensorlayer.org,http://tensorlayer.org,4,47,47,0,TensorLayer: Deep learning and Reinforcement learning library for Researchers and Engineers,,false,5061hr,,0,,false,1473090962,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/5061hr/tensorlayer_deep_learning_and_reinforcement/,t3_5061hr,,false,,
1471823434,MachineLearning,pemami,pemami4911.github.io,http://pemami4911.github.io/blog_posts/2016/08/21/ddpg-rl.html,0,61,61,0,Deep Deterministic Policy Gradients in Tensorflow,,false,4yxtqe,,0,,false,1473068238,false,http://b.thumbs.redditmedia.com/ND4HqmJuBAJnQsGM-AjiAZgxkj63DZQiHX8RcjXbsxQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yxtqe/deep_deterministic_policy_gradients_in_tensorflow/,t3_4yxtqe,,false,,
1472323877,MachineLearning,raverbashing,medium.com,https://medium.com/@dsracoon/poetic-edda-meets-word2vec-18c08eca27dc#.ohvmqenca,0,1,1,0,Poetic Edda meets Word2Vec,,false,4zvm7w,,0,,false,1473085488,false,http://b.thumbs.redditmedia.com/UPv5ZoT3urTEEIKugCfnzj-kI6gl0U1IFV220JcWcRw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zvm7w/poetic_edda_meets_word2vec/,t3_4zvm7w,,false,,
1470093102,MachineLearning,amplifier_khan,kdnuggets.com,http://www.kdnuggets.com/2016/07/bayesian-machine-learning-explained.html,3,113,113,0,"Bayesian Machine Learning, Explained",,false,4vpdym,,0,,false,1473008706,false,http://b.thumbs.redditmedia.com/UkSR_Qdk_FnRIzDQKPbrryZnO3luqWKSTUMaeYDLt6Q.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vpdym/bayesian_machine_learning_explained/,t3_4vpdym,,false,,
1470702846,MachineLearning,gmo517,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wth6w/char_rnn_architecture_questions/,7,0,0,0,char rnn architecture questions,"Hello everyone,

I'm trying to implement char-level rnn in tensorflow but I had a few conceptual questions. From the way I understand it, seems like most people like to have num_batches per epoch, and each batch has say 50 sequences and each sequence can have 50 tokens (chars). Now we feed in all the first tokens for all the sequences in the first batch at once into the first input of the RNN. This goes through the hidden layers and we get our output char distribution. 

My question is, lets say we have two hidden layers and each hidden layer has 128 hidden units. If our sequences only has 50 tokens, what happens after the 50th token. My next question is for making predictions with our trained model. If we want to predict a 200 token long sequence, but we only have 128 hidden units, how does this work?

sincerely,
dl-noob",false,4wth6w,,0,,false,1473029415,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wth6w/char_rnn_architecture_questions/,t3_4wth6w,,false,,
1471335906,MachineLearning,buy_some_wow,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xygux/harvard_cs_109_data_science_videos/,6,38,38,0,Harvard CS 109 - Data Science videos,"Does anyone have a link to the videos of CS109 course (any offering from 2013 to 2015)? I guess they've taken them down from the respective course pages. (eg. http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml)

**EDIT**: I've done some research. Here are the findings.

* The videos for 2013 and 2014 are no longer hosted. **[1]**
* As per **[1]**, only the HD videos for 2015 offering are available. They can be found in **[2]**
* Prof.
Joe Blitzstein's answer on Quora **[3]** about the availability of 2015 problem sets for public states that they are not released to the public. (I don't have enough information to comment on the availability of the problem sets as I haven't gone through any of the videos as of yet. The class material page **[4]** seems to have the working links to notebooks followed in labs though). He suggests to watch 2015 videos and do 2013 assignments **[5]** - I guess it would be tedious to align these two though.

* IMO the most resourceful version would be 2013, given that it has all the assignments + solutions available **[5]**, it would be great if we could find the videos for this offering. 
A while ago, /u/Chrispy645 has posted a script **[6]** to download all the 2013 material which are now no longer hosted. Maybe someone that already downloaded them can upload them to Youtube?


---

**[1]** http://stackoverflow.com/a/38924479/5864582

**[2]** https://matterhorn.dce.harvard.edu/engage/ui/index.html#/2016/01/14328

**[3]** https://www.quora.com/How-do-I-access-the-problem-sets-from-2015-Harvard-CS109/answer/Joe-Blitzstein?srid=sy8D

**[4]** http://cs109.github.io/2015/pages/videos.html

**[5]** https://github.com/cs109/content/

**[6]** https://www.reddit.com/r/MachineLearning/comments/2h2x3e/script_to_download_cs109_data_science_lecture/",false,4xygux,,0,,false,1473050315,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xygux/harvard_cs_109_data_science_videos/,t3_4xygux,,false,,
1470388519,MachineLearning,haffi112,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w9k9u/random_content_discovery/,3,1,1,0,Random content discovery,"I wanted to ask if you know of any services or products which are machine learning based and allow its users to perform random content discovery?

Let me explain better what I mean, let us stick to reddit to begin with as a source of content but of course the source can be the whole web in general. My problem is the following, when I browse reddit I'm subscribed to a set of subreddits which essentially limits what content is available to me. If there was a post outside what I subscribe which would be of interest to me it is quite unlikely that I will discover it.

I would love to have a feature which would discover such posts for me and I would even be ok with a somewhat high false positive rate in the beginning when it is learning. A UI where I'm presented with posts and I can swipe right/left (up/down vote) after studying them would be nice for example.

I see two approaches to this problem. First, collaborative filtering where the content a user sees is based on the votes of other users. This approach has a bootstrap problem though since new posts need to be evaluated by someone before they can be recommended. Second, a deep learning approach where a network is trained on my preferences. However, here the problem is that it is probably too expensive to have a neural network for every user on the server-side (perhaps many users could be aggregated into a single network?). In my case I would be happy to do that learning on the client side though if the outcome is more efficient content discovery.

So my question is, does there anything like this exist and what are the main obstacles if I wanted to do it on the client side for example?",false,4w9k9u,,0,,false,1473019209,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w9k9u/random_content_discovery/,t3_4w9k9u,,false,,
1471292438,MachineLearning,nowimtrulyfree,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xvlo4/help_how_to_generate_labels_for_unsupervised/,3,1,1,0,[Help] How to generate labels for unsupervised classification?,"I want to classify a collection of documents, but I would also very much like it to be able to generate names for its categories on its own. Thanks.",false,4xvlo4,,0,,false,1473048858,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xvlo4/help_how_to_generate_labels_for_unsupervised/,t3_4xvlo4,,false,,
1470338378,MachineLearning,thinkdip,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w69ki/going_into_a_machine_learning_research_career/,5,0,0,0,Going into a machine learning research career,"I am currently going into my senior year of high school. For a little over a year now, I've made learning and studying machine learning my past time passion. I've taken a multitude of courses on statistics, (un)supervised/reinforcement learning, linear algebra, calculus, etc. I know a year is a very small amount of time to gain insight regarding a field, but at this point in time I believe machine learning research is the path I want to follow when it comes to a career. With that said, I have a number of things to consider going into the future, and if anyone can help me clear up a few things I would greatly appreciate it.

Is there any way I can utilize this machine learning knowledge while obtaining an undergraduate degree? From what I've seen, if you want to do anything with machine learning, it ALL comes after an initial computer science/math undergrad degree, and then you can start specializing once you get your masters. How true is that statement? I would love to get involved with ML long before a masters.

More specifically, what kind of research opportunities would I be able to take on (in machine learning) during an undergrad degree given I have no legitimate justification? Is research just something that has to wait until I actually have a degree, or can I do research on my own despite me not being taught it in school?

Thanks guys. My apologies for sloppy questions.",false,4w69ki,,0,,false,1473017523,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w69ki/going_into_a_machine_learning_research_career/,t3_4w69ki,,false,,
1470158175,MachineLearning,FutureIsMine,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vtl3h/customer_to_vector_question/,3,3,3,0,Customer to Vector question,"Having recently been working on the Stanford CS224 problem sets, Im looking into using similar deep learning techniques for taking customer descriptions and turning them into vectors. Looking into something like the skip-gram along with other word2Vector models, I'm wondering how well do those models work by taking the customers as the input vectors and using the words in their descriptions as the output vectors. Im also interested in seeing if there are any great papers on the topic of creating vector representations of other real world attributes using deep learning. ",false,4vtl3h,,0,,false,1473010907,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vtl3h/customer_to_vector_question/,t3_4vtl3h,,false,,
1470251753,MachineLearning,sanity,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w09og/need_a_recommendation_for_a_selfcontained_java/,5,1,1,0,Need a recommendation for a self-contained Java text-classification library,"I have a problem where I have several thousand example documents each of which is classified into one of around 600 classes.  I would like to use these to train a text classification algorithm.

I'm familiar with the Naive Bayes implementation in Mahout, but it's dependency on Hadoop is a negative, I really just want a library that I can run in a single JVM - rather than something that pulls in a large dependency like Mahout.

Any recommendations?",false,4w09og,,0,,false,1473014425,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w09og/need_a_recommendation_for_a_selfcontained_java/,t3_4w09og,,false,,
1472327278,MachineLearning,ricocotam,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zvvf0/ideas_for_rnn_learning/,0,0,0,0,Ideas for RNN learning,"Hey everyone, I'm a beginner with neural networks and I'd like to play with recurrent ones but I can't find ideas to do. 

I'd like ideas to train on RNN 1 to 1, Many to 1 and that's it because I got stuff to do with Many to Many and 1 to Many :)


Thanks for te help :D",false,4zvvf0,,0,,false,1473085618,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zvvf0/ideas_for_rnn_learning/,t3_4zvvf0,,false,,
1472462218,MachineLearning,hexmap,mediapost.com,http://www.mediapost.com/publications/article/282267/are-machine-learning-search-algorithms-to-blame-fo.html,0,0,0,0,Are Machine Learning Search Algorithms To Blame For Stereotypes?,,false,50458q,,0,,false,1473089944,false,http://b.thumbs.redditmedia.com/bLezOasrgoVcp-xTNL3IbFdsZhNwKsLuQiG1FoZuB9o.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50458q/are_machine_learning_search_algorithms_to_blame/,t3_50458q,,false,,
1471593761,MachineLearning,vvpreetham,medium.com,https://medium.com/autonomous-agents/part-1-error-analysis-how-not-to-kill-your-puppy-with-neuralnetwork-66a766b6b406#.bxir2o95v,0,2,2,0,Part1:Error Analysis - How not to kill your puppy with #NeuralNetwork - Foundations of Error Score,,false,4yhois,,0,,false,1473060028,false,http://b.thumbs.redditmedia.com/ge1-PNvO2HPVRXp74-gEqZCofeiN6Ksaf9uWDvNFirY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yhois/part1error_analysis_how_not_to_kill_your_puppy/,t3_4yhois,,false,,
1471352396,MachineLearning,cryptohacker,github.com,https://github.com/multunus/autonomous-rc-car,2,11,11,0,We built an Autonomous RC Car using Artificial Neural Networks!,,false,4xzcyn,,0,,false,1473050765,false,http://a.thumbs.redditmedia.com/wb826am4SEPidZZdCgvI5yavuQWx82Ri09BmC_ZJBu8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xzcyn/we_built_an_autonomous_rc_car_using_artificial/,t3_4xzcyn,,false,,
1472490559,MachineLearning,Tamazy,arxiv.org,https://arxiv.org/abs/1607.01097,6,22,22,0,[1607.01097] AdaNet: Adaptive Structural Learning of Artificial Neural Networks,,false,5062jj,,0,,false,1473090978,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/5062jj/160701097_adanet_adaptive_structural_learning_of/,t3_5062jj,,false,,
1472412711,MachineLearning,compsens,highnoongmt.wordpress.com,https://highnoongmt.wordpress.com/2016/08/28/millennial-whoop-with-derp-learning/,0,11,11,0,Millennial Whoop with Derp Learning,,false,500z9w,,0,,false,1473088279,false,http://b.thumbs.redditmedia.com/kiwJtJGQMWtCc5dOw-5y7hBOaktOzdCTlvSmA6ikynM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/500z9w/millennial_whoop_with_derp_learning/,t3_500z9w,,false,,
1470424206,MachineLearning,amplifier_khan,blog.kaggle.com,http://blog.kaggle.com/2016/08/02/facebook-v-predicting-check-ins-winners-interview-2nd-place-markus-kliegl/,0,2,2,0,Facebook V: Predicting Check Ins,,false,4wc2ms,,0,,false,1473020494,false,http://b.thumbs.redditmedia.com/oleGUx6DRZllB-s8xp2JnEO0zdGBJh0Iy3ByqEevPoE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wc2ms/facebook_v_predicting_check_ins/,t3_4wc2ms,,false,,
1471641971,MachineLearning,jacobgil,jacobcv.blogspot.com,http://jacobcv.blogspot.com/2016/08/class-activation-maps-in-keras.html,1,2,2,0,Class activation maps in Keras,,false,4yle88,,0,,false,1473061917,false,http://a.thumbs.redditmedia.com/jki3GcgwNVHjGHoqUe1lMOv-6Dbzaz0QgKwpJXFUIe0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yle88/class_activation_maps_in_keras/,t3_4yle88,,false,,
1470261414,MachineLearning,kemaswill,github.com,https://github.com/kemaswill/fasttext_torch,2,0,0,0,"An implementation of Joulin's FASTTEXT text classification model using Torch, achieve accuracy of 90% on the sentiment analysis task.",,false,4w14ev,,0,,false,1473014874,false,http://b.thumbs.redditmedia.com/6KGpin_PvvfZUh17D7TxIrG7SEwk2WqJsT_khVV6MXc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w14ev/an_implementation_of_joulins_fasttext_text/,t3_4w14ev,,false,,
1471060089,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xhhhq/how_to_learn_new_features_from_biological/,0,1,1,0,How to learn new features from biological sequences with a small dataset?,[removed],false,4xhhhq,,0,,false,1473041677,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xhhhq/how_to_learn_new_features_from_biological/,t3_4xhhhq,,false,,
1471341604,MachineLearning,PAC_2016,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xyqhw/predictive_analytics_in_mental_health_competition/,0,1,1,0,Predictive Analytics in Mental Health Competition (PAC) 2016,[removed],false,4xyqhw,,0,,false,1473050449,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xyqhw/predictive_analytics_in_mental_health_competition/,t3_4xyqhw,,false,,
1471834810,MachineLearning,strunberg,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yykif/at_what_point_would_we_factor_in_concept_of_ai/,5,0,0,0,At what point would we factor in concept of AI becoming addicted to activities?,"There are many who have an Gambling addiction problem.
At what point would we start seriously debating the ethics of having AI gamble for instance ?",false,4yykif,,0,,false,1473068613,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yykif/at_what_point_would_we_factor_in_concept_of_ai/,t3_4yykif,,false,,
1471791486,MachineLearning,loopnn,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yv8bo/how_to_go_from_the_optimization_perspective_of/,25,24,24,0,How to go from the Optimization perspective of Deep Learning to the probabilistic perspective?,"Hi,

I think this is a common problem for many beginners. How do you get from ConvNets, SGD, L2 regularization to Variational Inference, Bayesian Neural Networks and things like that?
",false,4yv8bo,,0,,false,1473066930,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yv8bo/how_to_go_from_the_optimization_perspective_of/,t3_4yv8bo,,false,,
1471945473,MachineLearning,internet_ham,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5oa4/is_google_patenting_dqn_really_justified/,30,20,20,0,Is Google patenting DQN really justified?,"'Don't be evil'
DQN was a great achievement for DeepMind, but I feel with since it's just the integration of existing technologies (CNNs, Q Learning, backprop, etc) 'owning' the concept is a bit of a stretch. 

Is this the start of something detrimental to the AI sector or just a way of Google keeping it away from bad people (weapons, etc)?",false,4z5oa4,,0,,false,1473072230,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4z5oa4/is_google_patenting_dqn_really_justified/,t3_4z5oa4,,false,Discusssion,
1471354844,MachineLearning,Icko_,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xzjck/simplest_example_that_svms_cant_handle_but_neural/,44,42,42,0,"Simplest example that SVMs can't handle, but neural nets can?","Me and a friend had an argument - he said that neural nets are overhyped right now (which is probably right) and that you can model everything NNs can by using the right kernel transformation. I had read [Learning Deep Architectures for AI](https://indico.cern.ch/event/439520/contributions/1941519/attachments/1147353/1645461/Learning_Deep_Architectures_for_AI_-_Bengio.pdf), which claimed that (1) deep architectures can model exponential (with depth) number of regions, and (2) SVMs and regression are depth 2 and 1.
&gt; Depth of architecture refers to the number of levels of composition of non-linear operations in the function
learned. Whereas most current learning algorithms correspond to shallow architectures(1, 2 or 3 levels),
the mammal brain is organized in a deep architecture (Serre, Kreiman, Kouh, Cadieu, Knoblich, &amp; Poggio,
2007) with a given input percept represented at multiple levels of abstraction, each level corresponding to
a different area of cortex. Humans often describe such concepts in hierarchical ways, with multiple levels
of abstraction. The brain also appears to process information through multiple stages of transformation and
representation. This is particularly clear in the primate visual system (Serre et al., 2007), with its sequence
of processing stages: detection of edges, primitive shapes, and moving up to gradually more complex visual
shapes.

[...]
&gt; Section 2 reviews theoretical results (which can be skipped without hurting the understanding of the remainder)
showing that an architecture with insufficient depth can require many more computational elements,
potentially exponentially more (with respect to input size), than architectures whose depth is matched to the
task. We claim that insufficient depth can be detrimental for learning.

[...]
&gt; More precisely, functions that can be compactly represented by a depth k architecture might require an
exponential number of computational elements to be represented by a depth k − 1 architecture. Since the
number of computational elements one can afford depends on the number of training examples available to
tune or select them, the consequences are not just computational but also statistical: poor generalization may
be expected when using an insufficiently deep architecture for representing some functions.

[...]
   
&gt; • If we include affine operations and their possible composition with sigmoids in the set of computational
elements, linear regression and logistic regression have depth 1, i.e., have a single level.

&gt;• When we put a fixed kernel computation K(u, v) in the set of allowed operations, along with affine
operations, kernel machines (Sch¨olkopf, Burges, &amp; Smola, 1999a) with a fixed kernel can be considered
to have two levels. The first level has one element computing K(x, xi) for each prototype xi (a
selected representative training example) and matches the input vector x with the prototypes xi
. The
second level performs an affine combination b +
P
i αiK(x, xi) to associate the matching prototypes
xi with the expected response.

&gt;• When we put artificial neurons (affine transformation followed by a non-linearity) in our set of elements,
we obtain ordinary multi-layer neural networks (Rumelhart et al., 1986b). With the most
common choice of one hidden layer, they also have depth two (the hidden layer and the output layer).

&gt;• Decision trees can also be seen as having two levels, as discussed in Section 3.1.

&gt;• Boosting (Freund &amp; Schapire, 1996) usually adds one level to its base learners: that level computes a
vote or linear combination of the outputs of the base learners.

&gt;• Stacking (Wolpert, 1992) is another meta-learning algorithm that adds one level.

&gt;• Based on current knowledge of brain anatomy (Serre et al., 2007), it appears that the cortex can be
seen as a deep architecture, with 5 to 10 levels just for the visual system.


 I'm looking for the simplest counterexample that SVMs/gaussian processes can't model - in the same way logistic regression can't model XOR. I haven't worked with kernel methods much, so I can't figure out one myself. Also, what does ""kernel machines with a fixed kernel"" mean?",false,4xzjck,,0,,false,1473050855,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xzjck/simplest_example_that_svms_cant_handle_but_neural/,t3_4xzjck,,false,,
1471533402,MachineLearning,n00bto1337,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yctgj/training_a_network_for_skin_segmentation/,0,1,1,0,Training a network for skin segmentation.,"I need to detect skin areas in an image, however whatever methods I found online, whether using OpenCV or ML based seem to be not so efficient. Most methods confuse cream colored clothes as skin. I was thinking of training a model using Keras to solve this, however, how do I prepare my data set? If I have an image of a person showing skin, wearing a cream colored clothing, how do I convert that into training data? 

Also, what sort of architecture would be suitable for this?",false,4yctgj,,0,,false,1473057576,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yctgj/training_a_network_for_skin_segmentation/,t3_4yctgj,,false,,
1472491456,MachineLearning,bluelite,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/5065dv/wanted_quick_deep_learning_wow_demo/,11,0,0,0,"Wanted: quick deep learning ""wow"" demo","I'm teaching a computer science survey course. With deep learning being one of the current hot topics, I'm looking for a quick demo I can show my students that will give them an idea of what deep learning can do and that they can relate to. It should be doable in 10-15 minutes and, ideally, be one they can experiment with themselves in a browser.

I've got some ideas, but I don't know how to find appropriate data sets.

* Detect faces in thousands of photos and make connections between the pictures. Eg, ""face 123 is found in photos 4567, 8312, and 1549.""
* Speech recognition, such as Siri and Alexa.

I have a Linux box with a GTX 980 to use.

Can you suggest some demos, or links to ""canned"" ones?

Thanks!",false,5065dv,,0,,false,1473091025,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/5065dv/wanted_quick_deep_learning_wow_demo/,t3_5065dv,,false,,
1471566255,MachineLearning,pierrelux,fastcompany.com,http://www.fastcompany.com/3062932/mind-and-machine/ai-is-a-male-dominated-field-but-an-important-group-of-women-is-changing-th,2,0,0,0,The Women Changing The Face Of AI,,false,4yfshs,,0,,false,1473059075,false,http://a.thumbs.redditmedia.com/xwkwpfupMi2849nEFFiMO0mOtm-w5ooY5nDs_2JJLP4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yfshs/the_women_changing_the_face_of_ai/,t3_4yfshs,,false,,
1470281247,MachineLearning,houseofnanking,gab41.lab41.org,https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa#.rlgtrreth,0,0,0,0,The 10 Algorithms Machine Learning Engineers Need to Know,,false,4w2k2o,,0,,false,1473015618,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w2k2o/the_10_algorithms_machine_learning_engineers_need/,t3_4w2k2o,,false,,
1471926964,MachineLearning,lvbu,redd.it,https://redd.it/4z0krx,0,0,0,0,Mini-PC GPU for ML,,false,4z4r3r,,0,,false,1473071754,false,http://b.thumbs.redditmedia.com/qpGEtgwcmiRSuIGH5DR3lCHcVYGYlDe0sazHoTILyBM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z4r3r/minipc_gpu_for_ml/,t3_4z4r3r,,false,,
1470316473,MachineLearning,datasciguy-aaay,openbenchmarking.org,https://openbenchmarking.org/test/pts/caffe,1,0,0,0,OpenBenchmarking.org - Caffe AlexNet Test Profile,,false,4w4fuw,,0,,false,1473016581,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w4fuw/openbenchmarkingorg_caffe_alexnet_test_profile/,t3_4w4fuw,,false,,
1470752869,MachineLearning,bdamos,bamos.github.io,http://bamos.github.io/2016/08/09/deep-completion/,29,322,322,0,Image Completion with Deep Learning in TensorFlow [OC],,false,4wwfs8,,0,,false,1473030924,false,http://b.thumbs.redditmedia.com/CRjbTYZ8sWPhJXyKkvdkR8x6p4LZ0zNVkrUXTX_bZWc.jpg,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wwfs8/image_completion_with_deep_learning_in_tensorflow/,t3_4wwfs8,,false,Research,
1472227016,MachineLearning,elisebreda,blog.yhat.com,http://blog.yhat.com/posts/rodeo-2.1.3.html,0,1,1,0,Rodeo v2.1.3 - We fixed Rodeo installation &amp; Rodeo for Windows,,false,4zpb4f,,0,,false,1473082243,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zpb4f/rodeo_v213_we_fixed_rodeo_installation_rodeo_for/,t3_4zpb4f,,false,,
1471033220,MachineLearning,c0cky_,medium.com,https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82#.micrgpff7,1,15,15,0,Recurrent Neural Networks for Beginners,,false,4xfm99,,0,,false,1473040730,false,http://b.thumbs.redditmedia.com/QLZ8aQMNOPE19bK3VkoJJmbVpgcCf29GiEQPeW3p0TQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xfm99/recurrent_neural_networks_for_beginners/,t3_4xfm99,,false,,
1472075489,MachineLearning,theweg88,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zf1hj/could_you_teach_a_machine_to_play_a_complex_game/,0,1,1,0,Could you teach a Machine to play a complex game like COD?,[removed],false,4zf1hj,,0,,false,1473076992,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zf1hj/could_you_teach_a_machine_to_play_a_complex_game/,t3_4zf1hj,,false,,
1471590397,MachineLearning,FFiJJ,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yhi49/algorithm_for_predicting_above_the_value/,5,0,0,0,"Algorithm for predicting ""above"" the value","Hello there guys,

I was wondering if there is any approachable paper/theory/book/algorithm that deals with regressions of n parameter vectors (or really any ML algorithm that can predict continuous quantities) which deals with the subject of predicting values while striving to be always above or equal with the highest possible values.

Obviously one approach would be implementing a regression (say ridge regression) on only the highest data-point in report to the parameter(s) that influence them (e.g. if for a parameter X there are 3 different values of Y, only take the highest Y), but this seems like quite a brute force approach and also a bit harder to apply when dealing with N parameter vectors instead of one.

(I'm am not very knowledgeable when it comes to the fields of mathematics so excuse me if some of the terms I use are misappropriate or outright silly)",false,4yhi49,,0,,false,1473059939,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yhi49/algorithm_for_predicting_above_the_value/,t3_4yhi49,,false,,
1470389187,MachineLearning,MusicidalOtaku,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w9lbr/a_question_about_unsupervised_pretraining_of_deep/,2,0,0,0,A question about unsupervised pretraining (of deep NNs) with denoising autoencoders,"I've been training a deep network on signals from a commercial brainwave sensor for a project and I've been using denoising auto-encoders (standard neural network, trained layer by layer by feeding representations from the previous one into the next, no RBM's or anything) as a pretraining mechanism. In analyzing the data I got from the training I performed I realized something that I find kind of weird: the reconstruction accuracy of the autoencoder layers and the final classification accuracy don't really seem to be positively correlated. The first layer's is, slightly, the rest aren't or are even negatively correlated. That would seem to suggest that improving the autoencoder's performance would have no impact or even a negative impact on the classification performance. Is this normal or does it indicate that I've done something wrong? For the last layer I can kind of understand why it would be negatively correlated: as the reconstruction error of the previous layers is already bad increasing its performance would, due to the way I'm training them, only cause it to get good at reconstructing rubbish, which isn't useful... 
I guess the reasoning behind why this doesn't really matter is because the weights of the autoencoder layers are just a starting point that turns out to be a good one in most cases but wouldn't having a better starting point usually tend to improve the final outcome?
I've tried to look into ways of ""solving"" this but I can't seem to find anyone that's really done research into it. I thought that discounting representations based on their reconstruction error might work, so that parts of the training set that are hard to reconstruct and are likely noise contribute less to the training of the deeper layers, but I have no idea whether that would actually work.
Does anyone here have an idea of why this could be and if it's a general ""issue"" with the method that someone has already tried to improve upon? Thanks in advance.",false,4w9lbr,,0,,false,1473019224,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w9lbr/a_question_about_unsupervised_pretraining_of_deep/,t3_4w9lbr,,false,,
1471093517,MachineLearning,godspeed_china,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xizk7/anyone_notice_this_confliction_in_ai/,4,0,0,0,anyone notice this confliction in AI?,"For general object model, we need a triplet: object-attribute-value to store information. This is demostrated by the popular object oriented programming and relational database. 
On the other hand, neural network/human brain is matrix based. Only pairwise synapse is available.  
The confliction is how the represent a triplet with a numerical matrix?  
For example, the apple is red can be represented by a numberical matrix m(apple, red)=1  
However, tom is john's father can not be represented by a matrix/synapse.  
Additional hint is that in a brain damaged person case, he can not distinguish between ""A is B's wife"" with ""B is A's wife"". It seem that triplet representation requires high level brain function rather than low level synapse matrix.  

",false,4xizk7,,0,,false,1473042444,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xizk7/anyone_notice_this_confliction_in_ai/,t3_4xizk7,,false,,
1471956217,MachineLearning,mln00b13,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z69fc/word_embeddings_trained_on_amazon_product/,6,0,0,0,Word embeddings trained on amazon product descriptions?,Is there any pre trained model or links or anything available for word embeddings training on amazon's product descriptions data set or any such similar set?,false,4z69fc,,0,,false,1473072526,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z69fc/word_embeddings_trained_on_amazon_product/,t3_4z69fc,,false,,
1471017151,MachineLearning,jcatw,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xe754/nips_2016_notification_station/,38,23,23,0,NIPS 2016 Notification Station,"The NIPS decisions are in!  Was your paper accepted?  Rejected?   Did reviewer #4 even bother to read the damn thing? Let us know!
 ",false,4xe754,,0,,false,1473040003,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xe754/nips_2016_notification_station/,t3_4xe754,,false,,
1472158737,MachineLearning,jstaker7,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zkto7/if_model_is_underfitting_where_to_start_adding/,3,0,0,0,If model is underfitting where to start adding parameters?,"I'm trying an image caption problem similar to https://arxiv.org/pdf/1411.4555.pdf. Due to limited hardware I'm starting with a small network, which unsurprisingly underfits. So now how do I know where the information flow bottleneck might be? For example, maybe I'm not picking up enough low-level features in the images, or maybe my initial state to the RNN isn't large enough to hold sufficient information about the text. 

Let's say an exhaustive parameter search like grid search or even fancy bayesian search methods are not an option. Is there any kind of intuition I can apply that can help with the design?

One idea I have is to try sizing the convolutional portion separately with something like an auto encoder. Then when satisfied with how well it trains, I could use those hyperparamers as a starting point for the full end-to-end model. Would that work or mostly a waste of time?",false,4zkto7,,0,,false,1473079947,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zkto7/if_model_is_underfitting_where_to_start_adding/,t3_4zkto7,,false,,
1471984456,MachineLearning,TheNASAguy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z8m1s/using_ml_to_construct_a_dynamic_structure_to/,14,0,0,0,Using ML to construct a Dynamic structure to derive and reverse derive equations.,"I'm really new to Machine Learning but i've setup Tensor Flow on WLS and have gone over the basic stuff, I'm trying to Construct a 8 to 10 Dimensional Hypercube like a Octaract or a Decaract, such that each individual point on the structure represents a set of data or a synapse or a node in a neural network and then use ML to supervise learning to Derive complex mathematical equations relating to Physics and Maths and to find relationships between all known mathematical facts of physics, like [this](https://www.youtube.com/watch?v=4ge_ukRbuOw) one, is this too ambatious, is this possible, please help me out guys.",false,4z8m1s,,0,,false,1473073723,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z8m1s/using_ml_to_construct_a_dynamic_structure_to/,t3_4z8m1s,,false,,
1471652645,MachineLearning,vivekjv,arxiv.org,https://arxiv.org/pdf/1607.01759v3.pdf,5,0,0,0,Bag of Tricks for Efficient Text Classification,,false,4ym966,,0,,false,1473062359,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ym966/bag_of_tricks_for_efficient_text_classification/,t3_4ym966,,false,,
1470016582,MachineLearning,iamkeyur,norvig.com,http://norvig.com/ipython/README.html,2,34,34,0,IPython from Peter Norvig,,false,4vk8k4,,0,,false,1473006003,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vk8k4/ipython_from_peter_norvig/,t3_4vk8k4,,false,,
1470623515,MachineLearning,mainguyenmth,youtube.com,https://www.youtube.com/attribution_link?a=xT-jkO7aR7w&amp;u=%2Fwatch%3Fv%3D5aAlofYeySQ%26feature%3Dshare,1,0,0,0,Hướng dẫn sử dụng máy đóng gói bánh nướng,,false,4wo1fq,,0,,false,1473026645,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wo1fq/hướng_dẫn_sử_dụng_máy_đóng_gói_bánh_nướng/,t3_4wo1fq,,false,,
1472351226,MachineLearning,mixmachinery,youtube.com,https://www.youtube.com/attribution_link?a=2Lv3NdsrYLc&amp;u=%2Fwatch%3Fv%3DVerR8Brxm-w%26feature%3Dshare,1,1,1,0,How does the industrial vacuum rubber mixer operate in JCT?,,false,4zxhtr,,0,,false,1473086452,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zxhtr/how_does_the_industrial_vacuum_rubber_mixer/,t3_4zxhtr,,false,,
1470190847,MachineLearning,watchroomfilm,futurism.com,http://futurism.com/whats-the-distinction-between-the-distinction-human-and-artificial-intelligence/,0,0,0,0,"""Watch Room"" discusses importance of understanding AI and distinction between human and artificial intelligence",,false,4vw9d8,,0,,false,1473012291,false,http://a.thumbs.redditmedia.com/Y5kifBUs6dddbrPZXZzwuquNC99VQVA2FZnXef_meH4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vw9d8/watch_room_discusses_importance_of_understanding/,t3_4vw9d8,,false,,
1471071480,MachineLearning,Ekinendustriyell,ekinesanjor.blogspot.com.tr,http://ekinesanjor.blogspot.com.tr/,0,0,0,0,Eşanjör seçiminde nelere dikkat edilmelidir,,false,4xi1dp,,0,,false,1473041961,false,default,t5_2r3gv,false,two,,false,false,,/r/MachineLearning/comments/4xi1dp/eşanjör_seçiminde_nelere_dikkat_edilmelidir/,t3_4xi1dp,,false,News,
1471078297,MachineLearning,algui91,phys.org,http://phys.org/news/2016-08-programme-replicates.html,3,6,6,0,New computer programme replicates handwriting,,false,4xic2d,,0,,false,1473042110,false,http://b.thumbs.redditmedia.com/26-rdUi6PXRmUfPkj4b2QeQ37z4vQLZs5E5vCl1N8EM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xic2d/new_computer_programme_replicates_handwriting/,t3_4xic2d,,false,,
1470127743,MachineLearning,mmattym,pat.ai,http://pat.ai/,6,0,0,0,The Biggest Breakthrough in Natural Language Understanding (NLU),,false,4vriql,,0,,false,1473009825,false,http://a.thumbs.redditmedia.com/JFYaLH7wylVmFKkyVcxuaYhpgL7RvBpLPbt6JDfPdO4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vriql/the_biggest_breakthrough_in_natural_language/,t3_4vriql,,false,,
1472542846,MachineLearning,Fredbob610,gregn610.github.io,https://gregn610.github.io/ttl/html/,4,2,2,0,[question] Interpreting learning curves from hyperparameters,,false,509yjl,,0,,false,1473092991,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/509yjl/question_interpreting_learning_curves_from/,t3_509yjl,,false,,
1470216870,MachineLearning,kirtirajp,papadmakingmachine.com,http://papadmakingmachine.com/prod/automatic-papad-making-machine,0,1,1,0,Fully Automatic Papad Making Machine India,,false,4vxoan,,0,,false,1473013022,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vxoan/fully_automatic_papad_making_machine_india/,t3_4vxoan,,false,,
1471011487,MachineLearning,jarth_or_north,mlr-org.github.io,https://mlr-org.github.io/Benchmarking-mlr-learners-on-OpenML/,6,4,4,0,Benchmarking Machine Learning Methods on Open Datasets using mlr [x-post /r/rstats],,false,4xdqp7,,0,,false,1473039768,false,http://a.thumbs.redditmedia.com/Hr7Bz_HnW-b7AKXLZ2Fdq2Rdi-mxK2ZNfGFwEK_u0E4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xdqp7/benchmarking_machine_learning_methods_on_open/,t3_4xdqp7,,false,,
1470762257,MachineLearning,Helix_Hoenikker,wolfram.com,http://www.wolfram.com/language/11/neural-networks/unsupervised-learning-with-autoencoders.html?product=language,0,0,0,0,"Train an autoencoder network to reconstruct handwritten digits after projecting them through to a lower-dimensional ""code"" vector space.",,false,4wx8v5,,0,,false,1473031335,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wx8v5/train_an_autoencoder_network_to_reconstruct/,t3_4wx8v5,,false,,
1472492285,MachineLearning,alasdairallan,motherboard.vice.com,http://motherboard.vice.com/en_uk/read/the-arrival-of-artificially-intelligent-beer,5,0,0,0,The Arrival of Artificially Intelligent Beer,,false,50685l,,0,,false,1473091070,false,http://b.thumbs.redditmedia.com/Z_u5TCD9nErO9baF5oZeu2G_qDZ4S4h-H1mKNuxQR9c.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50685l/the_arrival_of_artificially_intelligent_beer/,t3_50685l,,false,,
1471201934,MachineLearning,arshakn,conf.startup.ml,http://conf.startup.ml/blog/fraud,0,1,1,0,Deep Learning Approach to Fraud,,false,4xpk6b,,0,,false,1473045784,false,http://b.thumbs.redditmedia.com/DqU1PaeEgUfYwEsYmkRDe53mFH7t6iqWqwNTaZi4o2I.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xpk6b/deep_learning_approach_to_fraud/,t3_4xpk6b,,false,,
1472231994,MachineLearning,schmook,arxiv.org,https://arxiv.org/abs/1608.04471,2,17,17,0,[1608.04471] Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,,false,4zpqzd,,0,,false,1473082468,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zpqzd/160804471_stein_variational_gradient_descent_a/,t3_4zpqzd,,false,,
1472059153,MachineLearning,Dogsindahouse1,technologyreview.com,https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/,0,1,1,0,A machine can tell whether you are depressed just by looking at your photos on Instagram,,false,4zdo79,,0,,false,1473076294,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zdo79/a_machine_can_tell_whether_you_are_depressed_just/,t3_4zdo79,,false,,
1471976111,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z7wjr/advice_on_machine_tools/,1,1,1,0,Advice on machine tools?,[deleted],false,4z7wjr,,0,,false,1473073361,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z7wjr/advice_on_machine_tools/,t3_4z7wjr,,false,,
1471086893,MachineLearning,mega10d0n,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xio25/how_to_make_money_with_machine_learning/,7,1,1,0,How to make money with machine learning algorithms as self-employed?,[removed],false,4xio25,,0,,false,1473042278,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xio25/how_to_make_money_with_machine_learning/,t3_4xio25,,false,,
1472409735,MachineLearning,ecobost,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/500qpv/what_is_machine_learnings_main_question/,11,4,4,0,What is Machine Learning's main question?,[removed],false,500qpv,,0,,false,1473088151,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/500qpv/what_is_machine_learnings_main_question/,t3_500qpv,,false,,
1471915545,MachineLearning,datavis,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z3zad/how_to_optimize_call_times/,2,0,0,0,How to optimize call times?,"I have been asked to develop a predictive model to optimize call times from a call center. We have historical data on successful calls, date/time, member-specific attributes, etc.

Ideally, the output would be a time and day of week to call a member to have the best chance of reaching them. I would like recommendations on the type of model to apply and an appropriate method for evaluating the model. Also, any suggestions on potential features to employ would be helpful.

Thanks for any info/assistance!
",false,4z3zad,,0,,false,1473071366,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z3zad/how_to_optimize_call_times/,t3_4z3zad,,false,,
1472648407,MachineLearning,zayfran31189,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50h1ra/multivariate_responses_for_regression/,9,2,2,0,Multivariate responses for regression,"Hey guys,

Needed some help on this. Say there is a question what would you spend on :

1) Car 2) House 3) Wedding 4) xxx 5) xxx

They have to pick 3 options in terms of priority.

So my data has the respondents vertically with each of the options horizontally. So : ( I dont know how to format this but this is in tabular form)

Person Car Rank Wedding Rank House Rank ...... Jake 1 3 1 1 1 2 Alex 0 - 1 2 1 1

Where 1 says they've selected that option and 0 says they haven't.

How do I restructure this data to use in a regression model with weightage given to rank? Is it possible?

Thanks in advance.

EDIT : Table form, couldn't do it in reddit . http://imgur.com/a/02cL2",false,50h1ra,,0,,false,1473096678,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50h1ra/multivariate_responses_for_regression/,t3_50h1ra,,false,,
1471956884,MachineLearning,hkcqr,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z6b0o/matrix_completion_problem_for_data_matrix_with/,1,0,0,0,Matrix completion problem for data matrix with low-rank property.,"I have a data matrix X of size D by N, containing N samples {x^i }, and x^i is vector of size D by 1. Some entries of X is not given (I know where is missing?), and there is obvious linear correlation between samples in X. So I'm trying to recover/estimate the missing entries in matrix completion fashion, I have tried some softwares, but the optimization fails. I'm in a loss how to carry out the task. Any advice? ",false,4z6b0o,,0,,false,1473072548,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z6b0o/matrix_completion_problem_for_data_matrix_with/,t3_4z6b0o,,false,,
1472651428,MachineLearning,petiaradeva,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50h9ri/very_interesting_and_complete_course_strongly/,11,79,79,0,Very interesting and complete course. Strongly recommended!,http://imatge-upc.github.io/telecombcn-2016-dlcv/,false,50h9ri,,0,,false,1473096792,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50h9ri/very_interesting_and_complete_course_strongly/,t3_50h9ri,,false,,
1471207298,MachineLearning,hyperqube12,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xpz3s/a_small_question_about_the_normal_equation/,10,5,5,0,A small question about the normal equation.,"I am following Andrew Ng's ML course and he talked about the normal equation, as an alternative to gradient descent, for small data sets. I found a proof of why the normal equation works [HERE](http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/), however something is still not clear. He checks to see which value of theta minimizes J by looking at the point where the derivatives cancel. If I remember correctly, the point at which the derivative cancels is an extremal point, not necessarily a point of minimum. Is there an additional condition that makes sure the normal equation provides a point of minimum? Or is it that ""it just works in practice"" ? ",false,4xpz3s,,0,,false,1473045991,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xpz3s/a_small_question_about_the_normal_equation/,t3_4xpz3s,,false,,
1470180627,MachineLearning,PM_YOUR_NIPS_PAPERS,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vvirz/new_titan_x_vs_multiple_10xx_cards/,11,14,14,0,New Titan X vs multiple 10xx cards,"The new Titan X card is available today. Is anyone purchasing it? Or has anyone done cost/performance analysis versus 2 or 3 non Titan cards (e.g. Two 1080s vs One Titan X)? 

I'm thinking of grabbing a couple Titan Xs mainly because of the memory but am wondering what bottlenecks others are facing when you train your models. The new Titan has massive memory bandwidth and is 2x faster than the Titan X from March 2015 (source: Facebook/FAIR). For $1200 per card, you better hope you get more value from it than cheaper cards.",false,4vvirz,,0,,false,1473011907,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vvirz/new_titan_x_vs_multiple_10xx_cards/,t3_4vvirz,,false,,
1470710814,MachineLearning,hardmaru,arxiv.org,https://arxiv.org/abs/1606.06539,1,10,10,0,[1606.06539] Drawing and Recognizing Chinese Characters with Recurrent Neural Network,,false,4wtxw0,,0,,false,1473029653,false,default,t5_2r3gv,false,three,googlebrain,false,false,,/r/MachineLearning/comments/4wtxw0/160606539_drawing_and_recognizing_chinese/,t3_4wtxw0,Google Brain,false,Research,
1472445333,MachineLearning,redgansai,iotenthu.com,http://www.iotenthu.com/2016/08/tensorflow-post-deep-learning-with-tensorflow/,0,1,1,0,TensorFlow Post – Deep Learning with TensorFlow,,false,503b7u,,0,,false,1473089495,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/503b7u/tensorflow_post_deep_learning_with_tensorflow/,t3_503b7u,,false,,
1471924072,MachineLearning,mixmachinery,mixmachinery.com,http://www.mixmachinery.com/news/What-is-the-reactor-heater.html,1,1,1,0,What is the reactor heater,,false,4z4kvj,,0,,false,1473071667,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z4kvj/what_is_the_reactor_heater/,t3_4z4kvj,,false,,
1471930448,MachineLearning,kmrocki,arxiv.org,https://arxiv.org/abs/1608.06027,17,21,21,0,[1608.06027] Surprisal-Driven Feedback in Recurrent Networks,,false,4z4ycc,,0,,false,1473071854,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4z4ycc/160806027_surprisaldriven_feedback_in_recurrent/,t3_4z4ycc,,false,Research,
1470824457,MachineLearning,shrikantrane19,newsmaker.com.au,http://www.newsmaker.com.au/news/87473/artificial-intelligence-machines-industry-analysismanufacturersapplicationtechnology-market-overview-report-20162021,0,1,1,0,Global Artificial Intelligence Machines Market Research Report 2021,,false,4x1h17,,0,,false,1473033474,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x1h17/global_artificial_intelligence_machines_market/,t3_4x1h17,,false,,
1472141557,MachineLearning,dccpt,blog.dominodatalab.com,https://blog.dominodatalab.com/pca-on-very-large-neuroimaging-datasets-using-pyspark/,0,5,5,0,Using Apache Spark to Analyze Large Neuroimaging Datasets,,false,4zjapb,,0,,false,1473079170,false,http://a.thumbs.redditmedia.com/NVXO9b6FUHI-88XHWjvTBiDFq3po7gvggcEU6CD_aT8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zjapb/using_apache_spark_to_analyze_large_neuroimaging/,t3_4zjapb,,false,,
1470416037,MachineLearning,amplifier_khan,gab41.lab41.org,https://gab41.lab41.org/nearest-neighbors-at-scale-e55a8d02b16b#.ow6z2jl19,1,6,6,0,Nearest Neighbors At Scale,,false,4wbd6u,,0,,false,1473020132,false,http://b.thumbs.redditmedia.com/vTPpGVKRR92D93J7nuk0FVQPJ7ZxTnMZOEYMwukpBKg.jpg,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wbd6u/nearest_neighbors_at_scale/,t3_4wbd6u,,false,Research,
1472573410,MachineLearning,rubyantix,hallwaymathlete.com,http://www.hallwaymathlete.com/2016/05/introduction-to-machine-learning-with.html,0,5,5,0,Introduction to Machine Learning with Random Forest,,false,50bwf7,,0,,false,1473093988,false,http://a.thumbs.redditmedia.com/4Vl_hj_pMwiQnpj7dg9oFq3fH9UGUkOQI9yWk-nuyR8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50bwf7/introduction_to_machine_learning_with_random/,t3_50bwf7,,false,,
1471977705,MachineLearning,adamnemecek,cs.iit.edu,http://cs.iit.edu/~jkorah/papers/J-temp-12.pdf,0,1,1,0,Temporal Bayesian Knowledge Bases: Reasoning about uncertainty with temporal constraints,,false,4z81gq,,0,,false,1473073431,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z81gq/temporal_bayesian_knowledge_bases_reasoning_about/,t3_4z81gq,,false,,
1472300067,MachineLearning,[deleted],youtube.com,https://www.youtube.com/watch?v=kTfEFnYxej8,0,1,1,0,A very cool talk by Sebastian Thrun and Danny Shapiro on the development of self-driving cars.,[deleted],false,4ztz9a,,0,,false,1473084640,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ztz9a/a_very_cool_talk_by_sebastian_thrun_and_danny/,t3_4ztz9a,,false,,
1471476313,MachineLearning,[deleted],128.84.21.199,http://128.84.21.199/abs/1608.04428,0,1,1,0,[1608.04428] TerpreT: A Probabilistic Programming Language for Program Induction,[deleted],false,4y8w0a,,0,,false,1473055589,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y8w0a/160804428_terpret_a_probabilistic_programming/,t3_4y8w0a,,false,,
1471150409,MachineLearning,haltingpoint,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xmswf/what_approach_would_i_use_to_determine_which_set/,5,0,0,0,What approach would I use to determine which set of user attributes correlate with a higher propensity to do a certain action?,"I'm new to machine learning and still trying to understand the various approaches, algorithms, etc. and when to use them.

--------
**The Problem:**

Let's say I have a bunch of people that sign up for a free trial.  A percentage end up subscribing afterwards.  During the trial, users can take a variety of actions with my product, which I have logged in a DB in the form of dimensional event data, counts of activities, etc.

I want to understand what makes someone a ""qualified"" trial. Ie. someone who has a higher propensity to convert because they did X, Y and Z actions, and had certain ranges of values in their user data.

I understand that a basic classifier algorithm can tell me that, given all of my attributes that I'm looking at, a new trial is either likely or unlikely to subscribe.  However I want to understand what those combination of factors is so that I can better optimize for them.  For example, if they do action X, Y and Z, but had certain metric values greater than 5 in their account, they are 10x as likely to convert as someone who was otherwise equal but had &lt;5 for the same metrics. Thus, to improve overall conversions, I should focus my efforts on getting people to get that value &gt;5.

-------

Please let me know if I should clarify on anything and forgive me if this is a very poorly worded question.  Like I said, I'm just getting started, so not intimately familiar with all of the math and terminology yet, but eager to learn and dig deeper into concepts related to solving this problem so I can experiment with implementing something basic.",false,4xmswf,,0,,false,1473044386,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xmswf/what_approach_would_i_use_to_determine_which_set/,t3_4xmswf,,false,,
1472212053,MachineLearning,thenewstampede,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zo6hf/what_are_the_most_important_papers_for_maching/,1,1,1,0,What are the most important papers for Maching Learning Computer Vision?,"I posted a question on the Computer Vision subreddit about the [most important papers in Computer Vision](https://www.reddit.com/r/computervision/comments/4y97io/what_2030_papers_should_every_computer_vision/) and a lot of people recommended that I should familiarize myself with Machine Learning papers to be up to date on the latest of what is going on in Computer Vision.

What are some of the most important papers in the field of Machine Learning for Computer Vision?",false,4zo6hf,,0,,false,1473081662,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zo6hf/what_are_the_most_important_papers_for_maching/,t3_4zo6hf,,false,,
1471951025,MachineLearning,deepaurorasky,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5ybb/any_notable_differences_between_word2vec_glove/,2,3,3,0,Any notable differences between word2vec &amp;GloVE trained on different corpuses?,"So, I've seen word2vec trained on a variety of corpuses: Google News seems to be the most popular, but I've also come across the use of Wikipedia trained word embeddings. Also, Stanford seems to have pre-trained  Wikipedia, Common Crawl and Twitter GloVE embeddings available. 

1. Has anyone noticed any shortcomings/advantages of the pre-trained sentence embeddings on one corpus versus another?
2. If so, any recommended techniques for choosing a set of pre-trained embeddings? Where is the Google News trained embedding a better choice than a Wikipedia-trained embedding?
",false,4z5ybb,,0,,false,1473072371,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z5ybb/any_notable_differences_between_word2vec_glove/,t3_4z5ybb,,false,,
1470510602,MachineLearning,gj_gj,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wh9lo/batch_learning_the_most_common_in_practice/,9,4,4,0,"batch learning, the most common in practice?","From reading around I would assume that most ML is batch/offline -- a model learned, then let loose on the real world...

I've stumbled across various materials that talk about online learning approaches, which indicate 'real-time' learning exists in practice.

Intuitively, to me, it would seem that batch/offline is far more common in practice (i.e. out of the lab - for commercial use). But I'm having trouble finding any sources that confirm that this. 

Am I right? Any pointers to articles also welcome :)
",false,4wh9lo,,0,,false,1473023166,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wh9lo/batch_learning_the_most_common_in_practice/,t3_4wh9lo,,false,,
1471453902,MachineLearning,UCSDmath,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y6tk8/are_there_nonnefarious_ways_that_machine_learning/,6,0,0,0,Are there non-nefarious ways that machine learning is used for?,"I feel like machine learning is only used for companies so they can target ads to a demographic, or just collect data on people for your own gain. It feels a bit slimey getting into this field, because most of the jobs is for collected data on people and makes me feel like I work at the NSA. Can someone enlighten me on positive aspects and remove my ignorance, or am i right for the most part?",false,4y6tk8,,0,,false,1473054535,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y6tk8/are_there_nonnefarious_ways_that_machine_learning/,t3_4y6tk8,,false,,
1471972271,MachineLearning,pseudo_inverse,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z7ka2/what_is_new_in_introduction_to_linear_algebra_5th/,11,5,5,0,"What is new in ""Introduction to Linear Algebra"" 5th Ed by Strang ?","Hi, 

Im trying to buy the ""Introduction to Linear Algebra"" book by Prof Gilbert Strang to study for Machine Learning. But I see that there is a new edition available (5th Edition). Initially I was hoping to get a low priced copy of the 4th edition. Can somebody tell me, is there much difference between the two editions, 4th and 5th editions ? Is buying the 4th edition useful enough or does the 5th edition have ground breaking new material ?

thanks",false,4z7ka2,,0,,false,1473073187,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z7ka2/what_is_new_in_introduction_to_linear_algebra_5th/,t3_4z7ka2,,false,,
1472320484,MachineLearning,pulkitmaloo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zvcob/research_paper_topicsideas_help/,5,0,0,0,Research paper topics/ideas help,"Hey guys,
I want to do a research paper on Machine learning and I would really appreciate if you all could give me some suggestions or ideas about the topics that I should work on.

I came up with many ideas but the problem I'm facing is that whatever my topic is, I google it, and find so many good research papers already published on that topic.",false,4zvcob,,0,,false,1473085352,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zvcob/research_paper_topicsideas_help/,t3_4zvcob,,false,,
1471490301,MachineLearning,downtownslim,arxiv.org,http://arxiv.org/abs/1608.04980,7,28,28,0,[1608.04980] Mollifying Networks,,false,4ya03t,,0,,false,1473056150,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4ya03t/160804980_mollifying_networks/,t3_4ya03t,,false,Research,
1472452351,MachineLearning,banguru,digitalrev.com,http://www.digitalrev.com/article/facebook-makes-image-identification-software-available-to-the-public,0,8,8,0,Facebook Makes Image Identification Software Available To The Public,,false,503p6c,,0,,false,1473089709,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/503p6c/facebook_makes_image_identification_software/,t3_503p6c,,false,,
1470822912,MachineLearning,alexjc,arxiv.org,http://arxiv.org/abs/1608.02908,10,29,29,0,"[1608.02908] Residual Networks of Residual Networks: Multilevel Residual Networks (new SOTA on CIFAR-10, CIFAR-100)",,false,4x1efv,,0,,false,1473033438,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4x1efv/160802908_residual_networks_of_residual_networks/,t3_4x1efv,,false,Research,
1471609277,MachineLearning,OpenDataSciCon,opendatascience.com,https://www.opendatascience.com/blog/introduction-deep-learning-for-chatbots-part-1/,5,12,12,0,"Introduction: Deep Learning for Chatbots, part 1",,false,4yilhp,,0,,false,1473060492,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yilhp/introduction_deep_learning_for_chatbots_part_1/,t3_4yilhp,,false,,
1472041224,MachineLearning,metacurse,arxiv.org,http://arxiv.org/abs/1608.06581,1,1,1,0,[1608.06581] Fathom: Reference Workloads for Modern Deep Learning Methods,,false,4zca3d,,0,,false,1473075588,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zca3d/160806581_fathom_reference_workloads_for_modern/,t3_4zca3d,,false,,
1472045998,MachineLearning,OpenDataSciCon,opendatascience.com,https://www.opendatascience.com/blog/preprocessing-data-a-python-workflow-part-1/,0,1,1,0,"Preprocessing Data, a Python Workflow Part 1 By Caitlin Malone, Data Scientist - Civis Analytics",,false,4zcm3l,,0,,false,1473075757,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zcm3l/preprocessing_data_a_python_workflow_part_1_by/,t3_4zcm3l,,false,,
1472134899,MachineLearning,gthank,medium.com,https://medium.com/@Jaconda/a-concise-history-of-neural-networks-2070655d3fec#.obg8hxh1u,0,0,0,0,A Concise History of Neural Networks,,false,4zipyt,,0,,false,1473078873,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zipyt/a_concise_history_of_neural_networks/,t3_4zipyt,,false,,
1472490297,MachineLearning,belangrijk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/5061p8/who_wants_to_explain_frequentist_vs_bayesian/,7,0,0,0,Who wants to explain frequentist vs bayesian machine learning to me?,,false,5061p8,,0,,false,1473090966,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/5061p8/who_wants_to_explain_frequentist_vs_bayesian/,t3_5061p8,,false,,
1472051528,MachineLearning,jackerfrinandis,fonolive.com,http://fonolive.com/a/articles/5590/in-what-ways-can-lubrication-of-a-motor-be-helpful,0,1,1,0,In What Ways Can Lubrication of a Motor Be Helpful?,,false,4zd1yg,,0,,false,1473075979,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zd1yg/in_what_ways_can_lubrication_of_a_motor_be_helpful/,t3_4zd1yg,,false,,
1470768974,MachineLearning,benfred,katbailey.github.io,http://katbailey.github.io/post/gaussian-processes-for-dummies/,1,82,82,0,Gaussian Processes for Dummies,,false,4wxub6,,0,,false,1473031639,false,http://a.thumbs.redditmedia.com/roPp-dwgA7e5KjCgoJukncUE6hWAKXz1oo-1sBwH314.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wxub6/gaussian_processes_for_dummies/,t3_4wxub6,,false,,
1470863428,MachineLearning,hoaphumanoid,miguelgfierro.com,https://miguelgfierro.com/blog/2016/when-to-use-deep-learning-in-a-data-science-problem/,2,0,0,0,When to use Deep Learning in a Data Science Problem,,false,4x4fp9,,0,,false,1473035000,false,http://b.thumbs.redditmedia.com/93Da8A2dq5PVLAaQV0si-hnHGhweaYlEcFBLdGz2GCg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x4fp9/when_to_use_deep_learning_in_a_data_science/,t3_4x4fp9,,false,,
1470092540,MachineLearning,amplifier_khan,benjamintd.com,http://www.benjamintd.com/blog/spynet/,1,0,0,0,Teaching an AI to write Python code with Python code,,false,4vpcdc,,0,,false,1473008683,false,http://b.thumbs.redditmedia.com/SgB0ff0JrkDrysVMH0y7C1nWpUO7iVDGsuR0cbdEfog.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vpcdc/teaching_an_ai_to_write_python_code_with_python/,t3_4vpcdc,,false,,
1472109121,MachineLearning,redditshagger,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zhatw/combine_machine_learning_with_monte_carlo/,0,1,1,0,combine machine learning with monte carlo,[removed],false,4zhatw,,0,,false,1473078148,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zhatw/combine_machine_learning_with_monte_carlo/,t3_4zhatw,,false,,
1470064773,MachineLearning,StephenLasky,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vmzzo/machine_learning_graduate_school_options/,0,0,0,0,Machine Learning Graduate School Options,[removed],false,4vmzzo,,0,,false,1473007452,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vmzzo/machine_learning_graduate_school_options/,t3_4vmzzo,,false,,
1472067793,MachineLearning,andraxo123,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zeeas/why_is_scipyoptimizeminimize_so_slow_i_was_told/,18,0,0,0,why is scipy.optimize.minimize so slow? I was told python was better than matlab in ML ?,[removed],false,4zeeas,,0,,false,1473076663,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zeeas/why_is_scipyoptimizeminimize_so_slow_i_was_told/,t3_4zeeas,,false,,
1470677462,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wrc1m/demand_forecasting_help_crosspost/,3,0,0,0,Demand forecasting help [crosspost],"Let's say hypothetically that we are trying to attain highly accurate demand forecast for tens of thousands of products in a company. These products are available all over the world, and therefore not only is it important to get a product level demand forecast as well but a product-geography demand forecast.

Of course this means that there is product*geography number of forecasts that need to be created. (The whole point here is to avoid dis aggregation to boost accuracy)

Is there a way to cluster all of these product-geography combinations efficiently with the goal of assign a forecasting methodology to each cluster that is highly accurate?

How would someone go about doing this? What kind of clustering technique would need to be used? What would be the criterion that would need to be clustered on?

Note:
Hypothetically it would be important to use causal variables, not just time series/historical models. Would one of the other criterion be to cluster based on significant factors for each of the product-geography combinations?
Please let me know if I should elaborate on any of the points below.
Thanks!",false,4wrc1m,,0,,false,1473028321,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wrc1m/demand_forecasting_help_crosspost/,t3_4wrc1m,,false,,
1471459509,MachineLearning,dpietrek,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y7cwl/dtcnn_do_you_know_any_discrete_time_cellular/,2,1,1,0,[DT-CNN] Do you know any Discrete Time Cellular Neural Network implementation in C++ or other language (or pseudocode)?,I need implementation of Discrete Time Cellular Neural Network. I have templates for function i need only implementation of image processing.,false,4y7cwl,,0,,false,1473054805,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y7cwl/dtcnn_do_you_know_any_discrete_time_cellular/,t3_4y7cwl,,false,,
1470262442,MachineLearning,malexian,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w17h3/building_a_setup/,6,1,1,0,Building a setup.,"Hello, I'm unsure if there is a better place to post this but here is my question.

I'm planning on using a single Pascal card to do some ML related work (likely tensorflow) and I want to connect my monitor to this card as well.

Would I be able to reliably use the graphics card when it is calculating? ie can I still smoothly interact with my linux GUI if my monitor is attached to the card that is calculating. In my past experience, mining cryptocurrency will cause the GUI to become very very unresponsive even if I tone down the utilization. 
",false,4w17h3,,0,,false,1473014918,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w17h3/building_a_setup/,t3_4w17h3,,false,,
1472593903,MachineLearning,deeplearningmaniac,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50dnwy/loading_large_data_for_sgd_in_python/,0,1,1,0,loading large data for SGD in python,"Hi,

I'm working in the following scenario (with Python). I have a neural net optimised with SGD. Each datum is fairly large (let's say 10 megabytes) and I have a lot of data. The entire data set is too large to fit in the RAM. I can see three possible ways of working with this data.

1. Load each datum sequentially from hard drive. This is slow for obvious reasons.

2. Use multithreading (threading library). Then I can have multiple threads loading the data from the hard drive and putting them in the queue from which the main process is reading. This is working fairly well though it seems to me that GIL (https://wiki.python.org/moin/GlobalInterpreterLock) is slowing me down. The computation of the gradient is clearly not really executed in parallel with reading from the hard drive.

3. Use multiprocessing (multiprocessing library). This is analogous to 2. though the implementation of the queue I need to use then is using Linux pipes which are limited to be very small, so the main process spends most of the time reading data from the queue in tiny parts.

Can you think of a more efficient way of implementing such producer-consumer problem for the purposes of SGD in Python?",false,50dnwy,,0,,false,1473094918,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50dnwy/loading_large_data_for_sgd_in_python/,t3_50dnwy,,false,,
1470635758,MachineLearning,iamkeyur,experfy.com,https://www.experfy.com/blog/how-to-become-a-data-scientist-part-2-3,0,1,1,0,"How to Become a Data Scientist, Part 2",,false,4woppw,,0,,false,1473026989,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4woppw/how_to_become_a_data_scientist_part_2/,t3_4woppw,,false,,
1471402203,MachineLearning,mainguyenmth,youtube.com,https://www.youtube.com/attribution_link?a=D8V5PwL3TLQ&amp;u=%2Fwatch%3Fv%3DPPzKptqKltI%26feature%3Dshare,1,1,1,0,"Máy thái dược liệu, thảo dược giá rẻ",,false,4y3djd,,0,,false,1473052799,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y3djd/máy_thái_dược_liệu_thảo_dược_giá_rẻ/,t3_4y3djd,,false,,
1471317756,MachineLearning,Underwhelming_Force,machinedaydreams.com,https://machinedaydreams.com/,1,7,7,0,"I created a blog to follow my ML exploits, (focused on amusing uses of other people's models)",,false,4xxh7k,,0,,false,1473049815,false,http://b.thumbs.redditmedia.com/c2fdKYDuL_2StYjEL8a8o3JXCGcvt63pG78oZ6YYkRk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xxh7k/i_created_a_blog_to_follow_my_ml_exploits_focused/,t3_4xxh7k,,false,,
1470034796,MachineLearning,cstefanache,myrighttocode.org,http://myrighttocode.org/blog/artificial%20intelligence/particle%20swarm/genetic%20algorithm/collective%20knowledge/machine%20learning/gun-db-artificial-knowledge-sharing,0,3,3,0,Distributed Machine Learning using GunDB,,false,4vl9lx,,0,,false,1473006546,false,http://b.thumbs.redditmedia.com/-ASG4b1-Sus4tIoO4c_razrTM8Qsk8BUy3SbDzGQ2cM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vl9lx/distributed_machine_learning_using_gundb/,t3_4vl9lx,,false,,
1471594338,MachineLearning,mixmachinery,youtube.com,https://www.youtube.com/attribution_link?a=VsIC0Thw8Ik&amp;u=%2Fwatch%3Fv%3DH33q4_KLmyk%26feature%3Dshare,1,1,1,0,What is a ribbon blender video?,,false,4yhplq,,0,,false,1473060043,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yhplq/what_is_a_ribbon_blender_video/,t3_4yhplq,,false,,
1470483396,MachineLearning,MaxTalanov,medium.com,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.i24y2550d,8,122,122,0,Modern Face Recognition with Deep Learning,,false,4wfjz6,,0,,false,1473022282,false,http://b.thumbs.redditmedia.com/XOdMXWtT3a3I6sti6Z-ai8_wWF8Dp3KWTjvlbR9OoRY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wfjz6/modern_face_recognition_with_deep_learning/,t3_4wfjz6,,false,,
1472130716,MachineLearning,DontVoteForMe,medium.com,https://medium.com/@msantalucia/using-a-genetic-algorithm-to-draw-electoral-maps-346ebcb928d2#.gx88zamrl,6,0,0,0,Using a Genetic Algorithm to Draw Electoral Maps,,false,4ziegp,,0,,false,1473078711,false,http://b.thumbs.redditmedia.com/imM8XWyPIUgK8cMLb8qz2gGEa5kfhAFI2zWtE8tHOHs.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ziegp/using_a_genetic_algorithm_to_draw_electoral_maps/,t3_4ziegp,,false,,
1470925404,MachineLearning,dunnowhattoputhere,virtualizationpractice.com,https://www.virtualizationpractice.com/microsoft-readies-azure-gpus-38918/,1,3,3,0,Microsoft Readies Azure GPUs,,false,4x88c9,,0,,false,1473036941,false,http://b.thumbs.redditmedia.com/ByEkMjo9W3FJ_YSzTvHWUXTlpb39KmXEYPem6uPIQbA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x88c9/microsoft_readies_azure_gpus/,t3_4x88c9,,false,,
1470148425,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1608.00182,0,7,7,0,[1608.00182] Deep FisherNet for Object Classification,,false,4vspw6,,0,,false,1473010442,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4vspw6/160800182_deep_fishernet_for_object_classification/,t3_4vspw6,,false,Research,
1470934980,MachineLearning,shash27,github.com,https://github.com/shashankg7/Seq2Seq,11,6,6,0,Generic Seq2seq library in keras,,false,4x91np,,0,,false,1473037355,false,http://b.thumbs.redditmedia.com/82TDX_ToC_LDd0XpMOJsw_pwCWVUzn0osuo601uY25A.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x91np/generic_seq2seq_library_in_keras/,t3_4x91np,,false,,
1470355090,MachineLearning,Greendogo,github.com,https://github.com/tzutalin/labelImg,1,1,1,0,LabelImg: A graphical image annotation tool.,,false,4w7me1,,0,,false,1473018217,false,http://b.thumbs.redditmedia.com/oB0I4gwYOTG-IbLCjEE-HxNh9rn9F1WNwlIEiOYrgPc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w7me1/labelimg_a_graphical_image_annotation_tool/,t3_4w7me1,,false,,
1472260155,MachineLearning,[deleted],github.com,https://github.com/david-gpu/srez,0,1,1,0,Image Super-Resolution Through Deep Learning - Github,[deleted],false,4zry62,,0,,false,1473083600,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zry62/image_superresolution_through_deep_learning_github/,t3_4zry62,,false,,
1471892411,MachineLearning,[deleted],bafflednerd.com,http://bafflednerd.com/learn-machine-learning-online/,3,20,20,0,A good list of courses to learn Machine learning online,[deleted],false,4z25hn,,0,,false,1473070440,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z25hn/a_good_list_of_courses_to_learn_machine_learning/,t3_4z25hn,,false,,
1472158838,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zktzo/want_to_implement_vgg_from_scratch_any_useful/,3,0,0,0,"want to implement VGG from scratch, any useful tutorial/link?",[deleted],false,4zktzo,,0,,false,1473079953,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zktzo/want_to_implement_vgg_from_scratch_any_useful/,t3_4zktzo,,false,,
1471605005,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yibaj/ucl_msc_machine_learning_vs_msc_computational/,0,1,1,0,UCL MSc Machine Learning vs MSc Computational Statistics and Machine Learning,[removed],false,4yibaj,,0,,false,1473060346,false,default,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4yibaj/ucl_msc_machine_learning_vs_msc_computational/,t3_4yibaj,,false,Discusssion,
1470720793,MachineLearning,jvdalen,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wula8/neural_nets_seem_obviously_dumb_for_a_human_limit/,13,0,0,0,Neural Nets Seem Obviously Dumb (for a human) - Limit to further increase in DL results?,"Noobie here, some qs:
 
I've been reading a few papers, and did some small experiments, and it seems that neural nets sometimes do obviously stupid things, but not obvious for a neural net. 

Two examples to illustrate:

1) MNIST: I trained a net (followed an example) and looked at the wrong classifications. Sometimes, it would confuse a 6 and an 8. The 8 would often be an unfinished 8, so it would strongly resemble a 6. The thing is, a human would never confuse the two, cuz you can clearly see it was a 8 which wasn't properly finished. You can see the difference in were you start to draw an 8 vs a 6 (6 you start on the top, and 8 in the middle, hence the little ""dent in the middle left)"". This ""extra"" information, on how a number is drawn (were it starts and ends, etc) is not available to a neural net, but is to humans, and thus gives us extra information to classify a number correctly. 

2) I saw an image which was labeled: ""people standing under a table"", by a network. Obvious that is wrong, cuz people are smaller then tables (is 99% of the cases), so that would make no sense. It was a partytent, which looks like a table cuz it has 4 legs. Again, easy for me to use that extra information about a table being too small, and therefore it couldnt be a table. 

A few questions:
1) Doesn't the lack of ability to incorporate commonsense / world information, put a hard limit on how much better we can get with ml in its current form? It seems that more layers, and more neurons, or better activation, or better features, will not solve this? Or would it?
2) What is this problem formally called, ie, what should I look for when reading arxiv/blogs etc.
3) What are the solutions that are being tried here? 

Thanks!",false,4wula8,,0,,false,1473029986,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wula8/neural_nets_seem_obviously_dumb_for_a_human_limit/,t3_4wula8,,false,,
1470804049,MachineLearning,NichG,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x0hbi/natural_way_to_specify_arbitrary_recurrent/,6,2,2,0,Natural way to specify arbitrary recurrent connections in networks?,"I've been implementing a number of attentional models, and it occurs to me that while we have very natural ways to set up feed-forward architectures or architectures with compartmentalized recurrent parts, we don't have a natural way to make long-range or arbitrary recurrent connections between different parts of a network.

For example, if I make an AttentionLayer in something like Lasagne, I could design it to take an input of some data X, and another input of sequences of coordinates Y, and then output views of X that depend on each Y. That's relatively straightfoward as long as the sequence of coordinates can be generated in a way that does not depend on the output of the AttentionLayer. It's globally feed-forward, even if the generator of the Y sequence is an LSTM or something.

But now lets say I want the results of the AttentionLayer from the first coordinate to influence the choice of the next coordinate. Now I basically have to wrap the entire coordinate generator inside the same layer class as the attention mechanism, because they have dependencies which would cross scan boundaries due to the way the recurrent layers are compartmentalized. In practice, the way I'd do this in Lasagne is to use a CustomRecurrentLayer and use its hidden_to_hidden functionality to wrap the entire sub-network.

The attentional case is pretty simple, but also things like Neural Turing Machines or Reinforcement Learning problems where you need to query some black box for the next data-point need things with this kind of structure. So it seems like it would be very useful to have a language in which you could specify these kinds of recurrent links as easily as we specify feed-forward or locally-recurrent stuff now. If nothing else, it'd enable a lot of experimentation.

Part of the design trouble seems to be that you want to say 'I'm going to receive something here from a layer I haven't defined yet, from the previous recursion'. So one idea would be to use a placeholder or socket or something for that. You declare something as a receiver of data from a previous iteration of the network, then let that receiver be used as an input to any layer. Later on, you call a connect method which connects the output of some later layer at t to the input of the socket at t+1.

Anyhow, the purpose of this post is:

1) If this exists somewhere, I want to know about it so that I don't waste a lot of time re-inventing the wheel

2) Spur discussion on this, to see if it would be possible to add this kind of thing to existing libraries or if its too far outside of the design metaphors they're assuming.",false,4x0hbi,,0,,false,1473032977,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4x0hbi/natural_way_to_specify_arbitrary_recurrent/,t3_4x0hbi,,false,Discusssion,
1470300728,MachineLearning,soulslicer0,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w3mcq/given_a_covariance_and_a_sample_vector_how_do_i/,1,0,0,0,"Given a Covariance and a sample vector, how do I find the percentage likelihood?","I have sample data of 9 dimensions. 3 for position, 3 for dimensions and 3 for orientation. They are not of the same scale. I have computed the covariance of this data, and now I have sample vector. I want to compute the percentage likelihood of this sample vector. How do I do this?",false,4w3mcq,,0,,false,1473016161,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4w3mcq/given_a_covariance_and_a_sample_vector_how_do_i/,t3_4w3mcq,,false,Discusssion,
1472468224,MachineLearning,TK3C,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/504ft4/caffe_opencl_bvlc_googlenet_gpu_memory_questions/,1,1,1,0,caffe opencl / bvlc_googlenet / GPU memory questions,"Hello,

I am trying to train a front car view classification CNN with caffe by finetunning googlenet. Since I own some old AMD Radeon 7970 (3GB global memory), I am using the OpenCL branch. My dataset consists of weakly labeled car images scraped from the net and thus I am manually sorting it out, eliminating ""bad"" training images. Since this is a tedious work, I am curious to see the intermediate results as I am ""clearing"" the dataset and noticed the following:

* With ~30 classes and several thousand training images, I could use training batch size of 32 and testing batch size of 16 with no issues

* With ~160 classes and about 14000 images, I had to decrease training batch size to 28 and testing batch size to 4 to avoid running out of vram.

* With ~300 classes and about 30000 images, I had to decrease training batch size to 24 and testing batch size to 2 (!) to avoid running out of video memory

My images are split like 9/10 for training, 1/10 for validation. I am using the LMDB backend.

Since I am fairly new to this, my question is is that memory consumption behavior expected or I am doing something wrong with the train_val.prototxt parameters? Why is testing consuming so much more video memory? Does the memory consumption increase with the number of training images or the number of classes and if so why, shouldn't it be solely dependant on the batch size itself? Is that an issue with the OpenCL branch only?

Another question that bothers me is as I am iteratively finetunning (trained the first ~30 classes net for ~20 epochs using weights from bvlc_googlenet, changing the last 3 layers, then finetunning the resulting net with more classes, changing again the last 3 layers, then again the same procedure with more classes) - top-1 validation accuracy increases sharply for the first 1-2 epochs, then it begins to vary randomly from ~75% to ~85% without any further improvement (yes, I've looked at the training curve), should I decrease the learning rate?

Also, my dataset is imbalanced as some classes have up to twice as much training images as other classes, does that significantly affect the accuracy of classifying the ""underpresented"" classes or even worse - the accuracy of the net as a whole? If so, where approximately is the point where the training data is considered ""imbalanced"" (e.g no class should have less than x percent training samples as compared to any other or something).

Sorry if the questions are formulated badly (English is not my native language as well). 

Thanks in advance.",false,504ft4,,0,,false,1473090093,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/504ft4/caffe_opencl_bvlc_googlenet_gpu_memory_questions/,t3_504ft4,,false,,
1471169178,MachineLearning,huyhcmut,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xnl9c/hardware_for_reinforcement_learning/,6,3,3,0,Hardware for reinforcement learning,"I have a plan to train a small system play GO (size 8x8) using reinforcement learning on personal Macbook/laptop. Which level of hardware enough to do this with (for examble: Ram,gpu,....)? thank you very much.",false,4xnl9c,,0,,false,1473044784,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xnl9c/hardware_for_reinforcement_learning/,t3_4xnl9c,,false,,
1471876402,MachineLearning,thai_tong,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z0sqt/unsupervised_learning_of_functions_advice/,0,0,0,0,Unsupervised learning of functions advice,"I am trying to approximate two functions for two types of decision makers in a game and I am not sure if there exist unsupervised methods for approximating those functions.

I have data which describes the state of the game 'S' and the action which a player takes 'A'. There are two types of players with different playing styles and they have different functions to decide their action, I call the functions A=f(S) and A=g(S)

I'd like to model f and g but I don't have any labels for which function is being used to make a decision. I'm interested in what methods are available for approximating these functions. My best idea for an algorithm is a bit like k++ clustering and it works like this:

1. Approximate f by using all the data

2. Split the data into two parts, the half which was best predicted by the approximation of f, and the half which was worst predicted.

3. Use the two data sets to approximate f and g with two models.

4. Like the k++ algorithm keep updating the approximations for f and g with the data that they best predict.

This method might be naive, has anyone else attempted a problem of this type?",false,4z0sqt,,0,,false,1473069745,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z0sqt/unsupervised_learning_of_functions_advice/,t3_4z0sqt,,false,,
1470326675,MachineLearning,entercaspa,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w58wq/cnn_and_faces_number_examples/,3,1,1,0,"CNN and faces, number examples","I have defined a neural network using keras as 

        model = Sequential()

    # sort out the input layer later
    # this time try even smaller inpput shape like 40x40
    model.add(convolutional.Convolution2D(64, 3, 3, activation='relu', input_shape=(3, 44, 44)))
    # 224x224 following on from imagenet
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(convolutional.Convolution2D(64, 3, 3, activation='relu'))
    model.add(convolutional.MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(Flatten())

    model.add(Dense(128, activation='relu'))
    model.add(Dropout(p=0.2))
    model.add(Dense(number_types, activation='softmax'))

and am attempting to run around 350 faces of two people (44x44 res headshots cropped using opencv2) and achieving around 40% accuracy, the pictures are not grayscale, and I was wondering how many faces it takes of one subject for it to classify it with a decent degree of accuracy? 
also would love some comments on the setup I have in general and if it is okay for the label to be a one hot encoding of 1x3 matrix with a 1 for whoevers face it is 

EDIT: changed the optimizer from rmsprop to stochastic gradient descent and now the accuracy has dropped to 20%, which is weird to me, any thoughts?",false,4w58wq,,0,,false,1473016999,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w58wq/cnn_and_faces_number_examples/,t3_4w58wq,,false,,
1472223541,MachineLearning,mln00b13,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zp0ev/are_there_any_word2vec_models_available_for/,6,0,0,0,Are there any word2vec models available for transfer learning to train on smaller dataset?,"I have a custom data set for which the pre trained models trained on Wiki data isn't proving enough, but my data size is not much. Are there any models I can use for transfer learning for this case?",false,4zp0ev,,0,,false,1473082089,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zp0ev/are_there_any_word2vec_models_available_for/,t3_4zp0ev,,false,,
1471966714,MachineLearning,dreamchallenges,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z72xi/a_community_challenge_to_automate_and_improve_the/,4,0,0,0,A community challenge to automate and improve the radiology of mammograms using machine learning (x-post from /r/deeplearning),"I'm writing to invite you to participate an effort I have been helping to launch and that the White House highlighted at Vice President Biden's June 29 Cancer Moonshot Summit. 

The Digital Mammography DREAM Challenge is a crowdsourced computational Challenge focused on improving the predictive accuracy of digital mammography for the early detection of breast cancer. The primary benefit of this Challenge will be to establish new quantitative tools based in deep learning that can help decrease the recall rate of screening mammography, with a potential impact on shifting the balance of routine breast cancer screening towards more benefit and less harm.

The challenge has been donated approximately 640,000 mammogram images along with clinical metadata, a fleet of high powered GPU-based servers, and over a million dollars in prize money.

Our public Challenge website where people can register and read all of our details and timing is here:  https://www.synapse.org/Digital_Mammography_DREAM_Challenge

I hope you find this interesting.  We feel the challenge will only be successful with the engagement of people, such as yourselves, with strong machine learning expertise.",false,4z72xi,,0,,false,1473072941,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z72xi/a_community_challenge_to_automate_and_improve_the/,t3_4z72xi,,false,,
1470336892,MachineLearning,cjmcmurtrie,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w653i/ml_development_tips_on_putting_multiple_neural/,4,0,0,0,[ML development] Tips on putting multiple neural networks into a production environment for API requests?,"I have a large number of trained and serialized groups of neural network models. These groups of neural networks belong to different instances of a software service. I've written an API request handler to load models into memory and offer predictions on demand.

HOWEVER - I have all my neural networks under the auspices of the same process. If that process dies, all my models will die with it until I reload!

I had the idea that I might have separate processes loaded and working independently, and the API request handler routes requests to these listener processes. However, I have been advised this could be over-complicating things.

Anyone like to share experience or thoughts on this topic?",false,4w653i,,0,,false,1473017459,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w653i/ml_development_tips_on_putting_multiple_neural/,t3_4w653i,,false,,
1472664019,MachineLearning,shashank879,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50icbl/hey_i_want_to_write_neat_based_bots_for_some_nes/,4,1,1,0,"Hey, I want to write NEAT based bots for some NES games, is there some way i can extract the game states from running game in OpenEmu and simulate button presses?","Im aware of some lua based scripting options available for SNES9x directly (OpenEmu uses this as core for NES games), wondering if OpenEmu exposes an API for it.",false,50icbl,,0,,false,1473097346,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50icbl/hey_i_want_to_write_neat_based_bots_for_some_nes/,t3_50icbl,,false,,
1471032169,MachineLearning,Greendogo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xfizt/any_info_on_neural_nets_trained_on_moving/,3,1,1,0,Any info on Neural Nets trained on moving actuators?,Are there any papers or projects dealing with using Neural Nets to learn how to drive actuators in a desirable way?  Like robotic hands picking up balls or driving wheels to correctly guide a toy car?  I'm not even sure what kind of NN this would be and how the back-propagation would even work here.,false,4xfizt,,0,,false,1473040685,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xfizt/any_info_on_neural_nets_trained_on_moving/,t3_4xfizt,,false,,
1470428890,MachineLearning,enematurret,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wcgv5/depth_batch_normalization_and_also_dropout/,9,2,2,0,Depth &amp; Batch Normalization (and also Dropout),"I've been doing some kind of extensive testing on how deep nets can get with/without BN and also indirectly with/without dropout, and got some interesting results so far:

BTW, I've been mainly testing fully-connected nets, from 50 to 100 neurons per hidden layer only.

With pre-activation BN (dot -&gt; BN -&gt; ReLU), I can train networks with up to ~21 hidden layers effectively (~98% val acc). When I get to around 25 layers it breaks completely (stuck at 10% error forever).

However, with post-activation BN networks with up to around 35 layers converge normally, and start breaking when they have ~50 hidden layers.

Interestingly, dropout also has a really weird factor in all of this. For 10 hidden layers, a dropout rate of 0.5 in the last layer only makes the validation accuracy go 2% up. However, with 20 hidden layers the same rate makes the network break - while no dropout at all makes it converge normally.

Are there any studies and experiments on such things that you all are aware of? I'd like to compare and check what's been done and what hasn't before spending more time on this.

On a side-note, I'm testing this on Keras, using net = BatchNormalization(mode=1)(net) to apply BN. I'm not 100% that's the right way to do it, but that seems to be the correct way from what I read in the documentation (at least for fully-connected nets).",false,4wcgv5,,0,,false,1473020700,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wcgv5/depth_batch_normalization_and_also_dropout/,t3_4wcgv5,,false,,
1470356118,MachineLearning,evc123,nyu.edu,https://www.nyu.edu/projects/bowman/bowman2016phd.pdf,0,16,16,0,"Sam Bowman's thesis on ""Modeling Natural Language Semantics in Learned Representations""",,false,4w7p5e,,0,,false,1473018256,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w7p5e/sam_bowmans_thesis_on_modeling_natural_language/,t3_4w7p5e,,false,,
1471834108,MachineLearning,j_lyf,github.com,https://github.com/chrischoy/3D-R2N2,1,8,8,0,GitHub - chrischoy/3D-R2N2: Multi view images or single view image to voxel reconstruction using a recurrent neural network,,false,4yyiwj,,0,,false,1473068591,false,http://b.thumbs.redditmedia.com/-1s0Zzx6V2WSPFTLM361vrD9kIo4rDyhWA1wgxyW3TU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yyiwj/github_chrischoy3dr2n2_multi_view_images_or/,t3_4yyiwj,,false,,
1470649495,MachineLearning,perceptron01,curatedai.com,http://curatedai.com/,1,7,7,0,"CuratedAI: A literary magazine written by machines, for people",,false,4wpd0a,,0,,false,1473027320,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wpd0a/curatedai_a_literary_magazine_written_by_machines/,t3_4wpd0a,,false,,
1470833048,MachineLearning,reworksophie,re-work.co,https://re-work.co/blog/deep-learning-neil-lawrence-university-sheffield-computational-biology,0,1,1,0,Democratising Deep Learning: Q&amp;A With Prof. Neil Lawrence,,false,4x1xis,,0,,false,1473033707,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x1xis/democratising_deep_learning_qa_with_prof_neil/,t3_4x1xis,,false,,
1471964437,MachineLearning,tschellenbach,blog.getstream.io,http://blog.getstream.io/introduction-contextual-bandits/,1,55,55,0,Contextual Bandits - An Introduction,,false,4z6vpd,,0,,false,1473072840,false,http://b.thumbs.redditmedia.com/ZbBTh_osjIKx5PSXNR6Q7uf6Qc3Ys_vD0_ztGzjemgA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z6vpd/contextual_bandits_an_introduction/,t3_4z6vpd,,false,,
1471553289,MachineLearning,madisonjmyers,spark.tc,http://spark.tc/0-to-life-changing-app-new-apache-systemml-api-on-spark-shell/,0,1,1,0,New Apache SystemML API on the Spark Shell!,,false,4yeosx,,0,,false,1473058521,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yeosx/new_apache_systemml_api_on_the_spark_shell/,t3_4yeosx,,false,,
1470785097,MachineLearning,AlNejati,pseudoprofound.wordpress.com,https://pseudoprofound.wordpress.com/2016/08/03/differentiable-programming/,2,8,8,0,Differentiable Programming,,false,4wz6j6,,0,,false,1473032321,false,http://b.thumbs.redditmedia.com/8Vdci527lmCJNYZc1yGwmKnIrt6UDtOtMG3bogk6i7A.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wz6j6/differentiable_programming/,t3_4wz6j6,,false,,
1470888590,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x67lh/why_is_xgboost_performing_so_differently_on_two/,1,0,0,0,Why is XGBoost performing so differently on two machines?,[deleted],false,4x67lh,,0,,false,1473035902,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x67lh/why_is_xgboost_performing_so_differently_on_two/,t3_4x67lh,,false,,
1470894206,MachineLearning,[deleted],hardwarezone.com.my,http://www.hardwarezone.com.my/m/tech-news-nvidia-pushes-deep-learning-support-ai-startups-new-program,0,0,0,0,NVIDIA pushes Deep Learning support for AI startups with new program,[deleted],false,4x6jgo,,0,,false,1473036068,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x6jgo/nvidia_pushes_deep_learning_support_for_ai/,t3_4x6jgo,,false,,
1471498612,MachineLearning,somethingintheway23,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yajut/convolutional_nn_regression_with_heterogenous/,1,1,1,0,Convolutional NN Regression with Heterogenous Outputs?,"Hi all. I've got a problem I'm currently trying to solve using a CNN, and was hoping for some input on heterogeneous outputs and whether or not that's a bad idea.

The input to the net is a 360x140 black and white image. The images contain 8 alphanumeric characters of uniform size somewhere in the image, placed along the outside radius of a downward curve. The characters are rotated to follow the curve.

The output I need to get is three part: the 'font size' of the characters in pixels (falls between 50 and 70 pixels), the X and Y coordinates for 'upper left' of each character (if you were to draw a box around them), and the rotation of each character.

Is it a bad idea to attempt to train one CNN on all of these outputs? That would be 25 (3 per character + the font size).

Is it a better idea to train different CNN's on the three sets of homogenous outputs?",false,4yajut,,0,,false,1473056438,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yajut/convolutional_nn_regression_with_heterogenous/,t3_4yajut,,false,,
1470377822,MachineLearning,jazzsaxmafia,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w91te/a_question_about_an_equation_in_bishops_prml/,3,0,0,0,A Question about an equation in Bishop's PRML,"It doesn't seem that I can upload an image here... so I uploaded the image of the equation here:
https://drive.google.com/file/d/0B5o40yxdA9PqMnBkMWV2RjBhbVk/view?usp=sharing

I do not understand how equation (2.61) was derived. Two things are unclear,

1. In the transition from first to second line, I thought dz/dy = U, but I do not see this term
2. From second to third line, where does this lambda i came from?

Hope someone could help me get through this.
Thank you!",false,4w91te,,0,,false,1473018948,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w91te/a_question_about_an_equation_in_bishops_prml/,t3_4w91te,,false,,
1471501874,MachineLearning,ml_newcomer,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yaqph/how_to_detect_low_quality_image/,4,0,0,0,How to detect low quality image?,"Let say I have a million images (photos to be exact). I want to automatically detect low quality (low resolution, blurry, etc) photos. Is there exist an algorithm to do this? Or maybe a machine learning solution already exist?

Appreciate any help.",false,4yaqph,,0,,false,1473056533,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yaqph/how_to_detect_low_quality_image/,t3_4yaqph,,false,,
1470898966,MachineLearning,AnvaMiba,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x6se6/160802996_towards_crosslingual_distributed/,11,14,14,0,[1608.02996] Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders,"[arXiv](https://arxiv.org/abs/1608.02996)  
[Github](https://github.com/Avmb/clweadv)  
[Poster](http://www.di.unipi.it/~miceli/cleadv-poster.pdf)

In this preliminary work I try to learn a transformation word embeddings from one language (e.g. English) to another language (e.g. Italian) without using any parallel dataset.

My hypothesis is that this should be possible because languages are assumed to have a hidden vector-like ""concept"" space (of which word embeddings are a crude approximation, although it may make more sense to consider sentence or document embeddings) and if different languages are used to talk about similar themes, the stochastic processes that generate these latent representations should be near isomorphic.

So my general idea is to use generative adversarial networks (GANs) to learn to match word embedding distributions: instead of transforming Gaussian noise to images, as it is usually done in GAN papers, I transform English embeddings to Italian embeddings.

Unfortunately this basic setup doesn't work since training ends up in the pathological state where the generator collapses everything into a single output vector, a known problem of GANs which I think becomes even worse in my case since I use point-mass probability distributions instead of truly continuous ones.

Hence I use adversarial autoencoders (AAEs): I add a decoder that tries to reconstruct English embeddings from the artificial Italian embeddings produced by the generator, using cosine dissimilarity as a reconstruction loss.

Using a few tricks to aid optimization (a ResNet leaky relu discriminator with batch normalization to increase the magnitude of the gradient being backpropagated to the generator) I manage to make the model learn.

Qualitatively, it approximately learns some frequent mappings, but overall it is not competitive with cross-lingual embedding approaches that make use of parallel resources. I don't know if it is just a matter of architecture/hyperparameters or if I have already hit a fundamental limit of how much semantic transfer can be done by using only monolingual data.

Comments, suggestions, criticism are welcome. Also, if you are at ACL 2016 in Berlin, I will present this work as a poster today (Aug 11) in the REPL4NLP workshop.
",false,4x6se6,,0,,false,1473036193,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4x6se6/160802996_towards_crosslingual_distributed/,t3_4x6se6,,false,Discusssion,
1472112756,MachineLearning,doyer,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zhgm6/surviving_nips_2016/,18,14,14,0,Surviving NIPS 2016,"TL;DR: I am going to NIPS for the first time. What do I need to know to survive?


Goals: 1) Learn about some cool new ideas in ML!! I'd hate to sit through all the lectures and have it all go over my head. It would be too cruel a fate.
          2) Impress someone enough that they'll take me into their lab as a PhD candidate. Had good grades in undergrad + GRE's but honestly my undergrad was just such a weak school that I decided to spend a year improving my knowledge base before applying for grad school. Figure this will help to offset that deficiency (along with tons of studying of course!).


I've been an ML enthusiast for a long time and I just finished my undergrad (math) this past May so I decided to suck up the price and attend NIPS for the first time! I went now, I'd gain nothing from it but luckily I have a few months to prepare! 

Current strategy: Bought ""Machine Learning: A Probabilistic Perspective"" as recommended on Talking Machines. I hope to get through most if not all of it by december. As I go through it, I keep finding new dependencies I need such as matrix/tensor calculus, probability theory in general, etc so it's been going a lot slower than expected! And of course I am experimenting with making new algos and using existing algos (at this point I'm super comfortable with tensorflow and all of scikit as well as a few other random tools) for analyzing various data sets (mostly non-public) and reading about 3 papers a week (although they are not all recent papers, just ones relevant to the speech problem I've been working on for the past few months). 

I have about 3 months of actual work time available - any ideas on key things to learn (or sections of math etc to skip for now) would be great!
",false,4zhgm6,,0,,false,1473078231,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zhgm6/surviving_nips_2016/,t3_4zhgm6,,false,,
1471787781,MachineLearning,andraxo123,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yuylc/what_python_ide_do_you_guys_use/,123,93,93,0,what python IDE do you guys use?,"I fell for the pycharm meme, it's really slow and annoying 

I'd like to know what you guys use for maching learning or for python in general?",false,4yuylc,,0,,false,1473066792,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yuylc/what_python_ide_do_you_guys_use/,t3_4yuylc,,false,,
1470215527,MachineLearning,michal_sustr,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vxlyq/ml_processing_pipeline/,4,1,1,0,ML processing pipeline,"A while ago I put up a post here asking about how you organize your ML processing pipeline. 
https://www.reddit.com/r/MachineLearning/comments/4v65q6/ml_data_processing_pipeline/

Some claim that 80% of the effort in data engineering is spent on dealing with the ""plumbering"" while the rest is spent in building the actual machine learning models. Data engineering starts with identifying the right data sources. Data sources can be databases, third party APIs, HTML documents which needs to be scrapped and so on. Acquiring data from databases is a straight forward job, while acquiring data from third party APIs and scrapping may come with its own complexities. Once we manage to acquire the data, the next job is to clean it so that it can be used to train ML models.

I didn't find tools yet that would help to organize these ""plumbering"" tasks efficiently and readably, so I am building a Python pipeline at work. 

It is loosely inspired by:

- http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/

- https://github.com/unnati-xyz/fifthel-2016-workshop

If there would be a support from the community, I can persuade my boss to open-source this pipeline.

**The main features are following:**

1) Organize data scraping/transforming tasks in a DAG (directed acyclic graph), so that it handles dependencies between the tasks that can be distributed on multiple workers (right now I'm deciding between Airflow/Luigi. Probably Airflow).

The most common operations in handling data are:

- mapping data from various sources to a tabular format (X, Y) used to learn ML models

- (stratified) splitting of data to train/test/validation

- preprocessing of data (low pass filtering, binning, etc.)

- extracting features (PCA, SVD, etc.)

2) Organize chaotic experimenting with training models. Experimental code can get quite messy after a while, so it should be organized well. Do you want to try different settings for models? Try different preprocessing of the data? Find out which features are the most important? This pipeline can help you with that by providing a common structure.

3) Integration with TensorFlow / TensorBoard. The entire pipeline can be exported into a model that can be run completely separately from the pipeline, to provide an API / integrate into other applications. 

4) Create data resitories that contain cleaned data / features directly used to train models and can be shared with coworkers / other people seamlessly.

5) Support for Jupyter notebooks in various stages of the pipeline. Don't worry about data preprocessing, just get nice (X,Y) data in your notebook and play with it with your favorite tools.

6) CLI interface that sets up the whole environment automatically and runs the scripts with appropriate sources of data.

7) Outsource model training to 3rd party computational resources - cloud (AWS), custom computational grids, etc.


**Take a look at the pipeline visualisation:**

- https://postimg.org/image/l1mrgeb5t/

**Things I'm considering:**

A) Support other ML frameworks other than TF? Maybe they could be contained within DAG, but exporting will be more of a pain.

I'm very interested in your feedback. :) you can ping me at https://twitter.com/michal_sustr

",false,4vxlyq,,0,,false,1473012987,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vxlyq/ml_processing_pipeline/,t3_4vxlyq,,false,,
1470671200,MachineLearning,the_healer,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wqrze/an_investigation_of_different_between_lasso_and/,1,0,0,0,An investigation of different between lasso and ridge regression,"Hi guys, I am newbie in machine learning but love this subject. In my free time I self-study and would like to post some of my material on this subject. Here is my first post:
http://www.lycapheden.com/?p=9
All constructive comments are welcome. ",false,4wqrze,,0,,false,1473028039,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wqrze/an_investigation_of_different_between_lasso_and/,t3_4wqrze,,false,,
1472655636,MachineLearning,jm-mp,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50hlo4/bad_results_deploying_a_trained_cnn/,14,4,4,0,Bad results deploying a trained CNN,"Hi guys, 

I recently trained a CNN with caffe in order to recognize emotion (dataset is FER2013 http://www-etud.iro.umontreal.ca/~goodfeli/fer2013.html). I get 62% score on the test dataset wich is not too bad.
However when I deploy this trained model for real time recognition with my webcam, my results are very bad ! I am doing the same preprocessing (crop the face with openCV, grayscale, HOG and resize 48*48). 
Any ideas why this is wrong ? 
Cheers",false,50hlo4,,0,,false,1473096963,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50hlo4/bad_results_deploying_a_trained_cnn/,t3_50hlo4,,false,,
1470086109,MachineLearning,jinpanZe,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vou95/how_easy_is_it_to_design_and_debug_custom_modules/,2,0,0,0,How easy is it to design and debug custom modules in Tensorflow vs in Torch?,"From a torch perspective, designing and testing new modules is just writing forward/backward computations and testing the gradients. Because the compilation time is almost negligible due to JIT, this workflow is fairly streamlined.

I'm looking at Tensorflow's process, and it requires coding in C++ and writing hooks in python. At a glance this seems quite cumbersome. Especially for testing a module, wouldn't the compilation time be at the very least an inconvenience, or worse a bottleneck in development? I'm wondering if anyone has had experience with both torch and TF and could give some comment about this.",false,4vou95,,0,,false,1473008422,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vou95/how_easy_is_it_to_design_and_debug_custom_modules/,t3_4vou95,,false,,
1471641114,MachineLearning,xcfmv,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ylbib/what_are_the_algorithms_to_score_the_goodness_of/,6,2,2,0,What are the algorithms to score the goodness of pronunciation when practicing learning foreign language?,"Some applications like DuoLingo can listen to you reading out loud a sentence into the microphone, and give you a score of how good your pronunciation was. What algorithms do they use? Any references? Thanks.",false,4ylbib,,0,,false,1473061881,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ylbib/what_are_the_algorithms_to_score_the_goodness_of/,t3_4ylbib,,false,,
1471486539,MachineLearning,harishrithish7,github.com,https://github.com/harishrithish7/Fall-Detection,1,0,0,0,Human Fall Detection from CCTV camera feed,,false,4y9pck,,0,,false,1473056001,false,http://b.thumbs.redditmedia.com/K8Ghsm6aUBUnTuqSKz3hTD0eoWj-a9YX0e3aACsPFCk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y9pck/human_fall_detection_from_cctv_camera_feed/,t3_4y9pck,,false,,
1471573998,MachineLearning,mainguyenmth,youtube.com,https://www.youtube.com/attribution_link?a=8FVpuzdzYLQ&amp;u=%2Fwatch%3Fv%3D6pYwwWVqkJI%26feature%3Dshare,1,1,1,0,Máy đóng gói dạng bột mịn,,false,4ygeiz,,0,,false,1473059382,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ygeiz/máy_đóng_gói_dạng_bột_mịn/,t3_4ygeiz,,false,,
1471969487,MachineLearning,tbot2112,lucidchart.com,https://www.lucidchart.com/techblog/2016/08/23/multi-armed-multi-fingered-bandit-implementing-a-bandit-algorithm-in-a-multiple-payout-scenario/,1,21,21,0,"The Multi-Armed, Multi-Fingered Bandit: Implementing a Bandit Algorithm in a Multiple-Payout Scenario [x-post from /r/programming]",,false,4z7bf7,,0,,false,1473073062,false,http://b.thumbs.redditmedia.com/LUD2oQzonlKGIcrK_ZeczSZYubF2oodqpQBbGnLhRys.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z7bf7/the_multiarmed_multifingered_bandit_implementing/,t3_4z7bf7,,false,,
1471909408,MachineLearning,psykocrime,ai.stackexchange.com,http://ai.stackexchange.com/,5,12,12,0,Stack Exchange now (again) has an Artificial Intelligence site - in public beta now,,false,4z3it2,,0,,false,1473071133,false,http://a.thumbs.redditmedia.com/9ytdEgIgLUWo_3v17wXbLzu5m8_Tf9Ut6TSc-VPRYP0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z3it2/stack_exchange_now_again_has_an_artificial/,t3_4z3it2,,false,,
1470346533,MachineLearning,[deleted],quora.com,https://www.quora.com/unanswered/Are-the-Top-Artificial-Deep-Neural-Nets-like-GoogleNet-always-inspired-from-Neuroscience-Research,1,0,0,0,Are the Top Artificial Deep Neural Nets like VGG / ResNet / GoogleNet always inspired from Neuroscience Research ?,[deleted],false,4w6y5c,,0,,false,1473017872,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w6y5c/are_the_top_artificial_deep_neural_nets_like_vgg/,t3_4w6y5c,,false,,
1470874172,MachineLearning,tomerbe,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x599j/best_deep_learning_algorithm_for_multipleobject/,0,1,1,0,Best deep learning algorithm for multiple-object recognition,[removed],false,4x599j,,0,,false,1473035418,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x599j/best_deep_learning_algorithm_for_multipleobject/,t3_4x599j,,false,,
1472172694,MachineLearning,mathgirl1997,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zlx8r/does_crossvalidation_score_need_to_be_same_as_a/,0,1,1,0,Does cross-validation score need to be same as a model error function?,[removed],false,4zlx8r,,0,,false,1473080513,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zlx8r/does_crossvalidation_score_need_to_be_same_as_a/,t3_4zlx8r,,false,,
1471712256,MachineLearning,ndcubsfan0,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ypyzc/question_predicting_covariance_between_two/,0,1,1,0,Question: Predicting covariance between two parameterized variables,[removed],false,4ypyzc,,0,,false,1473064258,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ypyzc/question_predicting_covariance_between_two/,t3_4ypyzc,,false,,
1472671832,MachineLearning,lusmay,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50j0zd/tensorflow_vs_torch_which_one_is_suitable_for/,0,1,1,0,"TensorFlow vs Torch, which one is suitable for beginner?",[removed],false,50j0zd,,0,,false,1473097696,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50j0zd/tensorflow_vs_torch_which_one_is_suitable_for/,t3_50j0zd,,false,,
1470887319,MachineLearning,seann999,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x64tq/how_do_you_training_on_unshuffled_data/,8,2,2,0,How do you training on unshuffled data?,"For example:

* do supervised learning with MNIST, but with the data remaining in a sorted, sequential order (e.g. 0,0,0,...,1,1,1,...,2,2,2,...)
* do unsupervised with video of a few movies, streaming continuously, not random extracted clips

What methods/models would be able to learn under these conditions? Most machine learning methods I'm aware of assume IID data, and these scenarios would not allow this.

I think reinforcement learning typically doesn't assume IID data, although on a related note ""shuffled"" data can help (like experience replay for DQN). So I thought maybe considering RL approaches would help, but I'm not so sure.",false,4x64tq,,0,,false,1473035862,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x64tq/how_do_you_training_on_unshuffled_data/,t3_4x64tq,,false,,
1471323324,MachineLearning,HyperSpectralVoice,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xxtgr/help_tensorflow_offline_docker_image_and/,1,0,0,0,[HELP] TensorFlow Offline Docker Image and Installation Procedure,"Hello everyone!

Can someone please help me figure out from where can I download/ fetch the Docker image for TensorFlow using only Windows. The system (Computer-A) on which I am going to install Docker and TensorFlow doesn't have an internet connection. What I want to do is to download the latest TensorFlow docker image on a PC with internet (Computer-B) (along with Docker setup) and then transfer both of these to Computer-A and install them onto it.

Also, if someone can shed some light on the installation procedure that I have to follow (afterwards) in my case, I would be extremely obliged.

Thanks in advance!",false,4xxtgr,,0,,false,1473049988,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xxtgr/help_tensorflow_offline_docker_image_and/,t3_4xxtgr,,false,,
1471258833,MachineLearning,mundada,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xt1n8/whats_the_hello_world_program_of_reinforcement/,28,39,39,0,What's the hello world program of reinforcement learning ?,Basically I want to have some hands on experience through small projects.,false,4xt1n8,,0,,false,1473047560,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xt1n8/whats_the_hello_world_program_of_reinforcement/,t3_4xt1n8,,false,,
1471176616,MachineLearning,metacurse,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xnuv2/what_is_the_general_belief_on_value_of_neural/,21,56,56,0,"What is the general belief on value of ""Neural Turing Machines""?","When the paper came out it was kind of revolutionary. I remember thinking that this was the next wave of DL or something like that where this would be a fundamental new architecture.
But, as time has passed (2 years approx), I am personally quite disappointed with the results. Maybe I was thinking wrong but I remember seeing comments here in this sub and with the people I spoke with - the general opinion was that it was some revolutionary thing (esp, coming from DeepMind. Many Demis slide decks with making the ""computer differentiable"" and ""learn algorithms"").

Yes, there are exploratory works and there has been some progress. But nothing to the effect I had imagined tbh (compare it with resnets or even ELU). And I haven't seen any paper that showed very good improvements with using them on real world tasks. Actually there are papers that say that Memory Networks are more easily trained and they have been applied to a wide range of real world NLP tasks with excellent results.

Also in NIPS Alex Graves talked about training NTMs to learn shortest path and other graph algorithms. Will the utility of it be in such symbolic tasks only?

Also if you have tried to code one yourself what were results?",false,4xnuv2,,0,,false,1473044920,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4xnuv2/what_is_the_general_belief_on_value_of_neural/,t3_4xnuv2,,false,Discusssion,
1471450647,MachineLearning,hyperqube12,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y6i5a/what_are_some_good_resources_for_reinforcement/,12,15,15,0,What are some good resources for Reinforcement Learning?,"I went to the /r/machinelearning wiki and I saw there was a single resource related to reinforcement learning, namely Silver's Reinforcement Learning Lectures. The problem is the link does not work anymore (list was deleted from youtube). So this is why I ask the question in the title, what are some good resources? (Also maybe add them to the wiki page).",false,4y6i5a,,0,,false,1473054375,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y6i5a/what_are_some_good_resources_for_reinforcement/,t3_4y6i5a,,false,,
1472418568,MachineLearning,FourthHead,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/501flw/whats_the_equivalent_of_practice_problems_for_a/,6,5,5,0,"What's the equivalent of ""practice problems"" for a research paper?","I'm an undergrad and I want to do some research on my own, so I've been reading research papers to both understand how one is written and to maybe help me come up with a few ideas.

I've never really done this before, so I don't have a lot of experience. I've only been taught technical information through lectures. When I learn something in class, to make sure I remember the information, and really digest it, I do problems from the textbook; they make me a lot more comfortable with the material.

Now obviously there aren't any practice problems for a research paper. So far, I've just been reading the paper and trying to make sure I understand everything. It's been going okay so far, but *I know* that I'm going to completely forget the paper in a few weeks, if not days.

I was hoping for some advice on what I could to make sure I really remember the material and understand it well enough to be able to apply it in different contexts. Practice problems are obviously not an option, so I was hoping for an alternative.

Here are some ideas I've had so far, please add to them:

- Write a page summarising the research paper. I find that writing something out in my own words can be very helpful.

- While reading the research paper, add annotations to different parts. This will make reading the paper more memorable, and it'll help me connect different concepts and thoughts.

- Try to teach a friend about the research paper. Teaching someone else can often be the best way to make sure you understand the content yourself.

These all seem like decent methods, but they all seem very time consuming. I'm happy to do them if need be, but I was wondering if there are other, faster methods that researchers in the field use to help them out with this. ",false,501flw,,0,,false,1473088523,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/501flw/whats_the_equivalent_of_practice_problems_for_a/,t3_501flw,,false,Discusssion,
1470095838,MachineLearning,graz_yazz,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vple2/approaches_to_predict_a_generalised_target/,3,1,1,0,Approaches to predict a generalised target variable [x-post from /r/datascience],"[X-post from /r/datascience hoping to get more response]

I've been searching for ways to generalise a classifier but haven't had much luck with Google so hoping someone here can point me in the right direction. 


All classifiers I've seen &amp; used so far try to predict a class from a finite list. Eg if predicting what car someone will buy, the target variable would be a list of car names {car1, car2, car3,...,carN}. If the available cars (in this case) changes then the model has to be rebuilt with the new list. If a new car is added, then initially there will be very few/no observations and the retrained model is unlikely to predict that car will be purchased. Not ideal. 


I've been thinking that rather than predict which car, it might be possible to predict the cars features, eg engine size, number of seats etc, then the car prediction would be the car with the most similar features. If new cars are introduced (or old ones no longer available) that wouldn't matter. Thus far I've thought of 2 approaches.


First is to build a model predicting each feature of the target, then return the car with the smallest average distance to each target feature. Eg, 1 model for fuel economy, 1 for number of seats etc. This approach would work for features that are both continuous and categorical, eg fuel economy and car colour but seems quite inelegant, especially as the target space grows. Also, I've tried to run this approach in R, all with various errors. 


The second is to binarise my target's features, creating a large feature matrix as my target variable. For example if a car could have a combination of the following features:
    number of cylinders in {4,6,8} 
    number of seats in {2,5}. 
If carN had 6 cylinders and 2 seats it would be represented as [0,1,0,0,1]. In saying that, if binarising is unnecessary that'd also be cool, eg [6,2]. I've built some models using a binarised matrix as my target in Python using Keras on Theano, but should also work with the models present in scikit. This approach also has the issue of the target matrix growing quickly as the feature space grows. It also can't handle continuous variables unless they are first turned into groups.


My questions are:
1. Is this approach valid? Ie, are there any gaping pitfalls I've missed?
2. If valid, does anybody know of other/better ways to generalise the target variable?
3. Are there alternate methods?
4. Am I trying to bite off more that I can chew?

Any papers, blogs, tutorials, repos etc would all be much appreciated. Thanks (&amp; sorry for such a long post).",false,4vple2,,0,,false,1473008815,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vple2/approaches_to_predict_a_generalised_target/,t3_4vple2,,false,,
1471488036,MachineLearning,darkconfidantislife,anandtech.com,http://www.anandtech.com/show/10575/intel-announces-knights-mill-a-xeon-phi-for-deep-learning,4,4,4,0,"Intel Announces next-gen ""Knight's Mill"", Xeon Phi ""optimized for Deep Learning"".",,false,4y9tmn,,0,,false,1473056060,false,http://b.thumbs.redditmedia.com/aLLsPHsWKnGC3Ga4Zj5VWUvu8KyGks5XEhOMkCFVveA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y9tmn/intel_announces_nextgen_knights_mill_xeon_phi/,t3_4y9tmn,,false,,
1470808841,MachineLearning,malreddysid,malreddysid.github.io,https://malreddysid.github.io/deep_learning/2016/07/23/sudoku-solver-dl.html,1,0,0,0,Sudoku Solver using Deep Learning,,false,4x0qjg,,0,,false,1473033105,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0qjg/sudoku_solver_using_deep_learning/,t3_4x0qjg,,false,,
1471332064,MachineLearning,charissaquijano,seekingalpha.com,http://seekingalpha.com/article/3999660-machine-learning-algorithm-forecasts-market-gains-ahead,0,1,1,0,Machine Learning Algorithm Forecasts Market Gains Ahead,,false,4xya4e,,0,,false,1473050222,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xya4e/machine_learning_algorithm_forecasts_market_gains/,t3_4xya4e,,false,,
1471344243,MachineLearning,italartworld001,denonpu.com,http://www.denonpu.com/products/pu-machines/pu-gasket-series/2014/0328/43.html,0,1,1,0,"Polyurethane injection is a technique to fix the cracks in the foundation walls of buildings, garages or basements. This technique provides a long lasting solution to water leakages and also it is very cost efficient and this process of injection take no time to complete.",,false,4xyvb9,,0,,false,1473050519,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xyvb9/polyurethane_injection_is_a_technique_to_fix_the/,t3_4xyvb9,,false,,
1471445665,MachineLearning,Dogsindahouse1,springboard.com,https://www.springboard.com/blog/data-science-interviews-lessons/,2,9,9,0,What We Learned Analyzing Hundreds of Data Science Interviews,,false,4y60pn,,0,,false,1473054132,false,http://b.thumbs.redditmedia.com/pJ25MorzOCLB0kvXSmyezhddBK3L9wo_x6hVA7SHJQY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y60pn/what_we_learned_analyzing_hundreds_of_data/,t3_4y60pn,,false,,
1472066474,MachineLearning,wibr,backchannel.com,https://backchannel.com/an-exclusive-look-at-how-ai-and-machine-learning-work-at-apple-8dbfb131932b#.cgew4ytkk,8,34,34,0,An Exclusive Look at How AI and Machine Learning Work at Apple,,false,4zea6m,,0,,false,1473076604,false,http://b.thumbs.redditmedia.com/iqHfUrUxU-sxSnvk44Eh167v48065d5TsaVsCD-XMZQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zea6m/an_exclusive_look_at_how_ai_and_machine_learning/,t3_4zea6m,,false,,
1471556094,MachineLearning,compsens,nuit-blanche.blogspot.com,http://nuit-blanche.blogspot.com/p/reference-page.html,0,3,3,0,A list of curated technical reference pages,,false,4yexxo,,0,,false,1473058647,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yexxo/a_list_of_curated_technical_reference_pages/,t3_4yexxo,,false,,
1470347991,MachineLearning,amplifier_khan,ufldl.stanford.edu,http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/,0,1,1,0,Unsupervised Feature Learning and Deep Learning Tutorial,,false,4w72px,,0,,false,1473017939,false,http://b.thumbs.redditmedia.com/aqXdJi796QTyMJk6MrxkcqVrxZnGCrX1Inx6CgB44BQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w72px/unsupervised_feature_learning_and_deep_learning/,t3_4w72px,,false,,
1470953604,MachineLearning,Gustavo6046,notehub.org,https://notehub.org/z5riw,3,0,0,0,New Rehermann chains (by me) promise to be a good alternative to Markov when talking about lexically-correct strings,,false,4xamos,,0,,false,1473038173,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xamos/new_rehermann_chains_by_me_promise_to_be_a_good/,t3_4xamos,,false,,
1471378060,MachineLearning,smerity,minimaxir.com,http://minimaxir.com/2016/08/clickbait-cluster/,6,25,25,0,"Visualizing Clusters of Clickbait Headlines Using Spark, Word2vec, and Plotly",,false,4y1k6d,,0,,false,1473051881,false,http://b.thumbs.redditmedia.com/tuEY6_j2xL-7WuHzN3h5xY6qkJK30BdQ6qb7lz8jk6s.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y1k6d/visualizing_clusters_of_clickbait_headlines_using/,t3_4y1k6d,,false,,
1471474863,MachineLearning,jfields513,bookspace.co,http://www.bookspace.co/search/?query_book=&amp;plus=richard+feynman&amp;minus=,2,1,1,0,Book recommendations with doc2vec,,false,4y8rl4,,0,,false,1473055527,false,http://a.thumbs.redditmedia.com/EpdFOsva15yabOkU05REkPou9OJTSUM1mWXE3LuRzP8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y8rl4/book_recommendations_with_doc2vec/,t3_4y8rl4,,false,,
1470178340,MachineLearning,vanboxel,youtube.com,https://www.youtube.com/watch?v=CdlHNMPFKt4,2,1,1,0,Self-driving Car DeepDrive model Livestream,,false,4vvcls,,0,,false,1473011820,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vvcls/selfdriving_car_deepdrive_model_livestream/,t3_4vvcls,,false,,
1470277857,MachineLearning,[deleted],fastcompany.com,http://www.fastcompany.com/3062158/hit-the-ground-running/how-to-ace-a-data-science-interview,0,0,0,0,How To Ace A Data Science Interview,[deleted],false,4w2bt3,,0,,false,1473015498,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w2bt3/how_to_ace_a_data_science_interview/,t3_4w2bt3,,false,,
1470972197,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xbla7/1070_970_w_theano_on_windows/,3,0,0,0,1070 &amp; 970 w/ Theano on Windows,[deleted],false,4xbla7,,0,,false,1473038664,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xbla7/1070_970_w_theano_on_windows/,t3_4xbla7,,false,,
1470237611,MachineLearning,Brojesuss,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vz0jl/is_there_any_particularly_good_reason_to_use/,2,0,0,0,Is there any particularly good reason to use tensor flow instead of Matlab for deep neural networks?,[removed],false,4vz0jl,,0,,false,1473013752,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vz0jl/is_there_any_particularly_good_reason_to_use/,t3_4vz0jl,,false,,
1470156389,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vtf5e/multiuser_server_for_classificationml_problems/,0,1,1,0,Multi-user server for classification/ML problems?,[removed],false,4vtf5e,,0,,false,1473010806,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vtf5e/multiuser_server_for_classificationml_problems/,t3_4vtf5e,,false,,
1470830358,MachineLearning,deepaurorasky,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x1rq5/how_to_do_analysis_of_a_machine_learning_model_in/,23,9,9,0,How to do analysis of a machine learning model in order to improve it?,"So improving the performance of a neural network is, for me, a large amount of thumb-sucking and experimentation from adjusting number of nodes, number of layers, changing activations, types of training (batch vs single), different optimizations, training for longer, using drop-out etc etc.

What methods exist to analyse what a neural network is doing  
in order to guide what techniques may give a potential improvement? (Whether it be visual or statistical or otherwise). I'm no stranger to ML, but I am when it comes to implementing them in a commercial environment and now, when time is money, I would like to show some method to the madness.",false,4x1rq5,,0,,false,1473033625,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x1rq5/how_to_do_analysis_of_a_machine_learning_model_in/,t3_4x1rq5,,false,,
1470566828,MachineLearning,fuckinghelldad,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wkelo/what_are_some_publications_which_benchmarked/,3,5,5,0,"What are some publications which benchmarked ""input-connected networks""?","[Duvenaud (2014, equation 5.16)](http://www.cs.toronto.edu/~duvenaud/thesis.pdf) describes a model where the original input is connected to each layer of a given network, but he didn't try the model out in practice on any of the popular datasets, which is what I'd like to see. Searching for ""input-connected network"" doesn't yield many results unfortunately.

Duvenaud said in the same publication that the model was originally described in [Neal (1995, chapter 2)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf) but after sitting down with the chapter for a half hour, I still can't see what the actual fuck Duvenaud was on about, and his own description is open for interpretation.

So I'm just interested in any publications where the output of each layer in a neural net is a function of the original input. I'm even interested (but less so) in models like *ResNet*, and Deng and Yu's *deep convex network*.",false,4wkelo,,0,,false,1473024780,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wkelo/what_are_some_publications_which_benchmarked/,t3_4wkelo,,false,,
1471951089,MachineLearning,muoro,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5yfn/how_to_become_a_data_scientist/,0,0,0,0,How to Become A Data Scientist?,"When I was going through my Quora feed I observed that many people asked questions like how to start with data science or how to become a data scientist. The glance that I had on my Quora feed compelled me to write this article.
As the century is gearing up with latest technologies like cloud computing and internet of things (IoT) , everyone has witnessed the amount of high-velocity data coming through various resources all around and we call this Big Data, which is the latest buzzword of the decade, hence big data brings it with the unstructured data that is useful only when the systematic analytics is done around, thus the demand of data scientist needed by organizations has been in vogue. It’s ubiquitous to see that everyone wants to learn the skills to get into the trending things of today, but when I see questions on Quora it makes me feel anxious that how most of the aspiring data scientist just don’t know where to get the guidance from.

The resources are limited and there are nothing mainstream to get the insights to get into the field, many of them are confused what major should they pursue to get into data science there is nothing direct for these young data enthusiast and many of them leave their aspirations as they aren’t able to get things right.

As being one of you I would like to solve this arcane puzzle through sharing my one-year of experience in the field of data analytics. So starting with the answer of most ubiquitous dilemma these days of the major to pursue as being a data scientist, Okay now I would like to tell you that data science is an interdisciplinary field that derives its roots from mathematics, statistics, economics and for sure computer science. My research concludes that for a data scientist technology is just a medium to apply the knowledge of the core fields that are math, stats, and economics. One needs to have good fundamental knowledge of linear algebra, calculus, advance probability, advance statistics &amp; econometrics, to create their own prompt data models of given scenario, these models should have pre-determined objectives that eventually help to drive the economics growth of the organization.

Therefore one can do any major provided it covers the above-mentioned subjects, anything that involves logical reasoning &amp; critical thinking is a good fit. Also, the hard fact is that your major would help you deal with partial knowledge through data science being an interdisciplinary field, you need to gain tremendous knowledge from your internet resources.

To summarize it I would like to tell that data science as the field is still in development phase, and in near future, the data scientists equipped with Artificial Intelligence skills will help society to unleash the disguised patterns of the data all around. “In future, the information of a particular transaction would be much valuable than the actual value of that transaction”.

Links for online courses in data science - http://muoro.io - turn data into decisions with MUORO intelligence suite",false,4z5yfn,,0,,false,1473072372,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z5yfn/how_to_become_a_data_scientist/,t3_4z5yfn,,false,,
1471457639,MachineLearning,Palasokeri,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y76i6/how_do_you_tackle_a_ml_book/,3,1,1,0,How do you tackle a ML book?,"Hi all,

I just started Sutton's book on RL (my first real introduction to RL besides some YouTube videos). 
I was wondering how you all approach studying new material (in particular when you were starting out). Read the book front to back? Do all exercises? Come up with your own projects? A combination?

Me myself typically mostly read front to back and try to come up with a small scale project where I can code out the material or concepts and interact with them more easily. But I'm very much at the beginning of my studies so I'm not very experienced with unguided studies. 

I'm aware that the best way to study varies from person to person. Still i'm curios to know ways you approach studying new material. What helped for you and what did not?

Kind regards,

A fellow ML enthusiast",false,4y76i6,,0,,false,1473054715,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y76i6/how_do_you_tackle_a_ml_book/,t3_4y76i6,,false,,
1470524821,MachineLearning,iamquah,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wi9qv/questions_about_back_propogation_in_neural_network/,11,4,4,0,Questions about back propogation in neural network,"Edit: crisis averted everyone! Turns out my numpy in 2.7 and my numpy in python3 behave differently for some fucking reason which tied back to my third point, which led me to finally debug my errors. I know it was numpy because I tried some tutorial code that didn't work on 2.7 (exactly the same issue as I was facing) but worked on 3.....

Hey all,

I have a few questions: 

1) should I add a bias to my output layer? 

2) say I'm doing a binary classification task, should I add a round(like the function) to the end of my feedforward (right before returning the answer?) because when I don't I get numbers that are around 0.99

3) All my outputs, regardless of how I'm training my network seems to output to 1. Any ideas as to what might be causing this? My accuracy as far as I can tell seems to be hovering around 22-30% and I've checked my weights and they do seem to be changing.  I'm doing a 1 hidden layer NN with sigmoid activations all across.

4) Would I be changing the biases as I would the weights of the function? I'm reading through Mitchell and there is no mention of the delta_bias, but in other sources (internet) it seems like they change the bias during the derivation (either that or I'm misunderstanding their point)",false,4wi9qv,,0,,false,1473023690,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wi9qv/questions_about_back_propogation_in_neural_network/,t3_4wi9qv,,false,,
1470244521,MachineLearning,maddoc74,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vzm3x/optimizing_order_of_moves_for_speedrunning_in/,2,1,1,0,Optimizing order of moves for speedrunning in complex game,"I saw a pretty neat video yesterday of someone attempting a speedrun of ""barrows gloves"", which are a fairly intermediate account goal for players in Oldschool Runescape. Here's how it works:

1. You need to finish a quest, which has several sub quests as requirements

2. The quests take you all over the game world, and many can be done concurrently, so it's possible to optimize your path.

3. Quests require certain items, of which you can only carry 28 at a time.

4. Efficient questing is done using teleportation, some which is only unlocked after finishing searching quests.

5. Each location is some map distance from other locations, which is further complicated by the ability to teleport, eg location A is 1000 game tiles from location B, but only 100 game tiles from location B via teleport T.

6. Quests have dependencies, so even though you could technically just do a ""breadth first search"" from the starting area, completing pieces as you come across them, you can't always do this (location A may have 3 quests you can work on concurrently, but only if you have all the required subquests completed).

My idea for determining optimum order:

1. Start by building a graph of all the locations that are important in all quests, and labelling at that location what items are needed.

2. Do some sort of dependency resolution on the current progress of all quests in the local area to determine subtasks from started quests that you can complete in any given area. (as done in [apt-get?](https://people.debian.org/~dburrows/model.pdf))

3. Give a penalty to the overall cost of supplies; eg you could complete all quests 1 by 1 by using exclusively teleportation, but you would lose out if there were other quests in the area that you could work on with that same teleport.

4. Find ways to prune the decision tree, eg while doing certain quests you can't leave the quest area so you should limit the algorithm's ability to suggest doing so.

Thoughts? It seems almost intractable to figure out the *best* order in which to do things, but there may be a ""more optimal"" solution than the ones people have worked out by hand:

1. https://www.reddit.com/r/2007scape/comments/45fllj/the_most_efficient_quest_order_for_1166_qpsrfd/

2. https://www.reddit.com/r/2007scape/comments/3wx3ez/fastest_rfdbarrows_gloves_ever/",false,4vzm3x,,0,,false,1473014088,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vzm3x/optimizing_order_of_moves_for_speedrunning_in/,t3_4vzm3x,,false,,
1472586279,MachineLearning,gambs,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50d0w0/some_questions_on_hierarchical_reinforcement/,3,5,5,0,Some questions on hierarchical reinforcement learning,"1) What is the state of the art? I would like to do hierarchical reinforcement learning in a large POMDP with continuous states and actions, which leads me towards using a recurrent hierarchical policy gradient algorithm. I noticed this paper: http://arxiv.org/abs/1604.06057 which introduces hierarchical DQN -- would it be safe to assume that this is as close as I can get?

2) I'll be applying to PhD programs this December -- which schools would be good for deep hierarchical RL?",false,50d0w0,,0,,false,1473094589,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50d0w0/some_questions_on_hierarchical_reinforcement/,t3_50d0w0,,false,,
1470635623,MachineLearning,gilliamscruggs,wsj.com,http://www.wsj.com/articles/apple-buys-machine-learning-startup-turi-1470448718,0,1,1,0,Apple Buys Machine-Learning Startup Turi,,false,4wophk,,0,,false,1473026986,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wophk/apple_buys_machinelearning_startup_turi/,t3_4wophk,,false,,
1470385163,MachineLearning,marklit,tech.marksblogg.com,http://tech.marksblogg.com/tensorflow-nvidia-gtx-1080.html,0,0,0,0,Deep Fizz Buzz Tutorial with TensorFlow on an Nvidia GTX 1080,,false,4w9eru,,0,,false,1473019131,false,http://a.thumbs.redditmedia.com/zHjBhmhaYE5UcQnC6cwRdTHeaDcOQ6eP3v5Rjq9uqA0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w9eru/deep_fizz_buzz_tutorial_with_tensorflow_on_an/,t3_4w9eru,,false,,
1471861521,MachineLearning,john_philip,analyticsvidhya.com,https://www.analyticsvidhya.com/blog/2016/05/complete-tutorial-work-big-data-amazon-web-services-aws/,1,10,10,0,Complete tutorial for AWS,,false,4yzw11,,0,,false,1473069274,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yzw11/complete_tutorial_for_aws/,t3_4yzw11,,false,,
1472642810,MachineLearning,reworksophie,re-work.co,https://re-work.co/blog/women-in-machine-intelligence-healthcare-alice-gao-deep-genomics,0,1,1,0,Understanding the Complexity of Genomics With Machine Intelligence,,false,50gp01,,0,,false,1473096495,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50gp01/understanding_the_complexity_of_genomics_with/,t3_50gp01,,false,,
1471093792,MachineLearning,rhiever,github.com,https://github.com/arielf/weight-loss,37,92,92,0,Machine Learning meets ketosis: how to effectively lose weight,,false,4xj00x,,0,,false,1473042451,false,http://b.thumbs.redditmedia.com/-DvRyWNWns2AMqPGpkWtXvNvvGoVpL0xlZliR8U3UsQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xj00x/machine_learning_meets_ketosis_how_to_effectively/,t3_4xj00x,,false,,
1472480637,MachineLearning,tatou27,slackprop.wordpress.com,https://slackprop.wordpress.com/2016/08/28/the-three-faces-of-bayes/,9,100,100,0,The Three Faces of Bayes,,false,5058y1,,0,,false,1473090531,false,http://b.thumbs.redditmedia.com/esiWiRKrNy23fs37CyuhL-N2lD7VxUip18l4qecLUmU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/5058y1/the_three_faces_of_bayes/,t3_5058y1,,false,,
1470401460,MachineLearning,edkeens,github.com,https://github.com/janivanecky/Numpy-RNNs,2,2,2,0,"Vanilla RNN, IRNN, NPRNN in Numpy",,false,4wa83u,,0,,false,1473019546,false,http://b.thumbs.redditmedia.com/QRef9JjbFVcjEyV9vbs8x2XOPSQlwh9cvA9fasPhoWQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wa83u/vanilla_rnn_irnn_nprnn_in_numpy/,t3_4wa83u,,false,,
1471699595,MachineLearning,john_philip,medium.com,https://medium.com/swlh/the-7-best-data-science-and-machine-learning-podcasts-e8f0d5a4a419#.alderypxc,0,3,3,0,7 Best Data Science and Machine Learning Podcats,,false,4yp0u4,,0,,false,1473063770,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yp0u4/7_best_data_science_and_machine_learning_podcats/,t3_4yp0u4,,false,,
1472139594,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1608.06669,0,6,6,0,[1608.06669] On Clustering and Embedding Manifolds using a Low Rank Neighborhood Approach,,false,4zj4cw,,0,,false,1473079080,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4zj4cw/160806669_on_clustering_and_embedding_manifolds/,t3_4zj4cw,,false,Research,
1470235859,MachineLearning,jostmey,arxiv.org,http://arxiv.org/abs/1603.09420,15,8,8,0,Minimal Gate Unit for Recurrent Neural Networks,,false,4vyv89,,0,,false,1473013672,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4vyv89/minimal_gate_unit_for_recurrent_neural_networks/,t3_4vyv89,,false,Research,
1472497249,MachineLearning,omtcyfz,omtcyfz.github.io,https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html,0,17,17,0,Deep Learning Resources with short summaries,,false,506o0o,,0,,false,1473091311,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/506o0o/deep_learning_resources_with_short_summaries/,t3_506o0o,,false,,
1470334142,MachineLearning,J4ck-,theverge.com,http://www.theverge.com/2016/8/4/12369494/descartes-artificial-intelligence-crop-predictions-usda,17,31,31,0,This startup uses machine learning and satellite imagery to predict crop yields,,false,4w5wip,,0,,false,1473017338,false,http://b.thumbs.redditmedia.com/9A714oeYYuJmLZBinq0HIl_wWpUEHpuJx-1Ls9C4FOg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w5wip/this_startup_uses_machine_learning_and_satellite/,t3_4w5wip,,false,,
1472577201,MachineLearning,tashapengu,youtube.com,https://www.youtube.com/watch?v=AkLs9nVHv28,0,1,1,0,Numerai is synthesizing machine intelligence,,false,50c8cd,,0,,false,1473094177,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50c8cd/numerai_is_synthesizing_machine_intelligence/,t3_50c8cd,,false,,
1472159630,MachineLearning,pavanmirla,pmirla.github.io,https://pmirla.github.io/2016/08/04/what-is-perceptron.html,5,0,0,0,"Though invented in 1960’s it is still used today, at places like Google. Why?",,false,4zkwew,,0,,false,1473079987,false,http://b.thumbs.redditmedia.com/utPeRmKJbXWU0Q25TvhB4DtdbT3SDJ-hmgZ1YDPYV7E.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zkwew/though_invented_in_1960s_it_is_still_used_today/,t3_4zkwew,,false,,
1472067718,MachineLearning,Lajamerr_Mittesdine,medium.com,https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa,0,5,5,0,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,,false,4zee22,,0,,false,1473076659,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zee22/machine_learning_is_fun_part_5_language/,t3_4zee22,,false,,
1472580334,MachineLearning,wagonhelm,oreilly.com,https://www.oreilly.com/learning/dive-into-tensorflow-with-linux,0,2,2,0,"How to Train Image Classifier with Ubuntu, TensorFlow and GPU",,false,50cifw,,0,,false,1473094324,false,http://b.thumbs.redditmedia.com/5QuPYGJjHt7tnqomlWjei7I9VC-RXGP4GB7SltC97zs.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50cifw/how_to_train_image_classifier_with_ubuntu/,t3_50cifw,,false,,
1472164509,MachineLearning,sldsrt,ics.uci.edu,http://www.ics.uci.edu/~pjsadows/papers/LocalLearning2016.pdf,2,11,11,0,Local Learning Rules for Neural Networks,,false,4zlapo,,0,,false,1473080192,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4zlapo/local_learning_rules_for_neural_networks/,t3_4zlapo,,false,Research,
1471903586,MachineLearning,c0cky_,medium.com,https://medium.com/@camrongodbout/tensorflow-in-a-nutshell-part-one-basics-3f4403709c9d#.hx20k8wik,6,59,59,0,TensorFlow in a Nutshell - Part One: Basics,,false,4z32ts,,0,,false,1473070909,false,http://b.thumbs.redditmedia.com/l-gAw8ctlG-wp1IAErybhmdMkgCwXSjRWUpXpJC_ddE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z32ts/tensorflow_in_a_nutshell_part_one_basics/,t3_4z32ts,,false,,
1472446768,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/503e7l/updating_model_for_predictions/,3,9,9,0,Updating model for predictions,[deleted],false,503e7l,,0,,false,1473089537,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/503e7l/updating_model_for_predictions/,t3_503e7l,,false,,
1470509062,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wh5bw/need_help_finding_the_right_the_tools_for_the_job/,1,1,1,0,Need help finding the right the tools for the job,[deleted],false,4wh5bw,,0,,false,1473023105,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wh5bw/need_help_finding_the_right_the_tools_for_the_job/,t3_4wh5bw,,false,,
1470798850,MachineLearning,onlyml,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x05wi/should_gradient_vectors_in_sgd_be_normalized_to/,6,0,0,0,Should gradient vectors in SGD be normalized to avoid overshooting the target?,"In cases where we know a lower bound on the target/loss (e.g. 0 for a regression problem), should the gradient vector be normalized in some way to avoid overshooting or drastically undershooting? For example you could just scale the gradient vector such that the magnitude of the linear approximation to the change in the loss function, before multiplying by the learning rate, is equal to the remaining loss. 

Without doing this it seems to me the linear approximation to the change we make when doing a gradient descent update could be drastically larger or smaller than the remaining loss. In which case it seems fair to guess the actual resulting update may be drastically too large (or small), and thus more likely to overshoot the minimum even given a fixed learning rate.

This seems like a simple and obvious thing to do but I can't remember ever seeing it discussed so i'm wondering if I'm missing something. Is this implicitly handled by most of the SGD improvements that exist? Or is there another reason it is not necessary or helpful.",false,4x05wi,,0,,false,1473032817,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x05wi/should_gradient_vectors_in_sgd_be_normalized_to/,t3_4x05wi,,false,,
1471795047,MachineLearning,Kiuhnm,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yvifd/parameterizing_parameterizations_of_parameterized/,7,4,4,0,Parameterizing parameterizations of parameterized parametric parameters?,"I'm new to Deep Learning so I don't know what have already been done in the field yet.

I was halfway through the RNN chapter of the [DLBook](http://www.deeplearningbook.org/) and a thought came to my mind: *Why tying the weights instead of compressing them?*

In other words, we could simplify a model by modeling its weights instead of just tying them. Then, in principle, we could model the weights of this second model and so on.

This could be seen as adding more layers along orthogonal directions. The main advantage should be that our net can grow or reduce without breaking things (too much).

Modeled parameters are more than a compromise between free and tied parameters. In fact, modeled parameters might also depend on the input (or on part of it or indirectly) and thus appear to be *dynamic* and more powerful than static free parameters.

We could also use this method to make our net more powerful during training without wasting work already done. In particular, we attach to the main net a very simple ""parameter-modeling net"" and after some time we switch the parameter-modeling net with something more powerful (initialized or trained so that we resume more or less from where we left off with the previous modeling net). At the end, we have a main net fully trained with fully free parameters. Or we could keep going and make the parameters dynamic.
Instead of replacing the modeling net to make it more powerful, we could also model its parameters, and so on...

We could also use this strategy to automatically grow or reduce a net during training to adapt to the quality and amount of available data.",false,4yvifd,,0,,false,1473067072,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yvifd/parameterizing_parameterizations_of_parameterized/,t3_4yvifd,,false,,
1471874245,MachineLearning,n00bto1337,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z0mzp/how_to_analyze_high_dimensional_data_to_compare/,5,0,0,0,How to analyze high dimensional data to compare for which distance function is best suited to it?,"My original data is images, passed through Tensorflow's Inception model to get 2048 dim vector, which I reduced through PCA to 100 dimension vector. How do I now analyze my data to figure out the distance function most suited to it? I originally used euclidean distance, but having read up on the curse of high dimensionality, I am assuming that is not the right option?

Edit: I need to cluster my data, given a query vector to find it's nearest neighbors.",false,4z0mzp,,0,,false,1473069653,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z0mzp/how_to_analyze_high_dimensional_data_to_compare/,t3_4z0mzp,,false,,
1470156981,MachineLearning,Greendogo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vth35/can_neural_networks_exactly_learn_any_class_of/,17,4,4,0,Can Neural Networks Exactly Learn Any Class of Function Besides Linear Functions?,"We teach Neural Networks to approximate a function over a range of that function.  However, since a Neural Network is, most generally, a linear system of multi-dimensional matrices, I assume it can exactly solve linear equations (example: x = y, x * 2 + 10 = y, z*2 + x*3 = y, etc.) up to equations that are themselves linear systems of multi-dimensional matrices.

However, are there any other classes of function that can be ""exactly"" solved by Neural Networks where that solution is not just an approximation of the function over a range?

What if we allow for Recurrent Neural Networks?",false,4vth35,,0,,false,1473010834,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vth35/can_neural_networks_exactly_learn_any_class_of/,t3_4vth35,,false,,
1472057550,MachineLearning,datasciguy-aaay,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zdjmi/how_do_you_personally_distinguish_bias_from/,6,0,0,0,How do YOU personally distinguish BIAS from VARIANCE when you see it?,"I am seeing a lot of confusion as to what is variance, so much so that PhDs sometimes even don't get it right.  (And you absolutely cannot tell a PhD that.)

So let's do our part, and bring together the opinionated if not great minds of /r/machinelearning together here and accrete a pithy reddit discussion today, and see if we can't confuse the matter even more!  Please tell us what YOU personally do, to know VARIANCE when you see it, and why everyone else has it wrong.  

At the end of the day the reddit user comment getting the most upvotes will be nominated our Charismatic Tribal Leader of Variance, if not actually accurate whatsoever in their understanding and communication of variance versus bias. Thanks for playing!

",false,4zdjmi,,0,,false,1473076228,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zdjmi/how_do_you_personally_distinguish_bias_from/,t3_4zdjmi,,false,,
1472174169,MachineLearning,m_ke,arxiv.org,http://arxiv.org/abs/1608.06993,30,60,60,0,[1608.06993] Densely Connected Convolutional Networks,,false,4zm15o,,0,,false,1473080568,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4zm15o/160806993_densely_connected_convolutional_networks/,t3_4zm15o,,false,Research,
1470925005,MachineLearning,Hydreigon92,blogs.princeton.edu,https://blogs.princeton.edu/imabandit/2016/08/06/kernel-based-methods-for-bandit-convex-optimization-part-1/,0,8,8,0,"Kernel-based methods for bandit convex optimization, part 1 (of 3)",,false,4x875f,,0,,false,1473036923,false,http://b.thumbs.redditmedia.com/czqHIVMuiqyP-r2vl76Z77kqL4fF3Ujl6-KJKWcSabg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x875f/kernelbased_methods_for_bandit_convex/,t3_4x875f,,false,,
1471967446,MachineLearning,Init_ai,blog.init.ai,https://blog.init.ai/the-human-vector-incorporate-speaker-embeddings-to-make-your-bot-more-powerful-ade6fdfca035#.f6852bp6y,1,8,8,0,The human vector: Incorporate speaker embeddings to make your bot powerful,,false,4z7544,,0,,false,1473072973,false,http://b.thumbs.redditmedia.com/RlP7Zaxm_9jmyzV6xPM5aY_ARdzY-5wJ0ph9wCXMuFw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z7544/the_human_vector_incorporate_speaker_embeddings/,t3_4z7544,,false,,
1472658489,MachineLearning,DrLegend,online.cambridgecoding.com,http://online.cambridgecoding.com/notebooks/cca_admin/deep-learning-for-complete-beginners-recognising-handwritten-digits,0,3,3,0,Deep learning for complete beginners: Recognising handwritten digits,,false,50huiy,,0,,false,1473097089,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50huiy/deep_learning_for_complete_beginners_recognising/,t3_50huiy,,false,,
1470344145,MachineLearning,dharma-1,newton.ac.uk,https://www.newton.ac.uk/seminar/20160711160016301,5,15,15,0,Exponential Family Embeddings,,false,4w6qz0,,0,,false,1473017771,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w6qz0/exponential_family_embeddings/,t3_4w6qz0,,false,,
1470780056,MachineLearning,vanboxel,youtube.com,https://www.youtube.com/watch?v=mhMCU4IdtbE,0,3,3,0,Fixing an autonomous car model with constant outputs (livestream),,false,4wyshj,,0,,false,1473032122,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wyshj/fixing_an_autonomous_car_model_with_constant/,t3_4wyshj,,false,,
1472256100,MachineLearning,[deleted],github.com,https://github.com/david-gpu/srez,0,1,1,0,[OC] Image super-resolution through deep learning - Github,[deleted],false,4zrofb,,0,,false,1473083460,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zrofb/oc_image_superresolution_through_deep_learning/,t3_4zrofb,,false,,
1471428255,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y4rf8/how_to_productify_a_machine_learning_solution/,18,17,17,0,"How to ""productify"" a Machine Learning solution?",[deleted],false,4y4rf8,,0,,false,1473053499,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y4rf8/how_to_productify_a_machine_learning_solution/,t3_4y4rf8,,false,,
1470157614,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vtj6m/building_a_box_for_classification_machine/,0,1,1,0,Building a box for classification &amp; machine learning problems?,[deleted],false,4vtj6m,,0,,false,1473010864,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vtj6m/building_a_box_for_classification_machine/,t3_4vtj6m,,false,,
1470201124,MachineLearning,thrawayoptim,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vwwdq/optimization_for_ml/,0,1,1,0,Optimization for ML?,[removed],false,4vwwdq,,0,,false,1473012619,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vwwdq/optimization_for_ml/,t3_4vwwdq,,false,,
1471164673,MachineLearning,ajeon66,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xnfgm/how_could_a_learning_environmenthigh_school/,1,1,1,0,"How could a learning environment(high school, college) benefit from a machine learning project?",[removed],false,4xnfgm,,0,,false,1473044703,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xnfgm/how_could_a_learning_environmenthigh_school/,t3_4xnfgm,,false,,
1471174657,MachineLearning,curiositor,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xns05/question_where_do_i_start_i_want_to_pick_the_item/,3,0,0,0,Question : Where do I start? I want to pick the item name and its price from a price list.,[removed],false,4xns05,,0,,false,1473044880,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xns05/question_where_do_i_start_i_want_to_pick_the_item/,t3_4xns05,,false,,
1472177003,MachineLearning,gavinmh,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zm8o8/thoughts_on_my_startups_machine_vision_product/,4,0,0,0,Thoughts on my startup's machine vision product?,"Hi all,

Seentient uses machine vision to find the apparel in your favorite Instagram posts. Instagrammers simply authorize the Seentient app, and then use Instagram as usual. When Instagrammers like a post, Seentient finds similar pieces from the retailer, and pushes a notification to the user to buy. Check out a demo of our technology at [seentient.com](http://seentient.com/).",false,4zm8o8,,0,,false,1473080676,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zm8o8/thoughts_on_my_startups_machine_vision_product/,t3_4zm8o8,,false,,
1472441376,MachineLearning,donaldsonfan1,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50328p/when_should_one_use_all_language_vs_domain/,5,3,3,0,When should one use all language vs. domain specific training sets when it comes to language models?,"the big tech companies are opensourcing their models, which are basically all uses of language.  but if i care about some specific domain - is it better to use data from that domain only? how do i think about it?",false,50328p,,0,,false,1473089364,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50328p/when_should_one_use_all_language_vs_domain/,t3_50328p,,false,,
1471410673,MachineLearning,Smartless,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y3wbx/starting_a_masters_in_statistics_my_program_has/,10,0,0,0,Starting a masters in statistics. My program has an interdisciplinary element that I want to aim at ML. Need some advice.,"As the title says, I'm about to start a masters in stats. But the school im going to has more of an interdisciplinary stats program; the basic set up is that each semester you take two stats related courses, then you take one or more courses related to the topic you want to relate your stats education to.

I'm wondering what kinds of courses would benefit me the most for aiming my degree at ML.

For some background, my undergrad was in math and I took courses like the real analysis sequence, intro and advanced linear algebra, theory of probability, theory of statistics, combinatorics, and stochastics (in addition to courses like diff EQ and the calc sequence). That being said, I have a pretty scant programming background, which I know I'll need to fix very soon.

Also, I'm already signed up for graph theory this coming fall, if that makes a difference.

Any advice would be greatly appreciated. (eg, are there other math classes I should take? Or should I focus on almost exclusively programming from here?)

Thanks in advance!",false,4y3wbx,,0,,false,1473053063,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y3wbx/starting_a_masters_in_statistics_my_program_has/,t3_4y3wbx,,false,,
1470391586,MachineLearning,SonE123,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w9p6u/where_to_start/,6,0,0,0,Where to start?,"I want to get into machine learning,so where do I start?
",false,4w9p6u,,0,,false,1473019278,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w9p6u/where_to_start/,t3_4w9p6u,,false,,
1472031654,MachineLearning,jocomoco,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zbr2k/what_is_your_opinion_why_is_the_concept_of/,15,17,17,0,What is your opinion : Why is the concept of Renormalization Group Transformation and Phase Transitions not so popular in ML ?,"This paper shows a connection (Renormalization Group Transformation) between physics and ML : http://arxiv.org/abs/1410.3831 

I have post-grad educational background in physics and ML, and I am really fascinated by this connection (between physics and ML). Is it only me ? 

What are your opinions about this paper ? 

What is your opinion about the idea that phase transitions are important for brain functioning and also for deep learning? 

This blog post goes into this topic too : https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/ 

I am kinda surprised that this connection does not get bigger attention. Why not ? I wonder. Opinions ?
 ",false,4zbr2k,,0,,false,1473075324,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zbr2k/what_is_your_opinion_why_is_the_concept_of/,t3_4zbr2k,,false,,
1471957922,MachineLearning,ofirpress,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z6dld/using_the_output_embedding_to_improve_language/,8,25,25,0,Using the Output Embedding to Improve Language Models,"[on arXiv](http://arxiv.org/abs/1608.05859)

Co-author here with a short summary:

The word2vec skip-gram model consits of two matrices which are learned, the word embedding matrix (from which the embeddings are extracted from after training) and a second matrix, of the same size, just before the loss, which we refer to as the ""output embedding"". 

In this paper we show that the word embeddings learned by this output embedding matrix preform almost as well as the ones in the original ""input embedding"" matrix, on multiple benchmarks of word embeddings.


The language model of [Zaremba et. al.](https://arxiv.org/abs/1409.2329) consists of a word embedding layer, connected to two LSTM layers, which are finally connected to a softmax layer. Like in word2vec, the softmax matrix (""output embedding"") is of the same size as the word embedding matrix.

We show that this output embedding preforms much better than the input embedding of the same model. By training a modified model, in which the input and output embeddings are tied (equal), we significantly reduce the model's perplexity. 

Finaly, we show that this weight tying can also be used to increase the performance of [attention based translation models](https://arxiv.org/abs/1409.0473).

Our code is available [here](https://github.com/ofirpress/UsingTheOutputEmbedding), any comments or questions are welcome!",false,4z6dld,,0,,false,1473072584,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z6dld/using_the_output_embedding_to_improve_language/,t3_4z6dld,,false,,
1471201717,MachineLearning,ill-logical,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xpjkc/will_cudnn_always_require_registration_to_download/,18,8,8,0,Will cuDNN always require registration to download?,"Will cuDNN always require registration to download, or will it have the same distribution terms as cuBLAS in the future?",false,4xpjkc,,0,,false,1473045775,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xpjkc/will_cudnn_always_require_registration_to_download/,t3_4xpjkc,,false,,
1471886410,MachineLearning,cnn-news-network,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z1mjg/questions_regarding_vae_math/,2,5,5,0,Questions regarding VAE math,"So I'm a relative neophyte to all this stuff, have a decent math background but definitely still a ton to learn.

I was reading this paper: https://arxiv.org/pdf/1606.05908v2.pdf and can track it pretty well up until some of the math that happens in section 2.1. Can anyone point me in the direction of or give a decent explanation for ""E_z∼QP(X|z)"" (ie: what the hell is this term?) and K-L divergence in this context? Is it effectively a measure of how effectively some feature z represents P(X)? ",false,4z1mjg,,0,,false,1473070171,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z1mjg/questions_regarding_vae_math/,t3_4z1mjg,,false,,
1471888235,MachineLearning,sanity,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z1sa2/how_to_explain_why_random_forests_can_be/,60,41,41,0,How to explain why random forests can be effective even when there is no way they are modeling the actual underlying mechanism?,"Sorry that this is a little open-ended.

We've found random forests to be the best performing predictive model in an application related to geology.

A customer is very disturbed by the idea that a random forest would perform well even though it couldn't possibly be modeling the underlying geological processes involved.  

They just don't trust it, and we're not getting too far with the argument that ""it works in cross-validation, so why do you care?"".

Would appreciate if anyone has any thoughts on the best way to get this customer comfortable with this, or good ways to articulate what's going on.",false,4z1sa2,,0,,false,1473070254,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z1sa2/how_to_explain_why_random_forests_can_be/,t3_4z1sa2,,false,,
1471963129,MachineLearning,Im_oRAnGE,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z6ruk/looking_for_c_document_classification_tool/,1,0,0,0,Looking for C++ document classification tool,"Are there any good and preferably simple C++ libraries which basically has the same features as a sklearn Pipeline with a tfidfVectorizer + (SGD)Classifier? It should be able to run on windows.

Is it feasable to use dlib for the classification and have some other tool for the vectorizing? I couldn't find a good c++ tfidf implementation with some googling, and I'm not sure if a word2vec implementation is appropriate for my problem.

The documents themselves come from Apache Tika and are very diverse, often containing lots of noise, especially ones coming from xsl files.

I can reach around 93% accuracy with sklearn, which is more than enough. I just need it to run on windows, and preferably in C++/C#.",false,4z6ruk,,0,,false,1473072786,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z6ruk/looking_for_c_document_classification_tool/,t3_4z6ruk,,false,,
1470681848,MachineLearning,gusuk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wrqwg/ontology_for_machine_learning_concepts/,4,1,1,0,Ontology for Machine Learning concepts,What are some of the most comprehensive or commonly used or actively updated ontologies for machine learning?,false,4wrqwg,,0,,false,1473028532,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wrqwg/ontology_for_machine_learning_concepts/,t3_4wrqwg,,false,,
1472581081,MachineLearning,ETTeddy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50ckrs/anyone_has_built_a_software_product_with_machine/,1,0,0,0,Anyone has built a software product with machine learning component?,"I am going to offer a consulting service to help enterprise perform machine learning on their data. I'm also trying to build a SaaS to serve some of my algorithms. 

I have tons of questions about how to go about it. If anyone has done something like this, I'd love to get to know you.

For now, I wonder what you are doing and how did you get there to where you are? Do you have a PhD? Did you do research on your own before turning it into a software? Is it a collaboration between machine learning engineer and software engineer, or did you do it all by yourself?",false,50ckrs,,0,,false,1473094357,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50ckrs/anyone_has_built_a_software_product_with_machine/,t3_50ckrs,,false,,
1471398282,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1608.04481,0,13,13,0,[1608.04481] Lecture Notes on Randomized Linear Algebra,,false,4y33zf,,0,,false,1473052664,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4y33zf/160804481_lecture_notes_on_randomized_linear/,t3_4y33zf,,false,Research,
1470876286,MachineLearning,JoeyRob,robusttechhouse.com,http://robusttechhouse.com/can-artificial-intelligence-be-the-future-of-doing-business/,0,1,1,0,Can Artificial Intelligence be the future of doing business,,false,4x5ehp,,0,,false,1473035491,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x5ehp/can_artificial_intelligence_be_the_future_of/,t3_4x5ehp,,false,,
1470028830,MachineLearning,hooba_stank_,quora.com,https://www.quora.com/Is-there-something-that-Deep-Learning-will-never-be-able-to-learn-1,28,3,3,0,Is there something that Deep Learning will never be able to learn?,,false,4vkz3v,,0,,false,1473006396,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vkz3v/is_there_something_that_deep_learning_will_never/,t3_4vkz3v,,false,,
1470635810,MachineLearning,iamkeyur,experfy.com,https://www.experfy.com/blog/how-to-become-a-data-scientist-part-1-3,0,1,1,0,"How to Become a Data Scientist, Part 1",,false,4woptf,,0,,false,1473026991,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4woptf/how_to_become_a_data_scientist_part_1/,t3_4woptf,,false,,
1471072985,MachineLearning,mixmachinery,youtube.com,https://www.youtube.com/attribution_link?a=oKUtJJt3EpE&amp;u=%2Fwatch%3Fv%3DRcdd75BJcYA%26feature%3Dshare,1,1,1,0,What is semi filling system towards silicone sealant of sigma mixer extr...,,false,4xi3uz,,0,,false,1473041996,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xi3uz/what_is_semi_filling_system_towards_silicone/,t3_4xi3uz,,false,,
1472454843,MachineLearning,Alexey75,spectrum.ieee.org,http://spectrum.ieee.org/energywise/energy/environment/tackling-air-quality-prediction-in-south-africa-with-machine-learning,0,1,1,0,Tackling Air Quality Prediction in South Africa With Machine Learning,,false,503to9,,0,,false,1473089772,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/503to9/tackling_air_quality_prediction_in_south_africa/,t3_503to9,,false,,
1470297676,MachineLearning,InaneMembrane,arstechnica.com,http://arstechnica.com/gadgets/2016/08/ibm-phase-change-neurons/?comments=1,2,26,26,0,IBM creates world’s first artificial phase-change neurons | Ars Technica,,false,4w3gz7,,0,,false,1473016086,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w3gz7/ibm_creates_worlds_first_artificial_phasechange/,t3_4w3gz7,,false,,
1471540592,MachineLearning,alxndrkalinin,openai.com,https://openai.com/blog/machine-learning-unconference/,33,95,95,0,Machine Learning Unconference | OpenAI,,false,4ydhl4,,0,,false,1473057916,false,http://b.thumbs.redditmedia.com/b5RUCu8j0YeI9WGh4ZOnRKNzPBZoDd7L-2JZQ4xAvpA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ydhl4/machine_learning_unconference_openai/,t3_4ydhl4,,false,,
1470847868,MachineLearning,elisebreda,blog.yhat.com,http://blog.yhat.com/posts/ML-resources-you-should-know.html,1,8,8,0,Machine Learning and Data Science Resources You Should Know About,,false,4x33i9,,0,,false,1473034306,false,http://b.thumbs.redditmedia.com/xKRt-rGgB5fbAEA-C8xdXsFkP1GJSkc01v-qrw3_m6o.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x33i9/machine_learning_and_data_science_resources_you/,t3_4x33i9,,false,,
1471022940,MachineLearning,akelleh,medium.com,https://medium.com/@akelleh/causal-data-science-721ed63a4027#.b9dczx7uo,0,5,5,0,weekend reading: Causal Data Science,,false,4xepdu,,0,,false,1473040266,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xepdu/weekend_reading_causal_data_science/,t3_4xepdu,,false,,
1471970777,MachineLearning,sanity,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z7fjj/is_genetic_programming_used_as_an_alternative_to/,21,1,1,0,Is Genetic Programming used as an alternative to more conventional supervised learners like NNs and RFs? Why/why not?,,false,4z7fjj,,0,,false,1473073120,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z7fjj/is_genetic_programming_used_as_an_alternative_to/,t3_4z7fjj,,false,,
1471464907,MachineLearning,sudeepraja,sudeepraja.github.io,http://sudeepraja.github.io/Neural/,1,4,4,0,Backpropagation Equations in Matrix Form - Easy to derive and remember,,false,4y7vb7,,0,,false,1473055064,false,http://b.thumbs.redditmedia.com/vES_OL6ruHJ9Pmg7TO6jhdvVzYZt_HlKhHzFYSOgs7w.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y7vb7/backpropagation_equations_in_matrix_form_easy_to/,t3_4y7vb7,,false,,
1470354621,MachineLearning,deep_debater,davheld.github.io,http://davheld.github.io/GOTURN/GOTURN.html,1,1,1,0,Learning to Track at 100 FPS with Deep Regression Networks,,false,4w7l5a,,0,,false,1473018199,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w7l5a/learning_to_track_at_100_fps_with_deep_regression/,t3_4w7l5a,,false,,
1470173670,MachineLearning,amplifier_khan,blog.siftscience.com,http://blog.siftscience.com/2015/large-scale-decision-forests-lessons-learned/,0,17,17,0,Large Scale Decision Forests: Lessons Learned,,false,4vuzfz,,0,,false,1473011632,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vuzfz/large_scale_decision_forests_lessons_learned/,t3_4vuzfz,,false,,
1470626748,MachineLearning,hlyates,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wo8be/inquiry_caffee_theano_and_cnn/,4,0,0,0,"Inquiry: Caffee, Theano, and CNN?",[removed],false,4wo8be,,0,,false,1473026743,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wo8be/inquiry_caffee_theano_and_cnn/,t3_4wo8be,,false,,
1471191843,MachineLearning,BlueFolliage,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xos4v/newcomer_lost_how_did_you_get_started_with/,8,2,2,0,"[Newcomer, Lost] How did you get started with machine learning? Do you know of any courses online that have projects/assignments publicly available with solutions to them as well?",[removed],false,4xos4v,,0,,false,1473045388,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xos4v/newcomer_lost_how_did_you_get_started_with/,t3_4xos4v,,false,,
1470080216,MachineLearning,nbendov,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vock3/machine_learning_for_big_data_analytics_webinar/,0,1,1,0,Machine Learning for Big Data Analytics Webinar (http://bit.ly/29YN­snQ),[removed],false,4vock3,,0,,false,1473008160,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vock3/machine_learning_for_big_data_analytics_webinar/,t3_4vock3,,false,,
1471152270,MachineLearning,Foxtr0t,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xmw4v/drmad_distilling_reversemode_automatic/,6,2,2,0,DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks,"Arxiv:
http://arxiv.org/abs/1601.00917

GitHub:
https://github.com/bigaidream-projects/drmad

The authors advertise the ability to tune thousands of hyperparams. They train an MLP on MNIST and tune L2 _for each neuron separately_ :D

&gt; Each neuron has its own penalty on its parameter and thus we are going to optimize 934 hyperparameters in total.

Your thoughts?

",false,4xmw4v,,0,,false,1473044431,true,nsfw,t5_2r3gv,false,three,,false,true,,/r/MachineLearning/comments/4xmw4v/drmad_distilling_reversemode_automatic/,t3_4xmw4v,,false,Research,
1470061261,MachineLearning,bctfcs,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vmpz3/data_mining_with_lenses_a_more_formal_approach/,4,4,4,0,"Data mining with ""lenses"": a more formal approach?","Hi!

I was watching a specific course on Topological Data Analysis from Anthony Bak of Ayasdi, and at several occasions he used the term ""lenses"" to describe a statistical, mathematical or even computational ""device"" through which one can query the raw data to learn something about it. He gave several examples such as the mean, the variance, PCA, etc. on page 102 of his slides http://topology.cs.wisc.edu/Bak.pdf .

I already thought of something similar in the specific field of clustering, but this is the first time I see someone naming all these objects as such. Do you know other names than ""lenses"" for this? Is there any more formal approach of this idea -- taking raw data and projecting it on some subspace to look for specific material, or even writing data mining tools as composition of these ""lenses""?

Thanks!

E: I'm not talking about TDA, which is awesome and which I'm already studying. My question is precisely about the formalization of looking to the data through ""lenses"".",false,4vmpz3,,0,,false,1473007306,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4vmpz3/data_mining_with_lenses_a_more_formal_approach/,t3_4vmpz3,,false,Discusssion,
1471521180,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ybu4v/comprehensive_experiments_of_preprocessing_on/,4,2,2,0,Comprehensive experiments of preprocessing on accuracy?,"I've found that choosing whether to use Standardization (zero mean, std. of 1), MinMaxScaling (e.g. to 0-1 interval) or other preprocessing techniques such as PCA have a substantial impact on the accuracy of my methods (both SVM's and deep networks).

Has anyone ever done some comprehensive experiments on the effect of these preprocessing techniques, incl. when to use what? I haven't been able to find anything like that on Google Scholar.",false,4ybu4v,,0,,false,1473057082,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ybu4v/comprehensive_experiments_of_preprocessing_on/,t3_4ybu4v,,false,,
1470067162,MachineLearning,captfitz,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vn78t/i_built_a_dataset_of_selfreported_symptoms_of/,1,8,8,0,I built a dataset of self-reported symptoms of autoimmune illness (x-post from r/datasets),"Over the past two years I've built a tool for patients of chronic autoimmune and ""invisible"" illnesses to track their symptoms, treatments, and disease triggers. It recently launched to the public at www.flaredown.com after a lengthy private beta.

The goal is to use the data the Flaredown gathers to identify patterns and correlations that might help patients of these hard-to-treat illnesses. For example:

- Does X treatment affect Y symptom positively/negatively/not at all?

- Are there subsets within our current diagnoses that could more accurately represent symptoms and predict effective treatments?

- Can we reliably predict what triggers a flare for a given user or all users with a certain condition?

- Could we recommend treatments effectively, based on similarity of users rather than specific symptoms? (like Netflix recommendations for treatments) 

- Can we quantify a patients level of disease activity based on their self-reported symptoms? How different is it from our existing measures?

We're starting to accrue a real body of data by now and I want to share it with data scientists, researchers, or anyone interested in questions like the ones above. To start, I could use advice on how to format the data for this kind of use. But after that I really want to dig in and start analyzing what we've got.

Let me know your thoughts, and if you're interested in helping out!",false,4vn78t,,0,,false,1473007557,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vn78t/i_built_a_dataset_of_selfreported_symptoms_of/,t3_4vn78t,,false,,
1470925139,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x87kh/interesting_research_topics_given_a_perfect/,0,5,5,0,Interesting research topics given a perfect sentiment model,"Suppose you had a perfect model for extracting sentiment from text. Besides the obvious things such as building a great chatbot and analyzing trends on twitter, what would you consider as interesting research topics using this model?",false,4x87kh,,0,,false,1473036930,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x87kh/interesting_research_topics_given_a_perfect/,t3_4x87kh,,false,,
1470624257,MachineLearning,gambs,arxiv.org,https://arxiv.org/abs/1608.01281,1,3,3,0,[1608.01281] Learning Online Alignments with Continuous Rewards Policy Gradient,,false,4wo323,,0,,false,1473026669,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wo323/160801281_learning_online_alignments_with/,t3_4wo323,,false,,
1472017923,MachineLearning,abhisar_mhptr,arxiv.org,http://arxiv.org/abs/1608.05148,3,54,54,0,Full Resolution Image Compression with Recurrent Neural Networks,,false,4zb2b3,,0,,false,1473074965,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zb2b3/full_resolution_image_compression_with_recurrent/,t3_4zb2b3,,false,,
1471252756,MachineLearning,MaxTalanov,blog.deeprobotics.es,"http://blog.deeprobotics.es/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/07/06/rl-intro/",0,5,5,0,Reinforcement learning in robotics,,false,4xsrtd,,0,,false,1473047421,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xsrtd/reinforcement_learning_in_robotics/,t3_4xsrtd,,false,,
1472095254,MachineLearning,textanalytics,rxnlp.com,http://www.rxnlp.com/why-use-stop-words-for-text-mining/,0,1,1,0,Why Use Stop Words for Text Mining?,,false,4zgid9,,0,,false,1473077742,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zgid9/why_use_stop_words_for_text_mining/,t3_4zgid9,,false,,
1471432464,MachineLearning,alexjc,arxiv.org,http://arxiv.org/abs/1608.04493,8,21,21,0,[1608.04493] Dynamic Network Surgery for Efficient DNNs (Reduces AlexNet by 17.7x),,false,4y4zbz,,0,,false,1473053610,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4y4zbz/160804493_dynamic_network_surgery_for_efficient/,t3_4y4zbz,,false,Research,
1471601563,MachineLearning,abhisvnit,linkedin.com,https://www.linkedin.com/pulse/identifying-clickbaits-using-machine-learning-abhishek-thakur,16,23,23,0,Identifying Clickbaits using Machine Learning,,false,4yi3x3,,0,,false,1473060243,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yi3x3/identifying_clickbaits_using_machine_learning/,t3_4yi3x3,,false,,
1470934960,MachineLearning,hlyates,community.hortonworks.com,https://community.hortonworks.com/questions/51055/assistance-with-extremely-difficult-hive-questionq.html#answer-51071,3,0,0,0,Newcomer question: What is the best approach in Hive or otherwise for aggregating this data I have for preparation to feed to my ML algorithms?,,false,4x91l5,,0,,false,1473037354,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x91l5/newcomer_question_what_is_the_best_approach_in/,t3_4x91l5,,false,,
1471380506,MachineLearning,hyperqube12,dvbuntu.github.io,https://dvbuntu.github.io/wisdom/,3,4,4,0,A very interesting ML blog with video tutorials.,,false,4y1rnw,,0,,false,1473051986,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y1rnw/a_very_interesting_ml_blog_with_video_tutorials/,t3_4y1rnw,,false,,
1470258110,MachineLearning,gavlaaaaaaaa,lewisgavin.co.uk,http://www.lewisgavin.co.uk/NLP/,0,1,1,0,What's the context? with Natural Language Processing,,false,4w0u42,,0,,false,1473014725,false,http://a.thumbs.redditmedia.com/xRSOVi4aPD2QQH-skGGInQCtfVaGM9llVIThL9E5j54.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w0u42/whats_the_context_with_natural_language_processing/,t3_4w0u42,,false,,
1470801548,MachineLearning,markth_wi,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x0byn/learning_path/,4,0,0,0,Learning path,[removed],false,4x0byn,,0,,false,1473032901,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x0byn/learning_path/,t3_4x0byn,,false,,
1470742746,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wvqd3/is_anyone_interested_in_a_service_to/,1,1,1,0,Is anyone interested in a service to automatically colorise b&amp;w photos?,[removed],false,4wvqd3,,0,,false,1473030565,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wvqd3/is_anyone_interested_in_a_service_to/,t3_4wvqd3,,false,,
1472448007,MachineLearning,windweller,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/503gsl/how_to_properly_pretrain_image_captioning_model/,6,1,1,0,How to properly pretrain Image Captioning model?,"As I understand, an image captioning model is built by both CNN (to extract features from image) and a RNN (to generate text). However, CNN can be pre-trained separately using ImageNet or CIFAR-10 corpus. Training on CIFAR-10 is easy, but on ImageNet is hard (time-consuming). My question is: since the state-of-art CNN models are kept being proposed (2 years ago it was VGG, now it's ResNet or DenseNet), how does new researchers avoid wasting (or spending) so much time pre-training a CNN model on the terabyte-level imagenet corpus but still claim a state-of-the-art (or near) model?

I'm eventually going to train the whole thing on Flickr8k and Flickr30k.",false,503gsl,,0,,false,1473089590,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/503gsl/how_to_properly_pretrain_image_captioning_model/,t3_503gsl,,false,,
1472155161,MachineLearning,traw123456,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zkix7/help_with_a_deep_learning_computer_build/,14,0,0,0,Help with a deep learning computer build,"I am attempting to build a pc for machine learning. I am trying to keep it below $700-800, and I am not looking for crazy performance; I just want a well performing computer that I can upgrade as my budget increases. This is my first time building a computer, so I am a bit of a beginner. Here is my list:

**Update** I have decided that it is going to be a much smarted move in the long run to increase my budget and just hold off for a little while until I can afford it. I appreciate all of the comments, and I have gotten much more information than I could have on my own. 

[PCPartPicker part list](http://pcpartpicker.com/list/dFgJM8) / [Price breakdown by merchant](http://pcpartpicker.com/list/dFgJM8/by_merchant/)

Type|Item|Price
:----|:----|:----
**CPU** | [Intel Core i7-6700K 4.0GHz Quad-Core Processor](http://pcpartpicker.com/product/tdmxFT/intel-cpu-bx80662i76700k) | $319.99 @ B&amp;H 
**CPU Cooler** | [Corsair H55 57.0 CFM Liquid CPU Cooler](http://pcpartpicker.com/product/w37wrH/corsair-cpu-cooler-h55) | $49.99 @ Newegg 
**Motherboard** | [Gigabyte GA-H110M-S2H GSM Micro ATX LGA1151 Motherboard](http://pcpartpicker.com/product/MVcMnQ/gigabyte-motherboard-gah110ms2hgsm) | $58.99 @ NCIX US 
**Memory** | [GeIL EVO POTENZA 16GB (2 x 8GB) DDR4-2133 Memory](http://pcpartpicker.com/product/KF8H99/geil-memory-gpr416gb2133c15dc) | $51.99 @ Newegg 
**Video Card** | [Gigabyte GeForce GTX 1060 6GB WINDFORCE OC 6G Video Card](http://pcpartpicker.com/product/7RKhP6/gigabyte-geforce-gtx-1060-6gb-windforce-oc-6g-video-card-gv-n1060wf2oc-6gd) | $268.50 @ B&amp;H 
**Case** | [Cooler Master N200 MicroATX Mid Tower Case](http://pcpartpicker.com/product/T3rG3C/cooler-master-case-nse200kkn1) | $42.99 @ NCIX US 
**Power Supply** | [Corsair CXM 750W 80+ Bronze Certified Semi-Modular ATX Power Supply](http://pcpartpicker.com/product/CVkD4D/corsair-power-supply-cx750m) | $69.89 @ Newegg 
**Wireless Network Adapter** | [Gigabyte GC-WB867D-I PCI-Express x1 802.11a/b/g/n/ac Wi-Fi Adapter](http://pcpartpicker.com/product/tTdqqs/gigabyte-wireless-network-card-gcwb867di) | $29.89 @ OutletPC 
 | *Prices include shipping, taxes, rebates, and discounts* |
 | Total (before mail-in rebates) | $922.23
 | Mail-in rebates | -$30.00
 | **Total** | **$892.23**
 | Generated by [PCPartPicker](http://pcpartpicker.com) 2016-08-25 15:58 EDT-0400 |",false,4zkix7,,0,,false,1473079796,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zkix7/help_with_a_deep_learning_computer_build/,t3_4zkix7,,false,,
1471056402,MachineLearning,llSourcell,youtube.com,https://www.youtube.com/watch?v=iLNHVwSu9EA,5,7,7,0,Build an Antivirus in 5 Minutes (Classifier in Python),,false,4xh9qs,,0,,false,1473041568,false,http://b.thumbs.redditmedia.com/o1sv7jxhuB9SCO-Qyvi5fEtmwZ2y5Q-71PaiY87kmss.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xh9qs/build_an_antivirus_in_5_minutes_classifier_in/,t3_4xh9qs,,false,,
1470203077,MachineLearning,elizabethmonty,einnews.com,http://www.einnews.com/pr_news/337791188/machine-tool-industry-growth-key-manufacturers-and-opportunities-for-global-markets-2016-2021,0,1,1,0,"Machine Tool Industry Growth, Key Manufacturers and Opportunities for Global Markets 2016-2021",,false,4vx029,,0,,false,1473012671,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vx029/machine_tool_industry_growth_key_manufacturers/,t3_4vx029,,false,,
1471434672,MachineLearning,udbladpedes1978,imgur.com,http://imgur.com/0ErFi06,0,1,1,0,Hottest Girls Looking For Sex Adventures Tonight At This Site (check img description),,false,4y54al,,0,,false,1473053679,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y54al/hottest_girls_looking_for_sex_adventures_tonight/,t3_4y54al,,false,,
1470772411,MachineLearning,andrewbarto28,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wy4vi/what_are_the_state_of_the_art_texture_features/,2,0,0,0,What are the state of the art texture features?,,false,4wy4vi,,0,,false,1473031789,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wy4vi/what_are_the_state_of_the_art_texture_features/,t3_4wy4vi,,false,,
1471637083,MachineLearning,greymatter-analytics,allaboutcircuits.com,http://www.allaboutcircuits.com/news/google-is-utilizing-machine-learning-to-cut-its-energy-usage/,0,1,1,0,Google is using Machine Learning to cut its energy usage,,false,4ykyty,,0,,false,1473061702,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ykyty/google_is_using_machine_learning_to_cut_its/,t3_4ykyty,,false,,
1470261637,MachineLearning,datajake,i.redd.it,https://i.redd.it/u6s5jbyto8dx.png,0,0,0,0,7 Tips for Getting Your Colleagues Hooked on R,,false,4w153m,,0,,false,1473014885,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w153m/7_tips_for_getting_your_colleagues_hooked_on_r/,t3_4w153m,,false,,
1472030274,MachineLearning,yazfield,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zbokh/theano_falls_back_to_cpu/,2,0,0,0,Theano falls back to CPU,[removed],false,4zbokh,,0,,false,1473075290,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zbokh/theano_falls_back_to_cpu/,t3_4zbokh,,false,,
1471210994,MachineLearning,Sky_Captain13,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xq8sa/recommended_reading/,0,0,0,0,Recommended Reading?,[removed],false,4xq8sa,,0,,false,1473046141,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xq8sa/recommended_reading/,t3_4xq8sa,,false,,
1471223909,MachineLearning,augustus2010,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xr6bn/overfitting_and_human_brain/,23,7,7,0,Over-fitting and human brain,"I just wonder if there is a over-fitting in human brain. Like when some kids study too much when they are young that they learn slowly or can't even learn when they grow older. Or someone is addicted to something and can't perform other things well. If so, is there any way we can do the dropout to cure this overfit? Maybe the human brain is not overfiting at all because of some mechanism that I am not aware of?",false,4xr6bn,,0,,false,1473046614,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xr6bn/overfitting_and_human_brain/,t3_4xr6bn,,false,,
1472260081,MachineLearning,dataislyfe,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zrxzf/coolest_introduction_to_ml/,12,2,2,0,Coolest introduction to ML?,"I'm teaching an introductory course on machine learning and was wondering if anyone had suggestions for the coolest (short) introduction to machine learning (an animation, video, example, etc.) that you think would really get people interested in the topic? 

EDIT: In particular, I'm looking for a cool introduction to supervised learning",false,4zrxzf,,0,,false,1473083597,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zrxzf/coolest_introduction_to_ml/,t3_4zrxzf,,false,,
1470731439,MachineLearning,perceptron01,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wv5w6/who_are_some_great_researchers_outside_of_north/,31,7,7,0,Who are some great researchers outside of North America?,"It is often recommended to try to do your PhD with someone well known in their field, but it seems like most of the well-known researchers are based somewhere in North America.

Anyone willing to talk about some other great researchers that one should be aware of? I'm most curious about UK, but any country is fine.",false,4wv5w6,,0,,false,1473030277,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wv5w6/who_are_some_great_researchers_outside_of_north/,t3_4wv5w6,,false,,
1470997635,MachineLearning,brockl33,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xcux9/multiple_gpus_for_a_model_too_large_to_fit_on_a/,31,10,10,0,Multiple GPUs for a model too large to fit on a single GPU?,"As far as I am aware multiple GPU setups can be used for training multiple models (one model per GPU) or for training the same model on different GPUs (a single model duplicated to multiple GPUs and combining updates).

Are there any libraries that currently support the use of multiple GPUs in training a large model that cannot fit on a single GPU?

I've read that Theano currently features multi-GPU support with its new libgpuarray backend but I am not sure if this includes the last case.",false,4xcux9,,0,,false,1473039315,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xcux9/multiple_gpus_for_a_model_too_large_to_fit_on_a/,t3_4xcux9,,false,,
1472475979,MachineLearning,rulerofthehell,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/504wjm/how_to_know_when_to_use_a_1d_convolutional_layer/,13,1,1,0,How to know when to use a 1D convolutional layer and when to use a dense layer?,"To my knowledge, 1D convolutional layers are used when we cannot divide the sample into smaller subsets, is there anything more to keep in mind before deciding?",false,504wjm,,0,,false,1473090345,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/504wjm/how_to_know_when_to_use_a_1d_convolutional_layer/,t3_504wjm,,false,,
1471533342,MachineLearning,harrdarr__,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yct9q/how_do_classify_noise/,3,0,0,0,How do classify noise?,"For now i'm clustering the data with [H]DBscan to label datapoints in few clusters and an huge noise/unknown cluster

so i have 19 classes, 18 real and 1 is ""UNKNOWN""


Then I train a SVM to classify new datapoints and assign them to a cluster, it has good results except for the UNKNOWN class which  gives a lot of false positives (e.g. 95% UNKNOWN and 5% target_class) screwing the accuracy

should I avoid training the SVM with the noise class and considering anything which is not &gt;threshold% as noise?

how do i handle noise in general?",false,4yct9q,,0,,false,1473057573,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yct9q/how_do_classify_noise/,t3_4yct9q,,false,,
1471877538,MachineLearning,BrokenGumdrop,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z0vre/training_in_sklearn_testing_in_c/,5,2,2,0,"Training in sklearn, testing in c++?","I'm using the sklearn mlp tool to train a neural network.  I need to use the network in a C++ application.  Are there any premade libraries that can import the network from mlp?    I would use this as an opportunity to use TensorFlow, but the customer is limited to 32 bit linux.  ",false,4z0vre,,0,,false,1473069787,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z0vre/training_in_sklearn_testing_in_c/,t3_4z0vre,,false,,
1471020692,MachineLearning,BlueFolliage,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xei18/newcomer_questions_when_would_you_recommend_using/,0,8,8,0,Newcomer Questions: When would you recommend using Isomaps/Nonlinear Dim. Reduction and how to apply it? Any insight?,"So I'm just getting started with using Isomap, after a professor recommended it, for my research. I'm using stanford's isomap matlab package(http://isomap.stanford.edu) and am getting familiar with it by playing around by using Nba player's PER 36 min from the 2016 season(http://www.basketball-reference.com/leagues/NBA_2016_per_minute.html). My understanding is that by using Isomaps, I can build a model(K-nearest neighbor) based off of my input thats an order of degree(dimensions) much less than the input, allowing for noise to be removed and trends to be discovered. 

My goal is to build a model around player positions, which will learn player positions based on the data provided, and then allow me to input in a new player and determine what kind of position he is most like(ex: Player A, 60% center, 28% pf, 8%pg, ...). So far I ran the isomap program 6 times: once for all the players, and 5 more times for each position where only players who play that position are put in. My output are the residual variances( calculated for K neighbors(2-10, 25, 50, 100) and for each K value, X dimensions(1-10). 

Now that I have this data, I'm really lost on what to do with them next and how to apply the models themselves. Any help, in the form of examples,analogies, or related projects would be very helpful. 

Thanks. ",false,4xei18,,0,,false,1473040161,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xei18/newcomer_questions_when_would_you_recommend_using/,t3_4xei18,,false,,
1472588759,MachineLearning,Wignorant,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50d8gh/how_to_get_a_good_data_scienceml_job_at_a_startup/,40,117,117,0,How to get a good data science/ML job at a startup in the Bay Area?,"I have lots of experience and held several senior positions, but personal issues made me relocate without enough time to find a good job in SF/Bay Area. Ideas? Tips? Suggestions?",false,50d8gh,,0,,false,1473094699,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50d8gh/how_to_get_a_good_data_scienceml_job_at_a_startup/,t3_50d8gh,,false,,
1471738927,MachineLearning,adversarialreviews,twitter.com,https://twitter.com/DeepMindAI/status/766605642345873408,0,1,1,0,20 NIPS papers from DeepMind,,false,4ys49u,,0,,false,1473065351,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ys49u/20_nips_papers_from_deepmind/,t3_4ys49u,,false,,
1470355320,MachineLearning,MasterEpictetus,opendatascience.com,https://www.opendatascience.com/blog/introduction-deep-learning-for-chatbots-part-1/?utm_source=Open+Data+Science+Newsletter&amp;utm_campaign=e947eccaa9-Newsletter_Vol_568_4_2016&amp;utm_medium=email&amp;utm_term=0_2ea92bb125-e947eccaa9-233915461,0,0,0,0,Deep Learning for Chatbots,,false,4w7n1f,,0,,false,1473018225,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w7n1f/deep_learning_for_chatbots/,t3_4w7n1f,,false,,
1471484736,MachineLearning,marcjschmidt,youtube.com,https://www.youtube.com/watch?v=_HsuYywGmXA,0,18,18,0,AETROS supports now monitoring and visualisation of already existing Keras models with two lines of code (instead of using the model designer),,false,4y9k6g,,0,,false,1473055928,false,http://b.thumbs.redditmedia.com/rPKG7Rr3YoaZjjnQ_BwZ8NgAN2jyntwiWpXJYlTmYys.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y9k6g/aetros_supports_now_monitoring_and_visualisation/,t3_4y9k6g,,false,,
1470711759,MachineLearning,iamkeyur,setosa.io,http://setosa.io/ev/image-kernels/,0,1,1,0,Image Kernels explained visually,,false,4wu0f3,,0,,false,1473029690,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wu0f3/image_kernels_explained_visually/,t3_4wu0f3,,false,,
1471235190,MachineLearning,sbt_,sebastianraschka.com,http://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html,0,40,40,0,"Model evaluation, model selection &amp; algorithm selection in machine learning - on stabilty and uncertainty",,false,4xrxnd,,0,,false,1473047001,false,http://b.thumbs.redditmedia.com/R6hV6R8oEbWeiDgvJC0IeQstjc44ZpbBcTTg3a8-BxQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xrxnd/model_evaluation_model_selection_algorithm/,t3_4xrxnd,,false,,
1471500473,MachineLearning,vvpreetham,medium.com,https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670?source=linkShare-613edaa99d02-1471500375,3,24,24,0,How #NeuralNetworks learn complex behaviour - Fundamental Math behind Backpropagation,,false,4yanvg,,0,,false,1473056494,false,http://b.thumbs.redditmedia.com/OZ9eYgqztdZGS0sNvJNFK97e5ENEgaNTh2G9LtN0f6g.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yanvg/how_neuralnetworks_learn_complex_behaviour/,t3_4yanvg,,false,,
1472063469,MachineLearning,andyandy16,arxiv.org,http://arxiv.org/abs/1608.06608,1,3,3,0,[1608.06608] Infinite-Label Learning with Semantic Output Codes,,false,4ze0tm,,0,,false,1473076473,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ze0tm/160806608_infinitelabel_learning_with_semantic/,t3_4ze0tm,,false,,
1470815379,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x11uo/is_anyone_interested_in_a_service_to/,8,0,0,0,Is anyone interested in a service to automatically colorise b&amp;w photos?,[deleted],false,4x11uo,,0,,false,1473033262,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x11uo/is_anyone_interested_in_a_service_to/,t3_4x11uo,,false,,
1472588643,MachineLearning,curryeater259,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50d83l/bias_unit_noob_question/,10,2,2,0,Bias Unit? Noob Question.,"What's up guys. 

I'm working my way through the Coursera course on Machine learning by Andrew Ng, and I'm really confused about the bias unit? In the programming assignment, we always set the parameter for the bias unit to 0, so how does it affect the neural network in any way? Since the bias unit will be multiplied by it's parameter/weight (which seems to always be zero), wouldn't it be completely insignificant? Thanks guys. ",false,50d83l,,0,,false,1473094693,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50d83l/bias_unit_noob_question/,t3_50d83l,,false,,
1472086052,MachineLearning,alxndrkalinin,blog.kaggle.com,http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/,6,2,2,0,"Avito Duplicate Ads Detection, Winners’ Interview: 1st Place (XGBoost + feat engineering)",,false,4zfuw3,,0,,false,1473077412,false,http://b.thumbs.redditmedia.com/HbJBQCQ-McCD4bCXjhHpzi7UEl83qTMtDFalHCrcpvM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zfuw3/avito_duplicate_ads_detection_winners_interview/,t3_4zfuw3,,false,,
1470457422,MachineLearning,jewishsupremacist88,ndtv.com,http://www.ndtv.com/health/artificial-intelligence-used-to-detect-rare-leukemia-type-in-japan-1440789,1,5,5,0,Artificial Intelligence Used To Detect Rare Leukemia Type In Japan,,false,4wedxi,,0,,false,1473021689,false,http://b.thumbs.redditmedia.com/DFZmDxr-S8qOyavvTRsWNdjuRcZx73sRVMnZzbO9XYo.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wedxi/artificial_intelligence_used_to_detect_rare/,t3_4wedxi,,false,,
1470811594,MachineLearning,vmayoral,blog.deeprobotics.es,"http://blog.deeprobotics.es/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/07/10/rl-tutorial/",2,24,24,0,A comprehensive approach to Reinforcement Learning,,false,4x0vdo,,0,,false,1473033173,false,http://b.thumbs.redditmedia.com/NR4bcYmpfqXxtpQwEITifSQOwvB2JwP_scMb48Ik2Oo.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0vdo/a_comprehensive_approach_to_reinforcement_learning/,t3_4x0vdo,,false,,
1471517092,MachineLearning,bbcomp,arxiv.org,http://arxiv.org/abs/1608.03983,16,49,49,0,[1608.03983] SGDR: Stochastic Gradient Descent with Restarts,,false,4ybl6d,,0,,false,1473056958,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ybl6d/160803983_sgdr_stochastic_gradient_descent_with/,t3_4ybl6d,,false,,
1472544539,MachineLearning,pkmital,blog.kadenze.com,https://blog.kadenze.com/2016/08/19/student-tensorflow-gifs/,0,14,14,0,GIFs from Student Submissions to Kadenze Academy's Tensorflow Course - Session 2,,false,50a1cx,,0,,false,1473093031,false,http://b.thumbs.redditmedia.com/G9vg-PXy_z84K0_z6gumglHoCLbN2zIcURonaa7K8ow.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50a1cx/gifs_from_student_submissions_to_kadenze_academys/,t3_50a1cx,,false,,
1472648875,MachineLearning,italartworld001,denonpu.blogspot.in,http://denonpu.blogspot.in/2016/08/polyurethane-machine-processing.html,0,1,1,0,"An efficient PU machine process and work to yield a better reach and excellent supply of foam. These PU injection machines play a key role in supporting the country’s economy and also give the aspiring business people, a great career option.",,false,50h31m,,0,,false,1473096696,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50h31m/an_efficient_pu_machine_process_and_work_to_yield/,t3_50h31m,,false,,
1470418769,MachineLearning,smerity,nlpers.blogspot.com,http://nlpers.blogspot.com/2016/08/fast-easy-baseline-text-categorization.html,5,6,6,0,Fast &amp; easy baseline text categorization with vw,,false,4wblpp,,0,,false,1473020254,false,http://b.thumbs.redditmedia.com/sXjw3iS-DPPXtNb8LvDLSCecd06xB9KU92PZ9Ee3eNw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wblpp/fast_easy_baseline_text_categorization_with_vw/,t3_4wblpp,,false,,
1472500402,MachineLearning,AlNejati,pseudoprofound.wordpress.com,https://pseudoprofound.wordpress.com/2016/08/28/notes-on-the-tensorflow-implementation-of-inception-v3/,0,24,24,0,Notes on the TensorFlow Implementation of Inception v3,,false,506yag,,0,,false,1473091456,false,http://b.thumbs.redditmedia.com/ZmpxQduYNZ1n1CR2mv4uJiO3dEryU6aV-ysGJrqmUCU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/506yag/notes_on_the_tensorflow_implementation_of/,t3_506yag,,false,,
1471386086,MachineLearning,XaroY,youtube.com,https://www.youtube.com/attribution_link?a=0uuvEDKBP4s&amp;u=%2Fwatch%3Fv%3DGj0iyo265bc%26feature%3Dshare,0,0,0,0,Classifying Handwritten Digits with TF.Learn - Machine Learning Recipes #7,,false,4y28bp,,0,,false,1473052222,false,http://b.thumbs.redditmedia.com/N7ix9z6IqoybDaN0oiyv_MWS38Rl885y1GJGoMYQuUM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y28bp/classifying_handwritten_digits_with_tflearn/,t3_4y28bp,,false,,
1472596125,MachineLearning,offswitched,github.com,https://github.com/deankeinan/SouthPark-rnn,2,1,1,0,I fed torch-rnn 19 seasons of South Park and now it writes nonsense,,false,50dua2,,0,,false,1473095008,false,http://b.thumbs.redditmedia.com/DF84of9S62GVDqsTI1iZgEkMuUuiytHTByWl49stfUk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50dua2/i_fed_torchrnn_19_seasons_of_south_park_and_now/,t3_50dua2,,false,,
1470911609,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x7dbx/can_nonlinearities_be_just_skipped_in_the/,1,0,0,0,Can nonlinearities be just skipped in the decoding stage of autoencoders ?,[deleted],false,4x7dbx,,0,,false,1473036488,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x7dbx/can_nonlinearities_be_just_skipped_in_the/,t3_4x7dbx,,false,,
1472636550,MachineLearning,NotThatLebowski,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50gdd6/help_needed_with_audio_analytics/,8,0,0,0,Help Needed with Audio Analytics,"I work in an organization which lends to SMEs and individuals. Before the loan is given, an interviewed is conducted of the potential recipient. I want to do speech analytics of this interviewed audio file and figure out people who might possibly default
Please suggest me how to do speech analytics of this audio file. 

I have few initial thoughts but not able to make any headway

* Making features for every 10 seconds and note down amplitude (or some other parameters) and make a predictive model. 

* Independent Component Analysis of audio files. To be honest, I don’t know what to with this technique and what to look for

* Audio to text conversion. I can make bag of words and use the most recurring words as variables in the logistic regression if the person will default or not. However, I don’t know how to convert audio files into text in R or Python or any other software.

I would really appreciate any help in this regard.

Thanks
",false,50gdd6,,0,,false,1473096324,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50gdd6/help_needed_with_audio_analytics/,t3_50gdd6,,false,,
1470133101,MachineLearning,Gletta,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vrqyo/august_issue_of_computer_vision_news/,1,0,0,0,August issue of Computer Vision News,"http://www.rsipvision.com/ComputerVisionNews-2016August/ 
Everybody can subscribe for free at page 23. Enjoy!",false,4vrqyo,,0,,false,1473009943,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vrqyo/august_issue_of_computer_vision_news/,t3_4vrqyo,,false,,
1470665357,MachineLearning,boccaff,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wqbij/are_random_forests_truly_the_best_classifiers/,59,119,119,0,"Are Random Forests Truly the Best Classifiers? - Response to the ""Do we need a hundred classifiers...""","[Response]( http://jmlr.org/papers/v17/15-374.html) to the ""Do we need hundreds of classifiers to solve real world classification problems"" [(Fernandez-Delgado et al, 2014)](http://www.jmlr.org/papers/v15/delgado14a.html).

The main critic is that they peaked in the test set while tuning and did not perform a unbiased evaluation of papers. The way they excluded classifiers that did not run altered the main ranking and this is discussed as well. Also, statistical procedures for evaluating results are criticized. 

With the new evaluation, ELM with kernel from matlab made to the top and SVM/NN do not seem to be so far from RF.

Related:
[Fast ML critic about the results with boosting](http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/)",false,4wqbij,,0,,false,1473027804,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wqbij/are_random_forests_truly_the_best_classifiers/,t3_4wqbij,,false,,
1472311913,MachineLearning,cvikasreddy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zup59/robotics_courses_for_a_deep_learning_guy/,7,17,17,0,Robotics courses for a Deep Learning guy?,"Hi, 
I have successfully completed cs231n and cs224d of stanford(I am not a student at stanford).

I am also doing a project on Deep Learning.

I just want to know if there are any good courses in Robotics that teach about Reinforcement Learning for Robotics or some Deep Learning for Robotics.",false,4zup59,,0,,false,1473085011,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zup59/robotics_courses_for_a_deep_learning_guy/,t3_4zup59,,false,,
1470688871,MachineLearning,andrewbarto28,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wsdc2/is_it_possible_evaluate_each_input_feature/,10,4,4,0,Is it possible evaluate each input feature importance by analyzing a trained neural net?,"For example, if all outgoing weights of an input neuron are small relative to the other weights, does this mean this input feature is less relevant? Is there a way to do feature selection?",false,4wsdc2,,0,,false,1473028853,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wsdc2/is_it_possible_evaluate_each_input_feature/,t3_4wsdc2,,false,,
1472505473,MachineLearning,__bee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/507e82/question_google_brain_residency_program/,3,11,11,0,[Question] Google Brain Residency Program,"Hi, Did anyone have information on this program and what are the profiles that they are looking for ? 

",false,507e82,,0,,false,1473091684,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/507e82/question_google_brain_residency_program/,t3_507e82,,false,,
1471215804,MachineLearning,the_aligator6,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xqltd/what_technique_to_use_for_binary_classification/,8,0,0,0,What technique to use for binary classification for URI detection?,"Hello,

I am trying to find all links within a given body of text. This includes pdfs, ppts, docx, html, xml, txt, etc.. which are then standardized to a single format and encoding. Given such a document, how can I find all links without relying heavily on heuristics? 

given that a URI must be a single token, would my feature space be all substrings within a given token? should I consider neighbouring tokens? 

Any help would be greatly appreciated.
",false,4xqltd,,0,,false,1473046325,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xqltd/what_technique_to_use_for_binary_classification/,t3_4xqltd,,false,,
1472324726,MachineLearning,insider_7,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zvokz/software_to_create_high_quality_artwork_for_ml/,13,15,15,0,Software to create high quality artwork for ML journals,"I would like to know the software that the ML community uses to create their artwork for their papers.
When I said artwork is everything except performance figures that can easily be done with matplotlib.

For example, which software was used to produce the following figure?
https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg
",false,4zvokz,,0,,false,1473085521,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zvokz/software_to_create_high_quality_artwork_for_ml/,t3_4zvokz,,false,,
1471993946,MachineLearning,anonDogeLover,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z9dui/cifar10_cnn_training_very_quickly_with_sudden/,9,2,2,0,"cifar10 cnn training very quickly, with sudden accuracy bump?","I'm training a convnet on cifar10 in caffe, and I'm a bit puzzled. First, accuracy on the test set (as reported by caffe during training) is 0 for a few minutes, then jumps to around 83% right away. Eventually, hours later, it gets up to 92%. Since I've never played with cifar before, I'm wondering:

(1) Why the sudden jump from 0 to 83%, and why so early?
(2) How long does it usually take to train a cifar net? Compared to imagenet, this is lightning fast. I expected it to be faster, but not this fast.

I know there isn't a lot of detail here, but perhaps common knowledge will solve my ""problem"".",false,4z9dui,,0,,false,1473074116,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z9dui/cifar10_cnn_training_very_quickly_with_sudden/,t3_4z9dui,,false,,
1472600231,MachineLearning,florinandrei,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50e5nd/geoffrey_hintons_neural_networks_for_machine/,3,3,3,0,"Geoffrey Hinton's ""Neural Networks for Machine Learning"" course on Coursera - what's the pre-requisites checklist to go through before taking it?","I've heard it's a great course, and I've also heard some people saying the bayesian statistics part was hard to follow.

So, before I dive in, I'd like to make sure I have all the pre-requisites in terms of math checked out. Perhaps someone could recommend a course to take before Hinton's? Or books to read?

My background is in Physics, so I'm comfortable with doing plenty of math, but I did my studies some years ago, so a few things may have to go through a refresh.

I took Andrew Ng's course on Coursera and I thought the partial derivatives, and the linear algebra part, were pretty easy, and actually quite fun. But for statistics I'd definitely need a refresher.

BTW, does anyone know how many lessons are in Hinton's course, and whether it's self-paced or is it on a strict schedule? I'm trying to figure out how many weeks it would take to complete if I just stick to the default schedule (if any).",false,50e5nd,,0,,false,1473095170,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50e5nd/geoffrey_hintons_neural_networks_for_machine/,t3_50e5nd,,false,,
1470805851,MachineLearning,nishank_varshney,youtube.com,https://www.youtube.com/watch?v=vMlZwQZMwDs,0,5,5,0,Grading codes using machine learning,,false,4x0ktq,,0,,false,1473033026,false,http://a.thumbs.redditmedia.com/IVzF6F8_p0cf0M1yUfqETo3RO6r5KHjhgNfZtQKbeh8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0ktq/grading_codes_using_machine_learning/,t3_4x0ktq,,false,,
1471330428,MachineLearning,mixmachinery,mixmachinery.com,http://www.mixmachinery.com/news/learn-stainless-steel-mixing-vessels.html,1,1,1,0,What can we learn stainless steel mixing vessels？,,false,4xy6xv,,0,,false,1473050177,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xy6xv/what_can_we_learn_stainless_steel_mixing_vessels/,t3_4xy6xv,,false,,
1470657192,MachineLearning,riomus,hub.docker.com,https://hub.docker.com/r/riomus/data-projector/,1,1,1,0,Docker image of data projection web application,,false,4wpr9q,,0,,false,1473027519,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wpr9q/docker_image_of_data_projection_web_application/,t3_4wpr9q,,false,,
1472119290,MachineLearning,pmigdal,freedom-to-tinker.com,https://freedom-to-tinker.com/blog/randomwalker/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/,8,38,38,0,"Language necessarily contains human biases, and so will machines trained on language corpora",,false,4zhrbz,,0,,false,1473078383,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zhrbz/language_necessarily_contains_human_biases_and_so/,t3_4zhrbz,,false,,
1471377158,MachineLearning,omoindrot,github.com,https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/README.md,4,0,0,0,TensorFlow-Slim : better than TFLearn?,,false,4y1h9j,,0,,false,1473051841,false,http://b.thumbs.redditmedia.com/PpGUTv8jzzoAvSrQZgE0ANpjTXfBoLTHlnaWvOsNzWE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y1h9j/tensorflowslim_better_than_tflearn/,t3_4y1h9j,,false,,
1472644760,MachineLearning,[deleted],github.com,https://github.com/arranger1044/awesome-spn,0,1,1,0,awesome-spn a list of resources about Sum-Product Networks,[deleted],false,50gt1d,,0,,false,1473096553,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50gt1d/awesomespn_a_list_of_resources_about_sumproduct/,t3_50gt1d,,false,,
1470845894,MachineLearning,[deleted],nextplatform.com,http://www.nextplatform.com/2016/08/10/nervana-ceo-intel-acquisition-future-technology-outlook/,0,0,0,0,"Nervana CEO Corrects Acquisition Amount, Talks 14nm &amp; SW Stack Future",[deleted],false,4x2xkj,,0,,false,1473034219,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x2xkj/nervana_ceo_corrects_acquisition_amount_talks/,t3_4x2xkj,,false,,
1470944144,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x9upe/unsupervised_document_classification_of_user/,0,1,1,0,Unsupervised document classification of user feedback,[deleted],false,4x9upe,,0,,false,1473037771,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x9upe/unsupervised_document_classification_of_user/,t3_4x9upe,,false,,
1470596677,MachineLearning,jennifercqcq,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wm6ez/does_anyone_know_of_any_resources_for_nlp_for/,2,0,0,0,Does anyone know of any resources for NLP for beginners?,[removed],false,4wm6ez,,0,,false,1473025686,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wm6ez/does_anyone_know_of_any_resources_for_nlp_for/,t3_4wm6ez,,false,,
1470239772,MachineLearning,koormoosh,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vz75e/neural_variational_inference_and_learning_in/,0,1,1,0,Neural Variational Inference and Learning in Belief Network,[removed],false,4vz75e,,0,,false,1473013852,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vz75e/neural_variational_inference_and_learning_in/,t3_4vz75e,,false,,
1470375986,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w8y1h/sentence_tokenizer_for_social_media_text/,4,1,1,0,Sentence tokenizer for Social Media text,"Has anyone implemented a good sentence boundary detection method (also known as sentence tokenizer) for text that does not have proper punctuation, capitalization, uses emojis etc.?",false,4w8y1h,,0,,false,1473018895,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w8y1h/sentence_tokenizer_for_social_media_text/,t3_4w8y1h,,false,,
1470549321,MachineLearning,xingdongrobotics,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wjozn/latex_template_for_arxiv_papers/,11,21,21,0,LaTeX template for arxiv papers ?,It seems most of machine learning papers on arxiv.org use the same latex template. Is there a source of this template ? It looks very like adapted from NIPS conference latex style files ?,false,4wjozn,,0,,false,1473024420,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wjozn/latex_template_for_arxiv_papers/,t3_4wjozn,,false,,
1471081303,MachineLearning,savs95,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xig4g/open_problems_in_optimizations_for_machine/,7,5,5,0,Open problems in optimizations for machine learning?,"I have to choose a semester long project, and I am planning to attempt some open doable problem in non-convex optimization field. I came across this wonderful link by Anima Anandkumar 

https://www.quora.com/What-are-some-recent-advances-in-non-convex-optimization-research

I am looking for papers / conferences which point to some open problems, mostly theoretical. Like finding a heuristic, or proving a bound, etc, which could result in a publication.

I am a computer science masters student.
",false,4xig4g,,0,,false,1473042167,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xig4g/open_problems_in_optimizations_for_machine/,t3_4xig4g,,false,,
1470989103,MachineLearning,TuringsEgo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xchfs/curriculum_learning_self_paced_learning_and_self/,7,2,2,0,"Curriculum learning, self paced learning, and self paced curriculum learning","Does anyone know where I could find some source code for such topics? They look extremely interesting and seem like they would give my project an extra boost in performance. ",false,4xchfs,,0,,false,1473039125,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xchfs/curriculum_learning_self_paced_learning_and_self/,t3_4xchfs,,false,,
1472479422,MachineLearning,sabse_bada_intellect,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/5055gd/need_help_on_a_problem/,1,0,0,0,Need help on a problem !!!,"So I have some data for spread of  H1N1. The three major categories of people are Uninfected, Infected &amp; Carriers.
This is a temporal data as in the tests were conducted monthly (Month_0, Month_1)

The dynamics of the flow are like this :-

1.] UnInfected &lt;--&gt; Carrier ( Bidirectional)

2.] Infected &lt;--&gt; Carrier ( Bidirectional)

3.] Uninfected --&gt; Carrier ( Unidirectional)

UnInfected+ Carrier+Infected = 9500 ( i.e. the same individuals were tested for Month_0 &amp; Month_1). Each patient record is a 244rowsX10columns matrix , ie values of 244 parameters.

So I have the data for month_0 for all these three states &amp; then for month_1.

Now Ive to develop a predictive model which could classify a new patient into these categories.
Can anyone please help as to which algorithm /models will work on this problem ?",false,5055gd,,0,,false,1473090481,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/5055gd/need_help_on_a_problem/,t3_5055gd,,false,Discusssion,
1472049626,MachineLearning,Maleficus187,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zcw7c/why_does_the_machine_learning_community_seem_to/,8,2,2,0,Why does the machine learning community seem to use single-precision rather than double?,"I'm a computational chemist by trade, so I am no expert in machine learning, but why does the community seem to use single-precision rather than double-precision so often? Is it due to the handicap of double-precision on affordable GPU's, or just no need for double-precision in most cases?",false,4zcw7c,,0,,false,1473075898,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zcw7c/why_does_the_machine_learning_community_seem_to/,t3_4zcw7c,,false,,
1470592454,MachineLearning,jdsutton,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wlv4a/how_would_you_solve_this/,9,2,2,0,How would you solve this?,"On the spiral dataset here: http://playground.tensorflow.org/

In all my efforts I'm able to classify within the spiral pretty well, but if in my test data the pattern were to continue for a few more loops the model would not generalize to that. A human would easily be able to spot this pattern and have no trouble continuing it. How can I generalize better?",false,4wlv4a,,0,,false,1473025526,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wlv4a/how_would_you_solve_this/,t3_4wlv4a,,false,,
1470601701,MachineLearning,calclearner,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wmk4s/interesting_results_for_nlp_using_htm/,26,2,2,0,Interesting results for NLP using HTM,"Hey guys! I know a lot of you are skeptical of Numenta and HTM. Since I am new to this field, I am also a bit skeptical based on what I've read.

However, I would like to point out that [cortical](http://www.cortical.io/), a startup, has achieved some interesting results in NLP using HTM-like algorithms. They have quite a few demos. Thoughts?",false,4wmk4s,,0,,false,1473025881,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4wmk4s/interesting_results_for_nlp_using_htm/,t3_4wmk4s,,false,Discusssion,
1470947843,MachineLearning,__bee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xa5yb/question_installation_and_performance_of_caffe_on/,3,1,1,0,[Question] Installation and performance of Caffe on server wihtout GPU,"I am installing Caffe on server without support of GPU. I didn't find a lot of information on the performance of Caffe on server without GPU. Does anyone here have experience on using that in these conditions. ",false,4xa5yb,,0,,false,1473037935,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xa5yb/question_installation_and_performance_of_caffe_on/,t3_4xa5yb,,false,,
1470948620,MachineLearning,nnever,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xa88a/nonml_what_are_you_hobbies/,45,10,10,0,[Non-ML] What are you hobbies?,"I'm asking this in this subreddit because, I assume, most people that are here are math, statistics, computer science, -savy; so predominantly the proverbial left-brainers*. I'm curious to see if more such people have hobbies that are closer related to those fields (web design for example) or something unrelated.

This is just out of curiosity and for fun, but maybe we'll get a statistically significant result.

**I'm using this to group like-minded people (yes, I could've used a better word), don't think it's because I believe in the left-right brain fad.*",false,4xa88a,,0,,false,1473037967,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4xa88a/nonml_what_are_you_hobbies/,t3_4xa88a,,false,Discusssion,
1470287886,MachineLearning,MaxTalanov,github.com,https://github.com/fchollet/deep-learning-models,2,20,20,0,Code and weights files for popular deep learning models,,false,4w2y82,,0,,false,1473015819,false,http://b.thumbs.redditmedia.com/LkQ1PgV6NtI7Qj-eV2iOKFbFYWxm6HafJrHp0ZPfSXg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w2y82/code_and_weights_files_for_popular_deep_learning/,t3_4w2y82,,false,,
1471768009,MachineLearning,ahousley,seldon.typeform.com,https://seldon.typeform.com/to/iHumR4?source=reddit,0,0,0,0,Quick 5-minute survey on machine learning,,false,4ytvyx,,0,,false,1473066240,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ytvyx/quick_5minute_survey_on_machine_learning/,t3_4ytvyx,,false,,
1472194128,MachineLearning,cptncrnch,svds.com,http://www.svds.com/learning-imbalanced-classes/,9,121,121,0,Learning from Imbalanced Classes,,false,4znalf,,0,,false,1473081212,false,http://b.thumbs.redditmedia.com/YSx5aJr1uCYRbBXsP657jwxeis3FmZNr96idaIeJQRw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4znalf/learning_from_imbalanced_classes/,t3_4znalf,,false,,
1470317658,MachineLearning,fhuszar,inference.vc,http://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/,11,41,41,0,"Notes on infoGANs (information maximising GANs), and the variational lower bound they use",,false,4w4irz,,0,,false,1473016623,false,http://b.thumbs.redditmedia.com/7E34acow8wF5GNFd7FDVWONKj_G8c0QFw4LNmQ-qWRE.jpg,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4w4irz/notes_on_infogans_information_maximising_gans_and/,t3_4w4irz,,false,Research,
1471801409,MachineLearning,akorchemniy,youtube.com,https://www.youtube.com/watch?v=VKjFpmYFEn8,0,1,1,0,Using text-to-speech to keep up with machine learning and ai progress,,false,4yw1tm,,0,,false,1473067346,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yw1tm/using_texttospeech_to_keep_up_with_machine/,t3_4yw1tm,,false,,
1470179255,MachineLearning,jay_mahadeokar,github.com,https://github.com/jay-mahadeokar/pynetbuilder,0,1,1,0,"Python tool for generating caffe networks, with examples to generate Residual network classification and object detection networks.",,false,4vvf1s,,0,,false,1473011855,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vvf1s/python_tool_for_generating_caffe_networks_with/,t3_4vvf1s,,false,,
1471807115,MachineLearning,[deleted],i.redd.it,https://i.redd.it/lhppdctncsgx.jpg,2,0,0,0,how to trigger trump supporters (machine learning),[deleted],false,4ywk4q,,0,,false,1473067600,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ywk4q/how_to_trigger_trump_supporters_machine_learning/,t3_4ywk4q,,false,,
1470601075,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wmier/htm_and_its_validity/,0,1,1,0,HTM and its validity,[deleted],false,4wmier,,0,,false,1473025856,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wmier/htm_and_its_validity/,t3_4wmier,,false,,
1472030561,MachineLearning,nomaderx,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zbp2j/my_paper_based_on_my_master_thesis_got_rejected/,4,4,4,0,My paper (based on my master thesis) got rejected by ICCV last year. Should I still upload it on arxiv?,[removed],false,4zbp2j,,0,,false,1473075297,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zbp2j/my_paper_based_on_my_master_thesis_got_rejected/,t3_4zbp2j,,false,,
1470814664,MachineLearning,muoro,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x10mg/the_mathematics_of_machine_learning/,51,171,171,0,The Mathematics of Machine Learning,"By AKINFADERIN ADEWALE

In the last few months, I have had several people contact me about their enthusiasm for venturing into the world of data science and using Machine Learning (ML) techniques to probe statistical regularities and build impeccable data-driven products. However, I’ve observed that some actually lack the necessary mathematical intuition and framework to get useful results. This is the main reason I decided to write this blog post. Recently, there has been an upsurge in the availability of many easy-to-use machines and deep learning packages such as sick it-learn, Weka, Tensorflow etc. Machine Learning theory is a field that intersects statistical, probabilistic, computer science and algorithmic aspects arising from learning iteratively from data and finding hidden insights which can be used to build intelligent applications. Despite the immense possibilities of Machine and Deep Learning, a thorough mathematical understanding of many of these techniques is necessary for a good grasp of the inner workings of the algorithms and getting good results.

Why Worry About The Maths?

There are many reasons why the mathematics of Machine Learning is important and I’ll highlight some of them below:

1. Selecting the right algorithm which includes giving  considerations to accuracy, training time, model complexity, the number of parameters and number of features.

2. Choosing parameter settings and validation strategies.

3. Identifying underfitting and overfitting by understanding the Bias-Variance tradeoff.

4. Estimating the right confidence interval and uncertainty.

What Level of Maths Do You Need?

The main question when trying to understand an interdisciplinary field such as Machine Learning is the amount of maths necessary and the level of maths needed to understand these techniques. The answer to this question is multidimensional and depends on the level and interest of the individual. Research in mathematical formulations and theoretical advancement of Machine Learning is ongoing and some researchers are working on more advanced techniques. I’ll state what I believe to be the minimum level of mathematics needed to be a Machine Learning Scientist/Engineer and the importance of each mathematical concept.


1. Linear Algebra: Someone recently said that “Linear Algebra is the mathematics of the 21st century” and I totally agree with the statement. In ML, Linear Algebra comes up everywhere. Topics such as Principal Component Analysis (PCA), Singular Value Decomposition (SVD), Eigendecomposition of a matrix, LU Decomposition, QR Decomposition/Factorization, Symmetric Matrices, Orthogonalization &amp; Orthonormalization, Matrix Operations, Projections, Eigenvalues &amp; Eigenvectors, Vector Spaces and Norms are needed for understanding the optimization methods used for machine learning. The amazing thing about Linear Algebra is that there are so many online resources. I have always said that the traditional classroom is dying because of the vast amount of resources available on the internet. My favorite Linear Algebra course is the one offered by MIT Courseware (Prof. Gilbert Strang).

2. Probability Theory and Statistics: Machine Learning and Statistics aren’t very different fields. Actually, someone recently defined Machine Learning as ‘doing statistics on a Mac’. Some of the fundamental Statistical and Probability Theory needed for ML are Combinatorics, Probability Rules &amp; Axioms, Bayes’ Theorem, Random Variables, Variance and Expectation, Conditional and Joint Distributions, Standard Distributions (Bernoulli, Binomial, Multinomial, Uniform and Gaussian), Moment Generating Functions, Maximum Likelihood Estimation (MLE), Prior and Posterior, Maximum a Posteriori Estimation (MAP) and Sampling Methods.

3. Multivariate Calculus: Some of the necessary topics include Differential and Integral Calculus, Partial Derivatives, Vector-Values Functions, Directional Gradient, Hessian, Jacobian, Laplacian and Lagragian Distribution.

4. Algorithms and Complex Optimizations: This is important for understanding the computational efficiency and scalability of our Machine Learning Algorithm and for exploiting sparsity in our datasets. Knowledge of data structures (Binary Trees, Hashing, Heap, Stack etc), Dynamic Programming, Randomized &amp; Sublinear Algorithm, Graphs, Gradient/Stochastic Descents and Primal-Dual methods are needed.

5. Others: This comprises of other Math topics not covered in the four major areas described above. They include Real and Complex Analysis (Sets and Sequences, Topology, Metric Spaces, Single-Valued and Continuous Functions, Limits), Information Theory (Entropy, Information Gain), Function Spaces and Manifolds.

Some online MOOCs and materials for studying some of the Mathematics topics needed for Machine Learning are:

Khan Academy’s Linear Algebra, Probability &amp; Statistics, Multivariable Calculus, and Optimization.
Coding the Matrix: Linear Algebra through Computer Science Applications by Philip Klein, Brown University.
Linear Algebra – Foundations to Frontiers by Robert van de Geijn, University of Texas.
Applications of Linear Algebra, Part 1 and Part 2. A newer course by Tim Chartier, Davidson College.
Joseph Blitzstein – Harvard Stat 110 lectures
Larry Wasserman’s book – All of the statistics: A Concise Course in Statistical Inference .
Boyd and Vandenberghe’s course on Convex optimization from Stanford.
Linear Algebra – Foundations to Frontiers on edX.
Udacity’s Introduction to Statistics.
Finally, the main aim of this blog post is to give a well-intentioned advice about the importance of Mathematics in Machine Learning and the necessary topics and useful resources for a mastery of these topics. However, some Machine Learning enthusiasts are the novice in Maths and will probably find this post disheartening (seriously, this is not my aim). For beginners, you don’t need a lot of Mathematics to start doing Machine Learning. The fundamental prerequisite is data analysis as described in this blog post and you can learn maths on the go as you master more techniques and algorithms.

To know more about machine learning please subscribe our newsletter www.muoro.io",false,4x10mg,,0,,false,1473033245,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x10mg/the_mathematics_of_machine_learning/,t3_4x10mg,,false,,
1471263718,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xtabw/what_is_the_best_resource_to_date_on_topic/,3,8,8,0,"What is the best resource to date on ""topic modeling""?","I know David Blei has some great papers---any other recommendations? ",false,4xtabw,,0,,false,1473047683,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xtabw/what_is_the_best_resource_to_date_on_topic/,t3_4xtabw,,false,,
1471173583,MachineLearning,obsoletelearner,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xnqkw/basic_question_why_is_the_covariance_matrix_of/,16,7,7,0,Basic question: Why is the covariance matrix of the multivariate gaussian distribution visualized as an ellipse?,"Hi, 

I'm just beginning to learn [Multivariate Gaussian Distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) and confidence intervals, In this post http://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/ 

 The author explains that the covariance matrix is visualized as an ellipse, but i do not understand the intuition behind it. 

Can anyone please help, Thanks. ",false,4xnqkw,,0,,false,1473044860,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xnqkw/basic_question_why_is_the_covariance_matrix_of/,t3_4xnqkw,,false,,
1471953126,MachineLearning,hassanzadeh,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z62lk/what_is_a_good_reference_for_rmsprop_method/,4,1,1,0,What is a good reference for rmsprop method,"Hello,
Any one can mention the paper which introduced rmsprop optiomization method?
I'm using it in my deep learning pipeline.",false,4z62lk,,0,,false,1473072430,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z62lk/what_is_a_good_reference_for_rmsprop_method/,t3_4z62lk,,false,,
1470419254,MachineLearning,stas_sl,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wbn81/are_deconvolution_layers_actually_needed/,16,7,7,0,Are deconvolution layers actually needed?,"Hi!

There two quite similar net architectures for semantic segmentation task:

1) [Learning Deconvolution Network for Semantic Segmentation](http://arxiv.org/abs/1505.04366)

2) [SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](http://arxiv.org/abs/1511.00561)


They both use similar VGG network for encoding part, but they differ a bit in decoding part. Though, still it is quite similar. They use the same layers as VGG net, but in reverse order, replacing max pooling operations with unpooling (remembering max element index). The difference is that in the first paper they use deconv layers (with stride 1), so it doesn't change spatial dimensions, while in the second paper they use just regular conv layers, thus they don't need deconv layers at all.

Before it was more logical for me that if you use convolution layers in encoder part going from input pixels to more abstract features, then you should use kind of reverse operation like ""deconvolution"" in the decoder part, which will take a single coarse element as input, multiply it by some kernel (e.g. 3x3) and a produce bigger output.

Though, as it turns out in SegNet paper, you don't actually need deconv layers, which is not very intuitive for me.

One case, when I thought deconv layers can be useful is with stride &gt; 1, when they actually upsample input. But in these papers, they always use stride=1 for all conv/deconv layers and all upsampling is done using similar unpooling layers.


So I see 3 options:

1. unpooling + conv
2. unpooling + deconv (stride 1)
3. deconv (stride &gt; 1)

Is any of them more preferable than the others, or they yield effectively the same result?
",false,4wbn81,,0,,false,1473020278,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wbn81/are_deconvolution_layers_actually_needed/,t3_4wbn81,,false,,
1471625630,MachineLearning,AElowsson,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yk0by/using_tanh_activation_functions_for_the_first/,12,7,7,0,Using tanh activation functions for the first hidden layer and ReLUs for subsequent hidden layers,"I have been using tanh activation functions for my first hidden layer and ReLUs for the subsequent hidden layers for a while now, based on my observations that it seems to work slightly better than other variations. Presented a paper last week with this setup [Beat Tracking with a Cepstroid Invariant Neural Network](https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/252_Paper.pdf) (though the machine learning is rather simple by the standards of this forum). Anyway, a researcher from Apple came by my poster and it became apparent that they are doing exactly this too.

**Is this a common setup?**

I mean it could be motivated in various ways, e.g.

* Less risk of vanishing gradients, as opposed to when tanh units are used in subsequent layers

* The sparsity induced by ReLUs could arguably be more relevant at later layers of processing (i.e. nice to use smooth activations for edges but more relevant to disentangle factors of variation for high-level representations).
",false,4yk0by,,0,,false,1473061210,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yk0by/using_tanh_activation_functions_for_the_first/,t3_4yk0by,,false,,
1471210861,MachineLearning,dielugi,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xq8gn/is_machine_learning_the_best_approach_to_do/,2,0,0,0,"Is machine learning the best approach to do ""product matching"" across different ecommerce sites ?","I wonder machine learning the best approach to find if two products in two different websites are exactly the same one ? If that is the case, what do you think is the best approach to the problem ?

For example, let's say you have a list of Amazon products with their ASIN numbers (Amazon Standard Identification Number), product name, and UPC. 

If you wanted to find the exact same product on Walmart.com or HomeDepot.com, what would the best approach be ? (by the way, the tricky part of doing product matching is that some manufacturers may use different UPC codes and names on different sites for the exact same product)

Here are a couple of articles to give you some context:
https://www.semantics3.com/blog/2015/04/12/product-matching-youve-not-heard-of-it-but-its-powering-your-price-comparison-engine/
https://www.promptcloud.com/blog/Product-Matching-e-Commerce-Success-Stories


",false,4xq8gn,,0,,false,1473046137,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xq8gn/is_machine_learning_the_best_approach_to_do/,t3_4xq8gn,,false,,
1470777622,MachineLearning,manionthecat,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wyl7v/cases_where_local_optimums_are_considered/,8,0,0,0,Cases where local optimums are considered superior to global optimums?,"Hi folks,
I was studying Pedro Domingo's ""A Few Useful Things to Know about Machine Learning"" and ran across what seems to be a very counter intuitive assertion in Part 3 (It's Generalization that Counts)

Regarding the last statement(**bolded**):

""Notice that generalization being the goal has an interesting
consequence for machine learning. Unlike in most other optimization
problems, we don’t have access to the function
we want to optimize! We have to use training error as a surrogate
for test error, and this is fraught with danger. How
to deal with it is addressed in some of the next sections. On
the positive side, since the objective function is only a proxy
for the true goal, we may not need to fully optimize it; **in
fact, a local optimum returned by simple greedy search may
be better than the global optimum.**""

This goes against everything I've learned so far (admittedly, limited to Andrew Ng's course)  How could an algorithm with a higher cost function be superior to one with a lower cost function?  is this simply a matter of the cost function not being the end measure of success, but rather success on test data sets?

thanks in advance for commenting.",false,4wyl7v,,0,,false,1473032020,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wyl7v/cases_where_local_optimums_are_considered/,t3_4wyl7v,,false,,
1470952136,MachineLearning,billyjayjohnson3416,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xaif7/newcomer_question_what_is_the_best_image_file/,2,3,3,0,Newcomer Question: What is the best image file format for deep learning?,"Image Capture Options:
1. Mono 8
2. Mono 12
3. Mono 16
4. Raw 8
5. Raw 12
6. Raw 16
7. YUV 411
8. YUV 422
9. YUV 444
10. RGB 8",false,4xaif7,,0,,false,1473038113,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xaif7/newcomer_question_what_is_the_best_image_file/,t3_4xaif7,,false,,
1471221007,MachineLearning,rerevelcgnihtemos,arxiv.org,http://arxiv.org/abs/1608.03824,0,1,1,0,Perceptual Reward Functions,,false,4xqyv4,,0,,false,1473046511,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xqyv4/perceptual_reward_functions/,t3_4xqyv4,,false,,
1471484138,MachineLearning,julian88888888,iamtrask.github.io,https://iamtrask.github.io/2016/08/17/grokking-deep-learning/,21,22,22,0,Grokking Deep Learning,,false,4y9ika,,0,,false,1473055906,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y9ika/grokking_deep_learning/,t3_4y9ika,,false,,
1471911070,MachineLearning,adamnemecek,arxiv.org,https://arxiv.org/pdf/1511.09230v1.pdf,1,5,5,0,A Type Theory for Probabilistic and Bayesian Reasoning,,false,4z3naf,,0,,false,1473071195,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z3naf/a_type_theory_for_probabilistic_and_bayesian/,t3_4z3naf,,false,,
1470893893,MachineLearning,retcase,arxiv.org,http://arxiv.org/abs/1608.02732,12,22,22,0,Shots fired! This paper claims that some influential results in RL/OL are wrong.,,false,4x6ivm,,0,,false,1473036060,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x6ivm/shots_fired_this_paper_claims_that_some/,t3_4x6ivm,,false,,
1472272697,MachineLearning,swiz0r,github.com,https://github.com/paarthneekhara/text-to-image,10,114,114,0,Text To Image Synthesis Using Thought Vectors,,false,4zsqbm,,0,,false,1473083999,false,http://a.thumbs.redditmedia.com/ChLdRqzhtRMAM_5RFcWVX_tfh_u-9BJtFTu7wonaz04.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zsqbm/text_to_image_synthesis_using_thought_vectors/,t3_4zsqbm,,false,,
1472095891,MachineLearning,textanalytics,rxnlp.com,http://www.rxnlp.com/api-reference/topics-and-themes-api-reference/,0,1,1,0,Cloud based API for extracting topics from text with supporting sentences,,false,4zgjuu,,0,,false,1473077764,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zgjuu/cloud_based_api_for_extracting_topics_from_text/,t3_4zgjuu,,false,,
1472183211,MachineLearning,flyforlight,github.com,https://github.com/daijifeng001/MNC,0,6,6,0,"Source code available for Multi-task Network Cascades, the first-place entry of COCO 2015 segmentation challenge",,false,4zmoau,,0,,false,1473080898,false,http://b.thumbs.redditmedia.com/uYR8mqtrLrGuwE4tQuj75_KlzX8JJkYSuGvyRAUBn7M.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zmoau/source_code_available_for_multitask_network/,t3_4zmoau,,false,,
1470927801,MachineLearning,computervision,ted.com,http://www.ted.com/talks/anthony_goldbloom_the_jobs_we_ll_lose_to_machines_and_the_ones_we_won_t,0,0,0,0,The jobs we'll lose to machines and the ones we won't,,false,4x8fm1,,0,,false,1473037043,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x8fm1/the_jobs_well_lose_to_machines_and_the_ones_we/,t3_4x8fm1,,false,,
1470677130,MachineLearning,jackerfrinandis,articles.org,http://articles.org/3-aspects-to-consider-while-purchasing-hydraulic-components/,0,1,1,0,3 Aspects To Consider While Purchasing Hydraulic Components,,false,4wraw4,,0,,false,1473028305,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wraw4/3_aspects_to_consider_while_purchasing_hydraulic/,t3_4wraw4,,false,,
1470240688,MachineLearning,andrewbarto28,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vza3g/are_there_cases_where_it_makes_sense_to_use_a_rnn/,4,2,2,0,Are there cases where it makes sense to use a RNN to classify sequences of fixed length or should I always use a feed-forward architecture in these cases? Suppose a length of 30 for instance.,,false,4vza3g,,0,,false,1473013897,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vza3g/are_there_cases_where_it_makes_sense_to_use_a_rnn/,t3_4vza3g,,false,,
1470952959,MachineLearning,shash273,github.com,https://github.com/shashankg7/Sparse-Autoencoder,0,3,3,0,Sparse Autoencoder in theano,,false,4xaktf,,0,,false,1473038146,false,http://b.thumbs.redditmedia.com/bGFQnT1CwtBU_J41LzAPGHEaLWEaujEFlAC7eVF-0aQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xaktf/sparse_autoencoder_in_theano/,t3_4xaktf,,false,,
1472260839,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zrzt9/should_i_abandon_this_project/,1,1,1,0,Should I abandon this project?,[deleted],false,4zrzt9,,0,,false,1473083623,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zrzt9/should_i_abandon_this_project/,t3_4zrzt9,,false,,
1472278365,MachineLearning,[deleted],kvfrans.com,http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/,5,0,0,0,An intuitive explanation of natural gradient descent,[deleted],false,4zt168,,0,,false,1473084151,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zt168/an_intuitive_explanation_of_natural_gradient/,t3_4zt168,,false,,
1471872554,MachineLearning,[deleted],hunanpipe.com,http://www.hunanpipe.com/news/productshow140-234.html,0,1,1,0,"Due to the corrosion and various other problems in iron, industries started to use steel tubes as the stainless steel tubes have chromium that makes it corrosion resistant and prevent it from being deteriorated. Now, these tubes are widely used in almost every industry and structural functions.",[deleted],false,4z0ivg,,0,,false,1473069595,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z0ivg/due_to_the_corrosion_and_various_other_problems/,t3_4z0ivg,,false,,
1471988725,MachineLearning,[deleted],nextplatform.com,http://www.nextplatform.com/2016/08/23/fpga-based-deep-learning-accelerators-take-asics/,4,1,1,0,"FPGA Based Deep Learning Accelerators Could Take on ASICs, GPUs, DSPs, etc...",[deleted],false,4z8yyn,,0,,false,1473073907,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z8yyn/fpga_based_deep_learning_accelerators_could_take/,t3_4z8yyn,,false,,
1472430809,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/502bvr/besides_using_exifmetadata_whats_a_good_approach/,0,1,1,0,"Besides using exif/metadata, whats a good approach to quality control photographs so that everyone understands the expectations?",[removed],false,502bvr,,0,,false,1473088985,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/502bvr/besides_using_exifmetadata_whats_a_good_approach/,t3_502bvr,,false,,
1471412329,MachineLearning,ikedude8,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y3zlu/opengl_lstm_benchmark_help/,0,1,1,0,OpenGL LSTM Benchmark Help,[removed],false,4y3zlu,,0,,false,1473053110,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y3zlu/opengl_lstm_benchmark_help/,t3_4y3zlu,,false,,
1472396107,MachineLearning,JDude13,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zzp7e/beginner_when_using_gradient_descent_whats_the/,1,3,3,0,"[Beginner] When using gradient descent, what's the best choice for the gradient coefficient (gamma, by the Wikipedia article)?",[removed],false,4zzp7e,,0,,false,1473087605,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zzp7e/beginner_when_using_gradient_descent_whats_the/,t3_4zzp7e,,false,,
1471019407,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xee64/should_i_do_a_masters_in_ml/,0,1,1,0,Should I do a masters in ML?,[removed],false,4xee64,,0,,false,1473040105,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xee64/should_i_do_a_masters_in_ml/,t3_4xee64,,false,,
1472076127,MachineLearning,bha_esac,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zf3bh/data_access_agreement/,0,2,2,0,Data access agreement,[removed],false,4zf3bh,,0,,false,1473077017,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zf3bh/data_access_agreement/,t3_4zf3bh,,false,,
1472613482,MachineLearning,Bashizzle,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50f50q/ml_noob_detecting_noise_vs_human_speech_then/,3,1,1,0,"ML noob - detecting noise vs. human speech, then denoising. Help?","I'm new to ML and have been studying Python for a few months. For context, I'm a self-taught web developer that is really interested in ML. I also have a professional audio background in audio post-production.

What I'm interested in trying out is automated denoising of an audio file. I want it to be able to distinguish between background noise and speech. It doesn't need to recognize the language, only needs to recognize noise vs. human speech. 

I'm not sure where to start on this exactly or what it would look like in the end. Here's my educated guess so far:

1. Use supervised ML to learn distinguish between noise and human speech. 
2. Write audio denoiser in MATLAB. 
3. Turn that into an executable file.
4. Automate so an audio file can be input, the machine distinguishes between noise and speech, then the denoiser automatically reduces the background noise.
5. Output the file at the same sample rate

Again, I don't have a degree in programming so it would take significant time for me to do all of this myself. I'm just trying to figure out how to do it (what languages it would need to be coded in, how long it would take 1 or 2 good developers, how much ""horsepower"" I would need for ML of audio files, etc).

I appreciate any help! :)",false,50f50q,,0,,false,1473095676,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50f50q/ml_noob_detecting_noise_vs_human_speech_then/,t3_50f50q,,false,,
1470566975,MachineLearning,Arech,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wkeu5/how_to_measure_if_a_feature_helps_to_classify/,5,6,6,0,How to measure if a feature helps to classify data with a feedforward NN?,"Hello everybody!
I wonder if there're techniques to measure how helpful some particular feature in solving classification problem with a feedforward neural network?

The straightforward approach is to evaluate NN performance on two datasets (with and without the feature) and compare results. However, there're two issues with it: 1) NN training is a rather costly process (at least in terms of time – and let's fix that we can't switch to a more powerful computing unit that will require significantly less time to train NN); 2) NN training heavily depends on weights initialization, which is usually stochastic process, therefore NN training process most of the time finds different suboptimal local extremums in an error surface, i.e. yields different performance every training session. So, it leads to a conclusion, that the straightforward approach will require a fairly large number of costly NN training sessions to draw a probabilistically correct decision.
However, on a first glance it looks like the only 100%-correct way to find out if a feature helps. I wonder if there’re more clever approaches available?
It’s not a strict requirement for a solution to be a 100%-correct. It’s okay if it “almost always” correct, or even “sometimes correct” provided that it almost never marks a helpful feature as useless.

If one to compute loss function derivatives w.r.t. NN input features (X data) over the whole dataset for a trained network, is it safe to say that features with a bigger mean/median absolute derivative value are more helpful than those feature with a smaller corresponding statistic? (provided that features were normalized under the same strategy). Could this approach be used to rank feature “helpfulness”? What are the tricks and drawbacks?

UPD1: I'm aware about L1-penalty technique. It's still very computationally expensive, though a bit better than the straightforward approach.
",false,4wkeu5,,0,,false,1473024783,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wkeu5/how_to_measure_if_a_feature_helps_to_classify/,t3_4wkeu5,,false,,
1470758991,MachineLearning,Hamush,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wwyc7/hey_experts_what_about_dem_markov_chains/,6,0,0,0,Hey Experts! What about dem Markov chains?,"I'm wondering how to use a neural net to to predict words based on context... But there's a twist! I would like the prediction word to be ""simpler"" or more common than the one it's replacing.

So for example you have a sentence like such: Be cognizant of products you frequently use

And the neural net would be tasked with changing the sentence into: Be AWARE of products you OFTEN use

Of course there would be another algorithm deciding which words are ""difficult"" and require changing.

For what it's worth, let's say I have a way to determine simplicity of words, I just need words to rank. This is what I imagine the ""work flow"" so to speak will look like:

1. Mark words that need to be simplified 
2. Feed the sentence into the neural net
3. Get neural net replacement suggestions
4. Rank the replacements from hardest to simplest and use the simplest one in the sentence

Thanks in advance for the help!",false,4wwyc7,,0,,false,1473031187,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wwyc7/hey_experts_what_about_dem_markov_chains/,t3_4wwyc7,,false,,
1472539414,MachineLearning,sealince,kefid.com,http://www.kefid.com/v3/Solution/Grinding_plant/,0,1,1,0,"Marble is composed of pure calcite and is extensively used for sculpture, and as a building material.",,false,509s8t,,0,,false,1473092903,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/509s8t/marble_is_composed_of_pure_calcite_and_is/,t3_509s8t,,false,,
1470059897,MachineLearning,bdamos,arxiv.org,https://arxiv.org/abs/1506.01066,0,24,24,0,[1506.01066] Visualizing and Understanding Neural Models in NLP,,false,4vmma1,,0,,false,1473007251,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4vmma1/150601066_visualizing_and_understanding_neural/,t3_4vmma1,,false,Research,
1471800249,MachineLearning,downtownslim,arxiv.org,https://arxiv.org/abs/1607.06450,7,0,0,0,[1607.06450] Layer Normalization,,false,4yvy86,,0,,false,1473067296,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4yvy86/160706450_layer_normalization/,t3_4yvy86,,false,Research,
1471803082,MachineLearning,pogopuschel_,wildml.com,http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/,2,53,53,0,"RNNs in Tensorflow, a Practical Guide and Undocumented Features",,false,4yw76v,,0,,false,1473067421,false,http://b.thumbs.redditmedia.com/6QkDde7bjikcvJJASN6PwYuQioEOU6GCq40WwtcHc1I.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yw76v/rnns_in_tensorflow_a_practical_guide_and/,t3_4yw76v,,false,,
1471003194,MachineLearning,rshah4,projects.rajivshah.com,http://projects.rajivshah.com/sportvu/Traj_RNN.html,5,32,32,0,Using Tensorflow and LSTMs to predict Basketball Trajectories,,false,4xd5gn,,0,,false,1473039464,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xd5gn/using_tensorflow_and_lstms_to_predict_basketball/,t3_4xd5gn,,false,,
1470583377,MachineLearning,visarga,www2016.net,http://www2016.net/proceedings/proceedings/p145.pdf,31,105,105,0,Yahoo publishes paper on online hate speech detection and promises to release public dataset [PDF],,false,4wl8i6,,0,,false,1473025204,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wl8i6/yahoo_publishes_paper_on_online_hate_speech/,t3_4wl8i6,,false,,
1472410511,MachineLearning,sudeepraja,sudeepraja.github.io,http://sudeepraja.github.io/Bandits/,1,8,8,0,Multi Armed Bandits and Exploration Strategies,,false,500sz5,,0,,false,1473088185,false,http://a.thumbs.redditmedia.com/iGvy_iPGt7tCDtCyfY5INIIB9SP58fkarF2n1Pnb7A8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/500sz5/multi_armed_bandits_and_exploration_strategies/,t3_500sz5,,false,,
1471884642,MachineLearning,mozart69,youtu.be,https://youtu.be/vm9CwlBI8uA,16,0,0,0,Sam Harris on Artificial Intelligence,,false,4z1gwx,,0,,false,1473070092,false,http://b.thumbs.redditmedia.com/85kUBU-sEtGl8BYm8aaf3flEOCBjwHgQG4cZLGPAGHc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z1gwx/sam_harris_on_artificial_intelligence/,t3_4z1gwx,,false,,
1471626227,MachineLearning,Dogsindahouse1,datasciencecentral.com,http://www.datasciencecentral.com/profiles/blogs/10-machine-learning-terms-explained-in-simple-english?overrideMobileRedirect=1,0,1,1,0,10 Machine Learning Terms Explained in Simple English,,false,4yk2ad,,0,,false,1473061239,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yk2ad/10_machine_learning_terms_explained_in_simple/,t3_4yk2ad,,false,,
1471362321,MachineLearning,phipple29,blog.thrivist.com,http://blog.thrivist.com/xapi-the-death-of-the-resume,1,0,0,0,Does xAPI mean the end of the resume? (xpost from r/adaptivelearning),,false,4y05lk,,0,,false,1473051170,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y05lk/does_xapi_mean_the_end_of_the_resume_xpost_from/,t3_4y05lk,,false,,
1472566315,MachineLearning,tylev,medium.com,https://medium.com/@Numerai/invisible-super-intelligence-for-the-stock-market-3c64b57b244c#.n3ojb5983,18,30,30,0,Invisible Super Intelligence for The Stock Market,,false,50bb1a,,0,,false,1473093678,false,http://a.thumbs.redditmedia.com/GlUUnC6H7i6YZIN5T-T2rKYXllqTwkdi2W_zXhXFd94.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50bb1a/invisible_super_intelligence_for_the_stock_market/,t3_50bb1a,,false,,
1470774587,MachineLearning,abhishkk65,nervanasys.com,http://nervanasys.com/intel-nervana,46,96,96,0,Intel pays $350 million to buy deep-learning startup Nervana Systems,,false,4wybub,,0,,false,1473031885,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wybub/intel_pays_350_million_to_buy_deeplearning/,t3_4wybub,,false,,
1470339089,MachineLearning,gwulfs,youtube.com,https://www.youtube.com/watch?v=4kqTBO5KDr4,0,0,0,0,Inference Using NBA Player Tracking Data,,false,4w6bmr,,0,,false,1473017553,false,http://b.thumbs.redditmedia.com/Fum8_kaWsWxQFNRlt1dKr2ruLdI_dHESEMfwg2J_fJI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w6bmr/inference_using_nba_player_tracking_data/,t3_4w6bmr,,false,,
1472600420,MachineLearning,llorgge,unpossib.ly,http://unpossib.ly/reddit-prediction/,4,2,2,0,Predicting which r/showerthoughts posts will get over 1000 points with 90% accuracy - is it possible?,,false,50e671,,0,,false,1473095179,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50e671/predicting_which_rshowerthoughts_posts_will_get/,t3_50e671,,false,,
1470195268,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vwjxd/nn_output_for_binary_classification/,1,0,0,0,NN output for binary classification,[deleted],false,4vwjxd,,0,,false,1473012442,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vwjxd/nn_output_for_binary_classification/,t3_4vwjxd,,false,,
1470839809,MachineLearning,[deleted],i.redd.it,https://i.redd.it/knd0xrmcgkex.jpg,0,1,1,0,Machine Learning explained like I'm five!!,[deleted],false,4x2f8z,,0,,false,1473033957,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x2f8z/machine_learning_explained_like_im_five/,t3_4x2f8z,,false,,
1471538315,MachineLearning,[deleted],imatge-upc.github.io,http://imatge-upc.github.io/telecombcn-2016-dlcv/,0,1,1,0,"Deep Learning for Computer Vision Barcelona, Summer Seminar UPC 2016 (slides available)",[deleted],false,4yd9rs,,0,,false,1473057803,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yd9rs/deep_learning_for_computer_vision_barcelona/,t3_4yd9rs,,false,,
1470727739,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wuzf3/retail_industry_embarks_on_machine_learning_to/,0,1,1,0,Retail Industry Embarks on Machine Learning to Increase Profits and Enhance User Experience,[removed],false,4wuzf3,,0,,false,1473030186,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wuzf3/retail_industry_embarks_on_machine_learning_to/,t3_4wuzf3,,false,,
1470744406,MachineLearning,shorkan2,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wvu2g/project_with_machine_learning/,1,0,0,0,Project with machine learning,[removed],false,4wvu2g,,0,,false,1473030617,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wvu2g/project_with_machine_learning/,t3_4wvu2g,,false,,
1472386214,MachineLearning,gabegabe6,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zz4xt/face_recognition_with_opencv_and_tensorflow/,4,12,12,0,Face recognition with OpenCV and Tensorflow?,[removed],false,4zz4xt,,0,,false,1473087295,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zz4xt/face_recognition_with_opencv_and_tensorflow/,t3_4zz4xt,,false,,
1470920399,MachineLearning,2goodlife,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x7ukh/machine_learning_ideas/,1,1,1,0,Machine Learning Ideas!!,[removed],false,4x7ukh,,0,,false,1473036745,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x7ukh/machine_learning_ideas/,t3_4x7ukh,,false,,
1470149626,MachineLearning,Mr__Christian_Grey,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vstm4/in_tensorflow_seq2seq_model_padding_the_input/,2,0,0,0,"In tensorflow, seq2seq model, padding the input results in [0,0,0,0,0,0,5,7,8,9] instead of [5,7,8,9], and then it is passed to the embedding model or the seq2seq model. Am I right?",[removed],false,4vstm4,,0,,false,1473010494,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vstm4/in_tensorflow_seq2seq_model_padding_the_input/,t3_4vstm4,,false,,
1471649988,MachineLearning,vumuvim,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ym24c/suggestion_of_a_custom_cost_function/,0,1,1,0,Suggestion of a custom cost function,[removed],false,4ym24c,,0,,false,1473062257,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ym24c/suggestion_of_a_custom_cost_function/,t3_4ym24c,,false,,
1472526129,MachineLearning,darkconfidantislife,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/508z5a/why_is_cifar100_and_cifar10_accuracy_lower_than/,10,8,8,0,Why is CIFAR-100 and CIFAR-10 accuracy lower than ImageNet?,"Why is CIFAR-10/100 accuracy lower than ImageNet? If we can get 3.7% error on ImageNet then why is our accuracy on CIFAR-10/100 comparatively so bad? Is it just due to the lack of amount of data? I mean, ImageNet, being much, much bigger, should intuitively seem ""harder"", right?
Thanks in advance guys ;)",false,508z5a,,0,,false,1473092490,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/508z5a/why_is_cifar100_and_cifar10_accuracy_lower_than/,t3_508z5a,,false,,
1470388172,MachineLearning,jm-mp,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w9jqb/emotion_classifier_in_sentences/,3,1,1,0,Emotion classifier in sentences,"Hi ML community.
Do you know a free software or API in order to label sentences with one of the 6 basic emotions ? (+ neutral state) 
I just found IBM Watson.
Thanks a lot",false,4w9jqb,,0,,false,1473019202,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w9jqb/emotion_classifier_in_sentences/,t3_4w9jqb,,false,,
1472119882,MachineLearning,xjackx,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zhsgi/stacked_lstm_for_binary_classification_keras/,7,1,1,0,Stacked LSTM for binary classification - Keras,"I am trying to implement a stacked LSTM for a time series binary classification problem in Keras, but am getting stuck. Can anyone help me debug my problem.
http://stats.stackexchange.com/questions/231569/stacked-lstm-for-sequence-classification-keras-error",false,4zhsgi,,0,,false,1473078398,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zhsgi/stacked_lstm_for_binary_classification_keras/,t3_4zhsgi,,false,,
1472474095,MachineLearning,anandsriraman,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/504s1v/single_vs_multigpu_aws_instance_for_deep_learning/,6,32,32,0,Single vs Multi-GPU AWS instance for deep learning,"I want to know how to decide when it's worth to train a deep network on a single GPU instance (g2.2xlarge on aws) vs on a multi-GPU instance (g2.8xlarge). Are there any rules of thumb regarding size of data/network depth or other metrics that could be useful? 

If you can provide any sources, that would be great!",false,504s1v,,0,,false,1473090275,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/504s1v/single_vs_multigpu_aws_instance_for_deep_learning/,t3_504s1v,,false,,
1470751495,MachineLearning,th3owner,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wwbxu/how_is_machine_learning_at_polytechnic_university/,3,0,0,0,How is Machine Learning at Polytechnic University of Milan?,"I have been admitted to MSc. in Computer Science and Engineering program at PoliMi. I am interested in Machine Learning and want to pursue this track (this program does not have official track but will arrange curriculum). Can anyone suggest me anything? (previous/current students at PoliMi, professors, etc.)",false,4wwbxu,,0,,false,1473030871,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wwbxu/how_is_machine_learning_at_polytechnic_university/,t3_4wwbxu,,false,,
1472487529,MachineLearning,Kiyori,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/505t1h/learning_ai_where_to_start/,10,0,0,0,Learning AI(?) where to start,"So lately ""learning Artificial Intelligence"" caught my attention. I would like to start experimenting with it as well but i have no idea where to start. I am talking about that part when using command line AI generations are created and they get a fitness value based on how close they got to completing the task. I do not even know what it is called and also sorry for this weird description, that's all i know about it. I saw it on a video in which an AI was learning how to complete old video games like super mario, mario kart and such, but i assume it's the same concept as AI trying to write books based on human written books?
Thank you for your help in advance and sorry again for the weird description.
(I did check the FAQ but it's too much information for an absolute beginner, i am interested in this particular field only, for now at least)",false,505t1h,,0,,false,1473090838,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/505t1h/learning_ai_where_to_start/,t3_505t1h,,false,,
1471884892,MachineLearning,coffeecoffeecoffeee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z1hpr/is_there_a_standard_scala_machine_learning/,3,2,2,0,Is there a standard Scala machine learning library that a lot of people use?,"I'm looking at MLib for Spark (which runs in Scala), and it's very stripped down compared to something like scikit-learn in Python to the point where you can't even do matrix multiplication.  Is there a main recommended Scala library with things like SVMs, random forests, and clustering?",false,4z1hpr,,0,,false,1473070103,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z1hpr/is_there_a_standard_scala_machine_learning/,t3_4z1hpr,,false,,
1470767395,MachineLearning,deephive,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wxp8e/future_directions_of_deep_learning/,8,0,0,0,Future Directions of Deep Learning,"What are the main research directions of deep learning as of 2016? ",false,4wxp8e,,0,,false,1473031566,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wxp8e/future_directions_of_deep_learning/,t3_4wxp8e,,false,,
1471899276,MachineLearning,MadWombat,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z2qbe/do_you_use_aws_gpu_instances_to_train/,6,0,0,0,Do you use AWS GPU instances to train?,"If you do, did you make your own AMI or do you use a pre-made? Does anyone use bitfusion.io pre-made AMI? Is it worth it? Would there be an interest in a better made ML targeted AMI? ",false,4z2qbe,,0,,false,1473070733,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z2qbe/do_you_use_aws_gpu_instances_to_train/,t3_4z2qbe,,false,,
1471986333,MachineLearning,I-cant_even,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z8rse/nvidia_trickling_out_the_titan_x_pascal_is_great/,19,12,12,0,Nvidia trickling out the Titan X Pascal is great,"I decided to bite the bullet and build out a rig for personal study and experimentation.  The Titan X Pascal was the clear choice for graphics card but I didn't want to pay $350 over sticker to buy it from ebay...

Googling around I found nowinstock.net which showed me that Nvidia is releasing small batches of the new Titan's almost daily at around the same general times.  I was able to buy one direct from Nvidia yesterday morning!

This way the only one price gouging me is Nvidia itself ;) ",false,4z8rse,,0,,false,1473073805,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z8rse/nvidia_trickling_out_the_titan_x_pascal_is_great/,t3_4z8rse,,false,,
1472592590,MachineLearning,galva-glava,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50djzm/getting_ready_for_a_machine_learningdata_science/,1,0,0,0,Getting ready for a Machine Learning\Data Science interview,"Hello everybody, 

I am a professional software engineer with more than 10 years of experience in developing software. Recently (1 year ago) I started a Master degree and become a researcher in my company. Most of my work in my current company is Machine Learning, NLP, Data Mining and such. 
Unfortunately, I have to leave my current company.
Anyway, in two days I have an interview for a Data Scientist position which requires 3 years of experience. They know I have only one year, but I want to come to the interview as ready as I can. 
What is the best way to prepare for that interview?
Please share any resource you think will be relevant.

Thank you!",false,50djzm,,0,,false,1473094863,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50djzm/getting_ready_for_a_machine_learningdata_science/,t3_50djzm,,false,,
1470929822,MachineLearning,MaxTalanov,github.com,https://github.com/keunwoochoi/music-auto_tagging-keras,1,21,21,0,Music auto-tagging (include pretrained model),,false,4x8lng,,0,,false,1473037128,false,http://b.thumbs.redditmedia.com/oIsSRWYZYpXjUHzDPgGblI5upo0DgQYBrmtsfqsT8RE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x8lng/music_autotagging_include_pretrained_model/,t3_4x8lng,,false,,
1470932321,MachineLearning,GuyHasNoUsername,developer.apple.com,https://developer.apple.com/library/prerelease/content/samplecode/MetalImageRecognition/Introduction/Intro.html#//apple_ref/doc/uid/TP40017385,6,0,0,0,Do you think Apple will use TensorFlow for their products?,,false,4x8tcw,,0,,false,1473037235,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x8tcw/do_you_think_apple_will_use_tensorflow_for_their/,t3_4x8tcw,,false,,
1472230962,MachineLearning,pmigdal,github.com,https://github.com/leriomaggio/deep-learning-keras-euroscipy2016,2,56,56,0,Deep Learning with Keras - Tutorial @ EuroScipy 2016,,false,4zpnl0,,0,,false,1473082420,false,http://b.thumbs.redditmedia.com/4udQPngztMuN4Ad4gjqux7poLg4A0-2csQ58rR7FL4I.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zpnl0/deep_learning_with_keras_tutorial_euroscipy_2016/,t3_4zpnl0,,false,,
1471370187,MachineLearning,xristos_forokolomvos,arstechnica.com,http://arstechnica.com/gadgets/2016/08/nvidia-pascal-laptop-specs-gtx-1080/,0,3,3,0,Good news for Deep Learning hobbyists - Laptops with decent GPUs,,false,4y0uut,,0,,false,1473051525,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y0uut/good_news_for_deep_learning_hobbyists_laptops/,t3_4y0uut,,false,,
1470854637,MachineLearning,peeyek,github.com,https://github.com/pyk/fastText.py,3,7,7,0,fasttext - A Python interface for Facebook fastText,,false,4x3ok7,,0,,false,1473034609,false,http://b.thumbs.redditmedia.com/FUkiLgbnHZxgqis-H_ar_6n2v0LSlyTMR97H5uVFwvI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x3ok7/fasttext_a_python_interface_for_facebook_fasttext/,t3_4x3ok7,,false,,
1470258013,MachineLearning,rhiever,youtube.com,https://www.youtube.com/watch?v=sRktKszFmSk,2,14,14,0,Gradient boosting in machine learning,,false,4w0tt4,,0,,false,1473014720,false,http://a.thumbs.redditmedia.com/EjRckEDpJ7rYDtBhjuPqRxWI0Pfz0JePjOz409Pror0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w0tt4/gradient_boosting_in_machine_learning/,t3_4w0tt4,,false,,
1471914720,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z3x41/help_with_rnns/,0,0,0,0,Help with RNNs,[deleted],false,4z3x41,,0,,false,1473071335,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z3x41/help_with_rnns/,t3_4z3x41,,false,,
1472511141,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/507use/how_to_split_dataset_into_training_test/,0,1,1,0,"How to split dataset into training, test, validation using scikit?",[deleted],false,507use,,0,,false,1473091919,false,default,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/507use/how_to_split_dataset_into_training_test/,t3_507use,,false,Discusssion,
1472397186,MachineLearning,YourWelcomeOrMine,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zzrt4/what_to_learn_first_linear_algebra_or_bayesian/,4,3,3,0,What to learn first? Linear algebra or Bayesian analysis?,[removed],false,4zzrt4,,0,,false,1473087643,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zzrt4/what_to_learn_first_linear_algebra_or_bayesian/,t3_4zzrt4,,false,,
1471010842,MachineLearning,modkzs,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xdoun/any_good_tutorials_to_learn_distributed_machine/,0,1,1,0,Any good tutorials to learn Distributed Machine Learning?,[removed],false,4xdoun,,0,,false,1473039742,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xdoun/any_good_tutorials_to_learn_distributed_machine/,t3_4xdoun,,false,,
1471743643,MachineLearning,VGMTaylor,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ysgil/machine_learning_in_games/,17,2,2,0,Machine Learning in Games,"Hi, I'm a Computer Science and I'm doing my disseration focused in using Machine Learning in games.

I'd have to develop a game and then make some learning algorithm thats useful.

Has anyone got any ideas on what games I could use to do this in the scale of university project.

I thought about poker or chess but I imagine these would be pretty complicated to even develop an AI for an especially the learning part. Then on the other end I've thought of blackjack but tere probably isn't much of a scale to perform machine learning on.

Any ideas?",false,4ysgil,,0,,false,1473065524,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ysgil/machine_learning_in_games/,t3_4ysgil,,false,,
1470098070,MachineLearning,ill-logical,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vprm2/fp16_rectified_convnets_problematic_convergence/,7,6,6,0,FP16 + rectified convnets = problematic convergence?,"I find that even if I scale the gradient just right, convergence with FP16 is worse than with FP32 (Using Adamax), and training is prone to divergence with rectified convnets.

Are there any papers about making FP16 work, tips, tricks, etc.?",false,4vprm2,,0,,false,1473008906,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4vprm2/fp16_rectified_convnets_problematic_convergence/,t3_4vprm2,,false,Discusssion,
1472089535,MachineLearning,funkadelic99,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zg3x7/determining_how_many_eigenvalues_are_enough_to/,10,1,1,0,"Determining ""how many eigenvalues"" are enough to predict accurately?","I'm working on a data by the approach of ""Principal Component Analysis"" relevant to a traffic engineering problem to understand the characteristic of a piece of road. I'm new in ""data science"" and just finished the course in Coursera, given by Andrew Ng.

Actually I am wondering that :
is there way in such a ""data science"" problem to LEARN
1) how many eigenvalues we need
and
2) which eigenvectors corresponding to these eigenvalues we should choose  

in order to understand the characteristics of ""the data"" better  ?",false,4zg3x7,,0,,false,1473077539,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zg3x7/determining_how_many_eigenvalues_are_enough_to/,t3_4zg3x7,,false,,
1472631511,MachineLearning,redsees,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50g4ob/where_should_i_start_learning_ml/,4,0,0,0,Where should I start learning ML?,"Hello,
I wanted to start learning about Machine Learning, the art of making a processor (or simply a computer software) learn solving a problem and improves itself in solving the given problem (those train and test stuff I see in a lot of ML videos). Where should I start?
My background in AI, DL, ML is zero.
My other background is:
- long experience in C and intel x86 asm.
- Digital design using embedded systems and development boards.
Those are my most experienced fields, don't know if mentioning them will help telling where to start, but it's logical to ask about the skills of someone in order to point them at the right place to start from.
Thanks!",false,50g4ob,,0,,false,1473096200,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50g4ob/where_should_i_start_learning_ml/,t3_50g4ob,,false,,
1470145683,MachineLearning,bagelorder,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vsibd/which_nonbayesian_frequentists_mlmethods_are_there/,19,15,15,0,Which non-bayesian / frequentists ML-methods are there?,"I know the philophical difference between bayesian and frequentists statistics and I have a learned about some bayesian methods for ML by now (mostly bishops PRML). It is not hard to spot a bayesian method, since they all take some priors over some parametrizations and then use bayes in some way.

But I often have a problem to identify what makes a methods a ""frequentists"" method. Can you guys help me and just tell me the biggest frequentists methods out there and what makes them frequentists methods?",false,4vsibd,,0,,false,1473010332,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vsibd/which_nonbayesian_frequentists_mlmethods_are_there/,t3_4vsibd,,false,,
1470332906,MachineLearning,Greendogo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w5sld/support_vector_machines_vs_neural_networks/,10,2,2,0,Support Vector Machines vs. Neural Networks,"So this summer I'm doing an internship at a robotics company where they have me doing some research with neural networks to do object detection.

Before this summer started, I went to my university faculty and asked around for information that might get me started so I'd be somewhat prepared before I got here.  However, the one professor who would really have anything to say about this told me that Neural Networks are outdated and that the real research is being done on Support Vector Machines.

Now that I've been here all summer, I'm looking back on this statement and wondering what exactly he was talking about.  I've witnessed some amazing things done by Neural Networks over the past several months.  Are there any amazing examples showing the power of Support Vector Machines?",false,4w5sld,,0,,false,1473017282,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w5sld/support_vector_machines_vs_neural_networks/,t3_4w5sld,,false,,
1471130127,MachineLearning,keidouleyoucee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xlk0k/training_time_prediction_analytically/,2,3,3,0,Training time prediction - analytically,"I am wondering how I can predict the training time given the CNN/RNN structures, and how accurate it would be.

Of course, it would depend on the hardware, so let's assume that we're using the same gpu. -- and also say we fix the batch size. And of course the same cudnn/library (theano for example).
I also think it would better to have a 'reference' (or baseline?) system that we know how long it takes for each training sample. 

Very simple example: 
* If a convnet (input size 128x128), that has **one** convolutional layer (3x3 convolution and 32 feature maps) + batch mornalization + max pooling (very weird 128x128 max-pooling just to make it simple...), and then a fully-connected layer of 10 node (e.g., for MNIST) took 100s for one epoch, how long do we need to train a convnet of conv(3,3,32)-BN-MP(2,2)-conv(3,3,32)-BN-MP(64,64)-FullyConnected(10) per epoch? 

My understanding is: 

* First, matrix multiplication would be the most important here. Naive implementation of matrix multiplication of (NxM) x (MxK) is up to O(NMK) (or people just say O(n**3) for the simplicity). There are then some optimisations to speed up to O(n**2.3ish), but I don't know how it can be applied to this case. 
* I also don't know how much the batch process would help here - but I want to fix the batch size and just want to compare the time consumption of the configuration, not something due to HW/SW/batch size.
* There are forward path, backprop w.r.t. input, backprop w.r.t. weights, and they are represented with matrix multiplication, so for conv layer or rnn, we can compute the sizes of matrices that are multiplied. I haven't thought about BN/Dropout much. 

I can actually just run and see how long does it take. But I want to understand what affects the most - and how precisely we can at least approximate the time consumption. I assume it is highly dependent on something like how well the library/cudnn is optimised, but still! Is there anyway to *try* to compute it?
",false,4xlk0k,,0,,false,1473043749,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xlk0k/training_time_prediction_analytically/,t3_4xlk0k,,false,,
1470259320,MachineLearning,carlthome,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w0xwv/question_training_neural_nets_on_copyrighted/,4,3,3,0,"[Question] Training neural nets on copyrighted material, is it legal?","So let's say the training data is copyrighted (movies, music, literature, etc.) and I obtain weights with that data, and then do inference in a commercial application. Would this be considered legal, in the same way that I would be allowed to paint a picture while listening to The Beatles, or would the application be subject to media licensing fees anyway?",false,4w0xwv,,0,,false,1473014781,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w0xwv/question_training_neural_nets_on_copyrighted/,t3_4w0xwv,,false,,
1472686549,MachineLearning,theironhide,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50k8dp/which_course_to_use_for_a_dive_into_deep_learning/,8,3,3,0,Which course to use for a dive into deep learning after Coursera Andrew Ng's ML?,"Hi,
I have a sound knowledge of image processing (mostly low level image processing, object detection, basic object recognition), and I have done undergraduate courses in image processing (equivalent to the Gonsalez Woods book) and machine learning (and the Andrew Ng Coursera course). I want to begin with deep learning with focus on image processing and recognition. The way I see it, the ideal place for me to go would be Fei-Fei Li's course CS231N at Stanford. But before that, would it be better if I took one or more of the following courses?

1. University of Washington's Machine Learning Coursera specialization - set of 5 courses (https://www.coursera.org/specializations/machine-learning)

2. Geoffrey Hinton's Coursera course on neural networks (https://www.coursera.org/learn/neural-networks)

3. Hugo Larochelle's class on neural networks (https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)


Which of these three should ideally be taken in order to strengthen concepts and are worth taking? Which order should these courses be taken in?

Thank you.",false,50k8dp,,0,,false,1473098319,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50k8dp/which_course_to_use_for_a_dive_into_deep_learning/,t3_50k8dp,,false,,
1471309252,MachineLearning,MaxTalanov,jack-clark.net,https://jack-clark.net/2016/08/15/import-ai-issue-3-synthetic-pokemon-brain-like-ai-and-the-history-of-dropout/,0,1,1,0,"Import AI Newsletter: Synthetic Pokemon, brain-like AI, and the history of Dropout.",,false,4xwvq9,,0,,false,1473049511,false,http://a.thumbs.redditmedia.com/vVHToscPjXhpwMozvb3XlC-yXQpktmwpr38cN3CxJc8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xwvq9/import_ai_newsletter_synthetic_pokemon_brainlike/,t3_4xwvq9,,false,,
1470710426,MachineLearning,anonDogeLover,arxiv.org,http://arxiv.org/abs/1608.02164,3,8,8,0,[1608.02164] Adapting Deep Network Features to Capture Psychological Representations,,false,4wtwx7,,0,,false,1473029640,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wtwx7/160802164_adapting_deep_network_features_to/,t3_4wtwx7,,false,Research,
1470632319,MachineLearning,carderellis,geekdad.com,https://geekdad.com/2016/08/daily-deal-complete-machine-bundle/,0,0,0,0,GeekDad Daily Deal: The Complete Machine Learning Bundle - GeekDad,,false,4wojbk,,0,,false,1473026899,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wojbk/geekdad_daily_deal_the_complete_machine_learning/,t3_4wojbk,,false,,
1471584148,MachineLearning,john_philip,r-bloggers.com,https://www.r-bloggers.com/free-kaggle-machine-learning-tutorial-for-r/,3,9,9,0,Kaggle Machine Leanrning Tutorial for R,,false,4yh4pq,,0,,false,1473059748,false,http://a.thumbs.redditmedia.com/u64vJS3U1yz376QwvI5ppvuh4I5ilX-H7m67Dly5498.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yh4pq/kaggle_machine_leanrning_tutorial_for_r/,t3_4yh4pq,,false,,
1471792605,MachineLearning,greymatter-analytics,m.csmonitor.com,http://m.csmonitor.com/Science/2016/0821/New-software-can-track-global-poverty-from-space,0,1,1,0,New software from Stanford uses Machine Learning to track poverty from space!!,,false,4yvbi2,,0,,false,1473066975,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yvbi2/new_software_from_stanford_uses_machine_learning/,t3_4yvbi2,,false,,
1471794661,MachineLearning,stkim1,pocketcluster.wordpress.com,https://pocketcluster.wordpress.com/2016/08/21/weekly-bigdata-ml-roundup-aug-21-2016/,0,1,1,0,"BigData &amp; ML Frameworks, Toolsets, and Models weekly roundup – Aug. 21, 2016",,false,4yvhcg,,0,,false,1473067057,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yvhcg/bigdata_ml_frameworks_toolsets_and_models_weekly/,t3_4yvhcg,,false,,
1471274153,MachineLearning,moklick,blog.webkid.io,http://blog.webkid.io/datasets-for-machine-learning/,0,0,0,0,Datasets for Machine Learning,,false,4xu103,,0,,false,1473048057,false,http://b.thumbs.redditmedia.com/Auk2A5QqSm293vLs7T0MXAO7gpuq_ViSuChdTQR6uKQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xu103/datasets_for_machine_learning/,t3_4xu103,,false,,
1470576971,MachineLearning,Valiox,arxiv.org,http://arxiv.org/abs/1608.01471,1,10,10,0,UnitBox: An Advanced Object Detection Network,,false,4wkuve,,0,,false,1473025012,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wkuve/unitbox_an_advanced_object_detection_network/,t3_4wkuve,,false,,
1471296388,MachineLearning,Cjratcliff,github.com,https://github.com/cjratcliff/tensorflow-repos,0,1,1,0,Every major TensorFlow repo (I could find),,false,4xvxad,,0,,false,1473049024,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xvxad/every_major_tensorflow_repo_i_could_find/,t3_4xvxad,,false,,
1470245241,MachineLearning,pavanmirla,twitter.com,https://twitter.com/KirkDBorne/status/754750596012466176,0,0,0,0,Intuition behind the concept of gradient vector and gradient descent,,false,4vzohh,,0,,false,1473014122,false,http://b.thumbs.redditmedia.com/jYsjkSXMa1o6oPDUrFKHApLjC7RVmtbWvcfT9hKBnUU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vzohh/intuition_behind_the_concept_of_gradient_vector/,t3_4vzohh,,false,,
1471989597,MachineLearning,vanboxel,youtube.com,https://www.youtube.com/watch?v=S3TbmSGLwVY,5,30,30,0,Visualizing a Self-Driving Car steering model in Keras and Matplotlib (livestream),,false,4z91jf,,0,,false,1473073943,false,http://a.thumbs.redditmedia.com/EGFc4COgCNVUvWN36pqj6nMLSRHy2vr7Wrf1l-e4E20.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z91jf/visualizing_a_selfdriving_car_steering_model_in/,t3_4z91jf,,false,,
1472333133,MachineLearning,modeless,arxiv.org,https://arxiv.org/abs/1608.05137,2,10,10,0,IM2CAD - Single photo to CAD model of a room and its furniture,,false,4zwaz7,,0,,false,1473085841,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zwaz7/im2cad_single_photo_to_cad_model_of_a_room_and/,t3_4zwaz7,,false,,
1470038060,MachineLearning,muoro,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vlf02/how_humanmachine_learning_partnerships_can_reduce/,4,0,0,0,How Human-Machine Learning Partnerships Can Reduce Unconscious Bias,[removed],false,4vlf02,,0,,false,1473006623,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vlf02/how_humanmachine_learning_partnerships_can_reduce/,t3_4vlf02,,false,,
1471927929,MachineLearning,unibrain,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z4t8f/this_scientific_work_claims_to_have_determined/,0,1,1,0,This scientific work claims to have determined life's purpose/meaning,[removed],false,4z4t8f,,0,,false,1473071784,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z4t8f/this_scientific_work_claims_to_have_determined/,t3_4z4t8f,,false,,
1470663211,MachineLearning,rag594,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wq5y3/outlier_detection/,0,1,1,0,Outlier Detection,[removed],false,4wq5y3,,0,,false,1473027726,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wq5y3/outlier_detection/,t3_4wq5y3,,false,,
1472139848,MachineLearning,fmb85,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zj55y/scandinavian_conference_on_image_analysis_1214/,0,1,1,0,"Scandinavian Conference on Image Analysis, 12-14 June 2017, Tromsø.",[removed],false,4zj55y,,0,,false,1473079092,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zj55y/scandinavian_conference_on_image_analysis_1214/,t3_4zj55y,,false,,
1471484103,MachineLearning,0xfd,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y9ig4/anyone_else_seeing_slow_start_with_tensorflow/,6,0,0,0,Anyone else seeing slow start with tensorflow 0.10rc0 + CUDA 7.5 + CUDNN 5?,I am seeing a very long wait before compute starts with the stated setup on a GTX 1060. Afterwards the results and speed are fine. the same code starts much sooner on CUDA 7.5 + CUDNN 4 on a GTX 970. Any clue?,false,4y9ig4,,0,,false,1473055904,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y9ig4/anyone_else_seeing_slow_start_with_tensorflow/,t3_4y9ig4,,false,,
1471756661,MachineLearning,OhmsFarooq,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ytb0b/twitter_refugee_sentiment_analysis/,4,0,0,0,Twitter Refugee Sentiment Analysis,"Hey r/MachineLearning, bit of a newbie here, looking for some help in NLP. 

My friend and I are attempting to see if people are in favor of or against refugees by analyzing Twitter sentiment. The problem we're running into is that sometimes tweets will be written in a manner that is, say, in favor of nativist leaders, but will be marked as a positive sentiment.*

We think the issue might just be the API we've selected (we're using Dandelion) so we'd love some recommendations as to better APIs to select. More importantly, though, I imagine this is a relatively common problem in sentiment analysis. How do most people face it?

*If that example's unclear,  [this link](http://www.europeanfutures.ed.ac.uk/article-2583) does a pretty good job of explaining the issue with Brexit starting with the paragraph ""For example, we have the tweet below."")",false,4ytb0b,,0,,false,1473065948,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ytb0b/twitter_refugee_sentiment_analysis/,t3_4ytb0b,,false,,
1470290139,MachineLearning,fuckinghelldad,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w32k9/which_neural_net_library_would_make_it_easiest_to/,5,0,0,0,Which Neural Net Library Would Make it Easiest to Get The Output Of Neurons in a Hidden Layer?,"I know nothing about NN libraries, but I'm adept at programming and NN theory. I want to do be able to do two things to a trained model:

1) Get the outputs of arbitrary hidden neurons after passing data through the input layer.

2) Get the outputs of arbitrary hidden neurons after passing data through a (lower) hidden layer.

Are there even any libraries which would allow me to do this, or would it be easier just to write my own program?",false,4w32k9,,0,,false,1473015881,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w32k9/which_neural_net_library_would_make_it_easiest_to/,t3_4w32k9,,false,,
1471527959,MachineLearning,godspeed_china,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ycc6y/training_large_dataset_with_midbatch_lbfgs/,1,1,1,0,training large dataset with mid-batch l-bfgs,"suppose we have 1 million sample to train a deep network.  
The popular way is to use SGD. However, we know that SGD has slow convergence rate. We can only get an approximate estimation even with enough time. More over there is many tricks/heuristics to do SGD well.  
My idea is that we use mid-batch (~10,000 sample) l-bfgs. Each CPU core train one mid-batch using l-bfgs. Finally the yhats for each mid-batch is averaged. There are three obvious advantages: (1)easy for tuning (2) bagging/pertubation effect (3) speed (~10 miniutes on CPU), partly due to smaller network. The disadvantage is that mid-batch model is less powerful than full-batch model, however this disadvantage is reduced (since we have many mid-batches, I have a 56-thread computer:-) and acceptable as we accept SGD approximation.  
any commnets?",false,4ycc6y,,0,,false,1473057334,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ycc6y/training_large_dataset_with_midbatch_lbfgs/,t3_4ycc6y,,false,,
1470314491,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w4b6u/density_estimation_for_points_regularly_spaced_on/,7,1,1,0,Density estimation for points regularly spaced on a grid? Infer spacing between pdf peaks?,"Due to a fundamental characteristic of the data, points are clustered together on a 1-D grid-like structure with equal spacing.
Plotting these points in a histogram shows a pdf with several individual peaks.

What clustering techniques could I use to infer the variance (the width) of each of these peaks, as well as the number of peaks?

Is this done with density estimation? Maybe Dirichlet processes?",false,4w4b6u,,0,,false,1473016515,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w4b6u/density_estimation_for_points_regularly_spaced_on/,t3_4w4b6u,,false,,
1470345084,MachineLearning,jeffatgoogle,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/,820,1169,1169,0,AMA: We are the Google Brain team. We'd love to answer your questions about machine learning.,"We’re a group of research scientists and engineers that work on the [Google Brain team](http://g.co/brain).  Our group’s mission is to make intelligent machines, and to use them to improve people’s lives.  For the last five years, we’ve conducted research and built systems to advance this mission.

We disseminate our work in multiple ways:

* By publishing papers about our research (see [publication list](https://research.google.com/pubs/BrainTeam.html))
* By building and open-sourcing software systems like TensorFlow (see [tensorflow.org](http://tensorflow.org) and [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow))
* By working with other teams at Google and Alphabet to get our work into the hands of billions of people (some examples: [RankBrain for Google Search](https://en.wikipedia.org/wiki/RankBrain), [SmartReply for GMail](https://research.googleblog.com/2015/11/computer-respond-to-this-email.html), [Google Photos](https://research.googleblog.com/2014/09/building-deeper-understanding-of-images.html), [Google Speech Recognition](https://research.googleblog.com/2012/08/speech-recognition-and-deep-learning.html), …)
* By training new researchers through internships and the [Google Brain Residency](http://g.co/brainresidency) program

We are:

* [Jeff Dean](http://research.google.com/people/jeff) (/u/jeffatgoogle)
* [Geoffrey Hinton](https://research.google.com/pubs/GeoffreyHinton.html) (/u/geoffhinton)
* [Vijay Vasudevan](http://research.google.com/pubs/VijayVasudevan.html) (/u/Spezzer)
* [Vincent Vanhoucke](http://research.google.com/pubs/VincentVanhoucke.html) (/u/vincentvanhoucke)
* [Chris Olah](http://research.google.com/pubs/ChristopherOlah.html) (/u/colah)
* [Rajat Monga](http://research.google.com/pubs/RajatMonga.html) (/u/rajatmonga)
* [Greg Corrado](http://research.google.com/pubs/GregCorrado.html) (/u/gcorrado)
* [George Dahl](https://scholar.google.com/citations?user=ghbWy-0AAAAJ&amp;hl=en) (/u/gdahl)
* [Doug Eck](http://research.google.com/pubs/author39086.html) (/u/douglaseck)
* [Samy Bengio](http://research.google.com/pubs/bengio.html) (/u/samybengio)
* [Quoc Le](http://research.google.com/pubs/QuocLe.html) (/u/quocle)
* [Martin Abadi](http://research.google.com/pubs/abadi.html) (/u/martinabadi)
* [Claire Cui](https://www.linkedin.com/in/claire-cui-5021035) (/u/clairecui)
* [Anna Goldie](https://www.linkedin.com/in/adgoldie) (/u/anna_goldie)
* [Zak Stone](https://www.linkedin.com/in/zstone) (/u/poiguy)
* [Dan Mané](https://www.linkedin.com/in/danmane) (/u/danmane)
* [David Patterson](https://www2.eecs.berkeley.edu/Faculty/Homepages/patterson.html) (/u/pattrsn)
* [Maithra Raghu](http://maithraraghu.com/) (/u/mraghu)
* [Anelia Angelova](http://research.google.com/pubs/AneliaAngelova.html) (/u/aangelova)
* [Fernanda Viégas](http://hint.fm/) (/u/fernanda_viegas)
* [Martin Wattenberg](http://hint.fm/) (/u/martin_wattenberg)
* [David Ha](http://blog.otoro.net/) (/u/hardmaru)
* [Sherry Moore](https://www.linkedin.com/in/sherry-moore-38b3a32) (/u/sherryqmoore/)
* … and maybe others: we’ll update if others become involved.

We’re excited to answer your questions about the Brain team and/or machine learning!  (We’re gathering questions now and will be answering them on August 11, 2016).

Edit (~10 AM Pacific time): A number of us are gathered in Mountain View, San Francisco, Toronto, and Cambridge (MA), snacks close at hand.  Thanks for all the questions, and we're excited to get this started.

Edit2: We're back from lunch.  Here's [our AMA command center](http://imgur.com/gallery/zHkoC)

Edit3: (2:45 PM Pacific time): We're mostly done here.  Thanks for the questions, everyone!  We may continue to answer questions sporadically throughout the day.",false,4w6tsv,,0,,false,1473017811,false,self,t5_2r3gv,false,one,googlebrain,false,true,,/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/,t3_4w6tsv,Google Brain,false,Discusssion,
1470619724,MachineLearning,mainguyenmth,youtube.com,https://www.youtube.com/attribution_link?a=hHeJNMhLYJ0&amp;u=%2Fwatch%3Fv%3DtulwK4szdbA%26feature%3Dshare,1,0,0,0,"Máy làm viên hoàn mềm, viên tễ đông y có đùn sợi",,false,4wnt4k,,0,,false,1473026523,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wnt4k/máy_làm_viên_hoàn_mềm_viên_tễ_đông_y_có_đùn_sợi/,t3_4wnt4k,,false,,
1471652604,MachineLearning,llSourcell,youtube.com,https://www.youtube.com/watch?v=2FmcHiLCwTU,1,2,2,0,Tensorflow Demo in 5 Minutes,,false,4ym921,,0,,false,1473062358,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ym921/tensorflow_demo_in_5_minutes/,t3_4ym921,,false,,
1472359671,MachineLearning,moschles,youtube.com,https://www.youtube.com/watch?v=U0ACP9J8vOU,0,39,39,0,Deep Learning with Python and Tensorflow (Ian Lewis),,false,4zxzzh,,0,,false,1473086711,false,http://b.thumbs.redditmedia.com/0CZgGIZSqpVPsej_YAYC_aTOlA03r3KoMBLMJVlmV4s.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zxzzh/deep_learning_with_python_and_tensorflow_ian_lewis/,t3_4zxzzh,,false,,
1470117699,MachineLearning,nischalhp,github.com,https://github.com/unnati-xyz/fifthel-2016-workshop,2,17,17,0,"Building Scalable Data Science Pipelines with Luigi, Apache Spark, Flask",,false,4vr1fm,,0,,false,1473009576,false,http://a.thumbs.redditmedia.com/jSdZKFfWucFCAPwrU8iBRd6eLFCMKuwivotJ5hI1EG8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vr1fm/building_scalable_data_science_pipelines_with/,t3_4vr1fm,,false,,
1471672906,MachineLearning,john_philip,analyticsvidhya.com,https://www.analyticsvidhya.com/blog/2016/06/12-free-mind-mapping-tools-data-scientist-enhance-structured-thinking/,0,1,1,0,12 Free Mind Mapping tools for Data Scientist to enhance structured thinking.,,false,4ynl1s,,0,,false,1473063041,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ynl1s/12_free_mind_mapping_tools_for_data_scientist_to/,t3_4ynl1s,,false,,
1472285139,MachineLearning,ciolaamotore,techtalks.tv,http://techtalks.tv/icml/2016/,8,28,28,0,ICML 2016 Videos available,,false,4ztc90,,0,,false,1473084308,false,http://b.thumbs.redditmedia.com/YC5SYOp4KwGZnWAHUB6wvxFvaxW6k8hy0-paik8BCko.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ztc90/icml_2016_videos_available/,t3_4ztc90,,false,,
1472198593,MachineLearning,futureroboticist,slideshare.net,https://www.slideshare.net/PetteriTeikariPhD/deep-learning-workstation,0,1,1,0,How to build your computer for Deep Learning,,false,4znij2,,0,,false,1473081325,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4znij2/how_to_build_your_computer_for_deep_learning/,t3_4znij2,,false,,
1471799957,MachineLearning,downtownslim,arxiv.org,https://arxiv.org/abs/1607.05690,12,27,27,0,[1607.05690] Stochastic Backpropagation through Mixture Density Distributions,,false,4yvxd7,,0,,false,1473067284,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4yvxd7/160705690_stochastic_backpropagation_through/,t3_4yvxd7,,false,Research,
1470752473,MachineLearning,akelleh,medium.com,https://medium.com/@akelleh/speed-vs-accuracy-when-is-correlation-enough-when-do-you-need-causation-708c8ca93753#.avsfvqz3s,0,11,11,0,Speed vs. Accuracy: When is Correlation Enough? When Do You Need Causation?,,false,4wwep1,,0,,false,1473030909,false,http://b.thumbs.redditmedia.com/5jc530YtgECG5zH3aIsZhZGUhY_pKT4vhb_N1YKFd-Y.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wwep1/speed_vs_accuracy_when_is_correlation_enough_when/,t3_4wwep1,,false,,
1470754164,MachineLearning,ZooglyWoogly,imgur.com,http://imgur.com/a/GYNHy,4,1,1,0,Trying to understand Bidirectional LSTMs - are these diagrams right?,,false,4wwjm5,,0,,false,1473030978,false,http://b.thumbs.redditmedia.com/BDL1rYZWNXG9AiGIm7qYER0_Hlat8CZQE16hSD4sYic.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wwjm5/trying_to_understand_bidirectional_lstms_are/,t3_4wwjm5,,false,,
1470669828,MachineLearning,yura_invrea,invrea.com,https://invrea.com/blog/,0,1,1,0,Probabilistic Programming predicts Olympic Gold Medal Counts using Excel,,false,4wqnyk,,0,,false,1473027981,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wqnyk/probabilistic_programming_predicts_olympic_gold/,t3_4wqnyk,,false,,
1471017360,MachineLearning,manux,tml.cs.uni-tuebingen.de,http://www.tml.cs.uni-tuebingen.de/team/luxburg/misc/nips2016/index.php,5,9,9,0,Details of the NIPS 2016 reviewing process,,false,4xe7sg,,0,,false,1473040012,false,http://a.thumbs.redditmedia.com/lGLXLlsEc2y-E5nRTHMqRSuskf_F6LS_Q1WnvbBLZ34.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xe7sg/details_of_the_nips_2016_reviewing_process/,t3_4xe7sg,,false,,
1470332913,MachineLearning,antinucleon,dmlc.ml,http://dmlc.ml/mxnet/2016/07/29/use-caffe-operator-in-mxnet.html,1,6,6,0,"Seamlessly Use Caffe Layers in MXNet (for distributed training, etc)",,false,4w5sm5,,0,,false,1473017282,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w5sm5/seamlessly_use_caffe_layers_in_mxnet_for/,t3_4w5sm5,,false,,
1471297433,MachineLearning,perceptron01,quora.com,https://www.quora.com/session/François-Chollet/1,6,29,29,0,AMA session with François Chollet (author of Keras),,false,4xw0ae,,0,,false,1473049066,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xw0ae/ama_session_with_françois_chollet_author_of_keras/,t3_4xw0ae,,false,,
1472575488,MachineLearning,YigitDemirag,openai.com,https://openai.com/blog/infrastructure-for-deep-learning/,4,41,41,0,Infrastructure for Deep Learning,,false,50c2up,,0,,false,1473094098,false,http://b.thumbs.redditmedia.com/zkpBRZScuLHe1wPK-IOOBIYr1DTxqNFI-ntLG3eBPlc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50c2up/infrastructure_for_deep_learning/,t3_50c2up,,false,,
1472577215,MachineLearning,smerity,github.com,https://github.com/baidu/paddle,6,51,51,0,Paddle: Baidu's open source deep learning framework,,false,50c8e3,,0,,false,1473094178,false,http://b.thumbs.redditmedia.com/j3ahNS_fkbPrNmhIqmq1BcDua80VV1xBzFOBorV5fDk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50c8e3/paddle_baidus_open_source_deep_learning_framework/,t3_50c8e3,,false,,
1470431884,MachineLearning,alxndrkalinin,iclr.cc,http://www.iclr.cc,10,17,17,0,"[cfp] ICLR 2017, 5th International Conference on Learning Representations - Toulon, France, April 24 - 26, 2017",,false,4wcpjh,,0,,false,1473020823,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wcpjh/cfp_iclr_2017_5th_international_conference_on/,t3_4wcpjh,,false,Research,
1471905071,MachineLearning,elisebreda,blog.yhat.com,http://blog.yhat.com/posts/rodeo-blog-contest.html,0,1,1,0,Calling all Rodeo Users: Yhat Blog Contest!,,false,4z36zk,,0,,false,1473070968,false,http://b.thumbs.redditmedia.com/12PjyQb_4L3KAn0HaKUqQtnmo2yhs1JNK9sC9kstEGg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z36zk/calling_all_rodeo_users_yhat_blog_contest/,t3_4z36zk,,false,,
1472300957,MachineLearning,[deleted],github.com,https://github.com/david-gpu/srez,0,1,1,0,Image Super-Resolution Through Deep Learning - Github,[deleted],false,4zu0x0,,0,,false,1473084664,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zu0x0/image_superresolution_through_deep_learning_github/,t3_4zu0x0,,false,,
1471130748,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xllic/can_someone_help_me_out_with_gradient_descent_in/,1,0,0,0,Can someone help me out with gradient descent in Python (Andrew Ng's Machine Learning Course)?,[deleted],false,4xllic,,0,,false,1473043769,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xllic/can_someone_help_me_out_with_gradient_descent_in/,t3_4xllic,,false,,
1472402870,MachineLearning,hyperqube12,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50071i/doing_ml_research_with_a_physics_phd/,5,2,2,0,Doing ML research with a physics PhD ?,[removed],false,50071i,,0,,false,1473087865,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50071i/doing_ml_research_with_a_physics_phd/,t3_50071i,,false,,
1471462101,MachineLearning,Evixum,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y7lsq/linear_regression_code/,0,1,1,0,Linear regression code,[removed],false,4y7lsq,,0,,false,1473054931,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y7lsq/linear_regression_code/,t3_4y7lsq,,false,,
1471688575,MachineLearning,decoder007,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yodzt/online_stylushandwritten_digit_recognition_using/,4,1,1,0,Online stylus/handwritten digit recognition using convolutional networks,"Is there an implementation of multiple character online digit recognition anywhere ? Please help. I want to implement it in my app. ",false,4yodzt,,0,,false,1473063449,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yodzt/online_stylushandwritten_digit_recognition_using/,t3_4yodzt,,false,,
1470655416,MachineLearning,dzyl,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wpnpd/using_embeddingshidden_representations_for/,3,1,1,0,Using embeddings/hidden representations for semi-supervised learning,"Unlabeled data is cheap, labeled data is expensive. I have only been dabbling into deep learning for the past 6 months and not a lot of hands on experience yet, however I regularly find myself in the situation where I have to hand label a lot of data to get enough training samples to train a model to automatically label the rest of the data and future data. This however doesn't take the full input space in consideration, only that of the data that I manually labeled. I feel like semi-supervised methods are very useful in real world applications, but I haven't seen too much literature about it. Maybe I'm not searching correctly.

I know that discrete sequences like characters or words can be embedded via methods like word2vec using windows over the sequences, and that these embeddings can help capture semantics and 'closeness' between inputs. Could I use autoencoders to achieve the same thing, not necessarily to just reduce the dimensionality of the problem but to get a better representation of the distribution of the input space using all my data as opposed to just the labeled data?

I'm very interested in attempts and thoughts about this, why this doesn't work or does, and would love to read some more papers about this and other semi-supervised learning approaches, since I think this is very valuable.",false,4wpnpd,,0,,false,1473027471,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wpnpd/using_embeddingshidden_representations_for/,t3_4wpnpd,,false,,
1471594637,MachineLearning,i_reddit_too_mcuh,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yhq63/finitehorizon_vs_infinitehorizon_problems_say_mdp/,1,1,1,0,"Finite-horizon vs Infinite-horizon problems? (say, MDP)","My understanding is that, given a long enough finite-horizon problem, the resulting policy would be equivalent to that of a discounted infinite-horizon problem. Infinite-horizon, by reaching convergence, would indicate a stable policy, which to me seems ""better"" than that produced by a finite-horizon, where an additional iteration may change the policy. So, except for computational resource concerns, why would I ever want to formulate a problem as finite-horizon? Is it wrong of me to assume infinite-horizon produces ""better"" results than finite-horizon?",false,4yhq63,,0,,false,1473060050,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yhq63/finitehorizon_vs_infinitehorizon_problems_say_mdp/,t3_4yhq63,,false,,
1470662778,MachineLearning,tomsal,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wq4we/skin_cancer_prediction_and_data_sets/,6,0,0,0,Skin cancer prediction and data sets,"Hi everybody,


this is more of a data/application related question. Still I hope that I am at the right place to ask.

Does anyone know of more applications like this one:

http://www.popularmechanics.com/science/health/a13391/ibm-skin-cancer-detection-system-memorial-sloan-kettering-17545836/


I think training a model to classify an image of a mole as potentially dangerous or not should be rather easy. However, does anybody know whether there exist non-commercial models already or if there are any data sets available?

Of course, when doing such a prediction one should not necessarily take the result as correct, but it would be a nice tool.
",false,4wq4we,,0,,false,1473027711,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wq4we/skin_cancer_prediction_and_data_sets/,t3_4wq4we,,false,,
1471189643,MachineLearning,cvikasreddy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/,25,42,42,0,Machine Learning - WAYR (What Are You Reading) - Week 5,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.  

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  

[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)  
[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)  
[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)  
[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)  

Besides that, there are no rules, have fun.",false,4xomf7,,0,,false,1473045308,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/,t3_4xomf7,,false,Discusssion,
1470932736,MachineLearning,nerd_guy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x8uk2/what_is_an_appropriate_dimensionality_reduction/,5,2,2,0,What is an appropriate dimensionality reduction approach for visualization of image hashes,"Posted it on stackoverflow and cross-validated, but didn't get any responses. Will be happy if folks share any insights or help reframe the question. 

&gt;I have a dataset of photographs of forms (say 1000 images). Since the forms belong to about 50 different layouts (i.e. templates), I expect the corresponding images to be clustered. I want to visualize these clusters prior to performing any classification or advanced processing.

&gt;My approach is to compute a 64-bit hash for each image and reduce to two dimensions before plotting. Conceptually, hash(""img001.jpg"") = ""0100101...1"" (64 bits). When applied on all 1000 images, this yields a 1000 x 64 matrix. However, I have doubts about PCA being the right tool for the dimensionality reduction step.

&gt;PCA seems appropriate for quantitative-valued matrices with Euclidean distance metric. For hashes, the proper distance metric is different (Hamming distance, for instance). As a result, I'm not sure if the traditional PCA-based visualization would make sense.

&gt;What is a more appropriate dimensionality-reduction approach for bit strings with Hamming-like similarity measure? More generally, is there a better approach to visualize clusters of images than my current hash -&gt; dimensionality reduction -&gt; visualization approach?

&gt;I've researched (1) Correspondence Analysis and (2) Nonlinear PCA but am yet to find an example applying it to hash-like objects.",false,4x8uk2,,0,,false,1473037253,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x8uk2/what_is_an_appropriate_dimensionality_reduction/,t3_4x8uk2,,false,,
1472071644,MachineLearning,kazanz,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zeq7f/how_to_combine_three_models_that_classify/,4,2,2,0,How to combine three models that classify different variable accurately?,"Lets say, I have three models that predict the severity of a disease.  Mild Moderate Severe, 10 variables and a good sample size.

Lets also say I have a decision tree that predicts Severe cases with 90% accuracy but &lt; 50% accuracy for the other two variables.  Another two different classification algorithms that does simliarly but with moderate, and  mild cases.

What is the best way to go about making an ensemble of these three alogrithms to even out the accuracy?  Is this even possible?  Do I need more information?  

Thanks",false,4zeq7f,,0,,false,1473076831,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zeq7f/how_to_combine_three_models_that_classify/,t3_4zeq7f,,false,,
1471948460,MachineLearning,arthomas73,github.com,https://github.com/andrewt3000/MachineLearning/blob/master/cnn4Images.md#convolutional-neural-networks-for-image-processing,0,0,0,0,Convolutional Neural Networks for image processing,,false,4z5tia,,0,,false,1473072304,false,http://b.thumbs.redditmedia.com/Nu0gXRnx7Qxblh7wkCvEUukI7kd05HgB5NlP85TodeA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z5tia/convolutional_neural_networks_for_image_processing/,t3_4z5tia,,false,,
1472652604,MachineLearning,DanielWaterworth,danielwaterworth.github.io,https://danielwaterworth.github.io/posts/what's-wrong-with-autoencoders.html,47,14,14,0,What's wrong with autoencoders?,,false,50hd0n,,0,,false,1473096839,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50hd0n/whats_wrong_with_autoencoders/,t3_50hd0n,,false,,
1472315395,MachineLearning,david-gpu,github.com,https://github.com/david-gpu/srez,59,131,131,0,Image Super-Resolution With a DCGAN - Github,,false,4zuybx,,0,,false,1473085145,false,http://b.thumbs.redditmedia.com/QKYHBQny1FpWldt-PnEh_JBQodIvjKxmYBdDqJzWKQs.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zuybx/image_superresolution_with_a_dcgan_github/,t3_4zuybx,,false,,
1472687795,MachineLearning,gmsc,betterexplained.com,https://betterexplained.com/articles/adept-machine-learning-course/,0,1,1,0,Studying a Course (Machine Learning) with the ADEPT Method,,false,50kbnh,,0,,false,1473098365,false,http://a.thumbs.redditmedia.com/6aeNNXCNq29sGwyRZ-D9AAwD0r-bT6nnv9OYs5jsaj8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50kbnh/studying_a_course_machine_learning_with_the_adept/,t3_50kbnh,,false,,
1471385872,MachineLearning,vanboxel,youtube.com,https://www.youtube.com/watch?v=PeoFPAr084s,0,0,0,0,Modeling self-driving car steering (livestream),,false,4y27qa,,0,,false,1473052214,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y27qa/modeling_selfdriving_car_steering_livestream/,t3_4y27qa,,false,,
1471327401,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xy1b7/bridging_nonlinearities_and_stochastic/,0,6,6,0,Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units,[deleted],false,4xy1b7,,0,,false,1473050099,false,default,t5_2r3gv,false,three,,false,true,,/r/MachineLearning/comments/4xy1b7/bridging_nonlinearities_and_stochastic/,t3_4xy1b7,,false,Research,
1472569438,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50bk8m/research_trends_in_terms_of_publication_statistics/,0,1,1,0,Research trends in terms of publication statistics,[deleted],false,50bk8m,,0,,false,1473093810,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50bk8m/research_trends_in_terms_of_publication_statistics/,t3_50bk8m,,false,,
1470090874,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vp7lw/if_you_knew_keras_theanotensorflow_really_well/,11,0,0,0,"If you knew Keras &amp; Theano/Tensorflow really well, would you ever use SciKit Learn?",[deleted],false,4vp7lw,,0,,false,1473008616,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vp7lw/if_you_knew_keras_theanotensorflow_really_well/,t3_4vp7lw,,false,,
1471803448,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yw8ec/good_reference_for_probabilistic_programming_and/,2,0,0,0,Good reference for probabilistic programming and variational inference?,[removed],false,4yw8ec,,0,,false,1473067437,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yw8ec/good_reference_for_probabilistic_programming_and/,t3_4yw8ec,,false,,
1470969042,MachineLearning,SwampyPk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xbddt/adaboosting_viola_jones_algorithm_breakdown/,1,5,5,0,Adaboosting (Viola Jones) Algorithm - Breakdown &amp; questions,"Hello! I'm trying to get my head around Adaptive boosting, specifically the Viola Jones Facial Detection modified version.

I've tried to outline roughly what it entails, I was hoping that anyone could try validate what I'm about to write and any further questions afterwards.

(Using a positive sample set of faces)

1) Apply Haar like features of all sizes and positions within each 24x24 window.

2) Assign weights to all positive results that pass within a predefined threshold, as 1/N (N being the number of features that passed).

3) Apply passed features to each window and assign higher weights to features pass with higher contrasts.
4) Re-normalise weights so they sum to 1.

5) Generate new thresholds to increase the difficulty for features to pass.

6) Repeat step 3 through 5 till the end of the training set.

7) Build strong classifier from the most successful features (Highest weights) where the classifier includes information of the weights and feature type.


A few questions:


 - Is the strong classifier built after each iteration or series of iterations of the total training sample (maybe used for the cascading afterwards)

 - Does the Viola Jones Algorithm only train using a positive sample set (only faces)

 - On what reasons are weightings modified, popularity? Misclassification? Size of feature (larger features -&gt; Larger weightings etc??)

I might very well be way off the mark, any insight would be appreciated. Please be blunt, I'm only trying to learn.

Thanks in advance!


 ",false,4xbddt,,0,,false,1473038552,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xbddt/adaboosting_viola_jones_algorithm_breakdown/,t3_4xbddt,,false,,
1470288428,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w2z7x/with_em_algorithm_can_you_infer_the_location_and/,5,3,3,0,"With EM algorithm, can you infer the location and variance of each ""peak"" in a pdf? Gaussian Mixture Models?","When I plot my data into bins, there is a frequency of data points per bin, which I can plot with a histogram. Based on this probability density function, I would like to find the maximum likelihood estimation of various parameters associated with this pdf. 

How can I infer the number of ""peaks"" of this pdf? Is this possible using a Gaussian Mixture Model (let's say using Expectation Maximization)? 

I believe in this case, we are inferring the location and variance of each ""peak"" using Gaussian clusters. 

Secondly, how does one infer the number of clusters which model the data best? ",false,4w2z7x,,0,,false,1473015834,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w2z7x/with_em_algorithm_can_you_infer_the_location_and/,t3_4w2z7x,,false,,
1470216680,MachineLearning,saseptim,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vxnyf/language_model_from_dictionary_using_ctc_loss_for/,0,2,2,0,language model from dictionary using ctc loss for speech recognition,"I already have a reasonable model which performs speech recognition using convnets + ctc loss. What would be a good way to use a language model based on a set dictionary of phrases in order to decode the output of the softmax in my net? ",false,4vxnyf,,0,,false,1473013017,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vxnyf/language_model_from_dictionary_using_ctc_loss_for/,t3_4vxnyf,,false,,
1471195116,MachineLearning,rantana,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xp10a/did_nips_ever_do_the_nips_consistency_experiment/,10,9,9,0,"Did NIPS ever do the ""NIPS consistency experiment"" again?","As an outsider looking into how academia works, I find the whole review process fascinating. In industry, I've found sifting through arxiv, /r/MachineLearning and Twitter to be far more useful to me than going through conference proceedings.

But I was told by a few colleagues this year, the review format was drastically different from previous years because of the huge influx in submissions. Specifically, anyone that submitted was also allowed to review.

I can't really tell if this was a very good idea or a very bad idea. But I remember there was the ""NIPS consistency experiment"" a couple of years ago which was very revealing about the randomness in the whole review process. Eric Price wrote a great post about it here:

http://blog.mrtz.org/2014/12/15/the-nips-experiment.html

Does anyone know if there was any followup experiment? I feel like all conferences should be doing these types of experiments just as a gauge of how the field is changing. And since this year there were so many additional reviewers and so many additional submissions, it seems like a good opportunity to do some interesting analysis.",false,4xp10a,,0,,false,1473045513,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4xp10a/did_nips_ever_do_the_nips_consistency_experiment/,t3_4xp10a,,false,Discusssion,
1472401027,MachineLearning,Professional_123,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/5001xk/is_there_a_way_to_approximate_the_hessian_with/,5,8,8,0,Is there a way to approximate the Hessian with complexity of the same order as gradient calculations?,"If not, what are the fastest methods?",false,5001xk,,0,,false,1473087787,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/5001xk/is_there_a_way_to_approximate_the_hessian_with/,t3_5001xk,,false,,
1472354844,MachineLearning,aurelautinuiy,industrialmanlifts.com,http://www.industrialmanlifts.com,0,1,1,0,Industrial Man Lifts Aircraft Maintenance Platforms Ladders,,false,4zxpwa,,0,,false,1473086568,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zxpwa/industrial_man_lifts_aircraft_maintenance/,t3_4zxpwa,,false,,
1472529444,MachineLearning,zhongwenxu,deepmind.com,https://deepmind.com/blog#decoupled-neural-interfaces-using-synthetic-gradients,28,149,149,0,DeepMind new blog: Decoupled Neural Interfaces using Synthetic Gradients,,false,5097cs,,0,,false,1473092605,false,http://b.thumbs.redditmedia.com/HdnhPBvln6MQbT5hfWC51k5WsVIUdhOkjzdIlxlKukI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/5097cs/deepmind_new_blog_decoupled_neural_interfaces/,t3_5097cs,,false,,
1470389243,MachineLearning,rilessx,stackoverflow.com,http://stackoverflow.com/a/17912660/1545917,0,10,10,0,Image Segmentation using Mean Shift explained,,false,4w9lf7,,0,,false,1473019225,false,http://b.thumbs.redditmedia.com/T13G7e4Sba3gBIvDWjc52fJmevhvC1BfwIn_UJi8LsM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w9lf7/image_segmentation_using_mean_shift_explained/,t3_4w9lf7,,false,,
1471883590,MachineLearning,jackerfrinandis,openarticles.com,http://www.openarticles.com/article.php?title=How-Proper-Greasing-of-Machines-Improves-Industrial-Operations?&amp;article=460677,0,1,1,0,How Proper Greasing of Machines Improves Industrial Operations?,,false,4z1dmt,,0,,false,1473070044,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z1dmt/how_proper_greasing_of_machines_improves/,t3_4z1dmt,,false,,
1471544455,MachineLearning,tonylstewart,code.facebook.com,https://code.facebook.com/posts/1438652669495149/fair-open-sources-fasttext,13,43,43,0,"Facebook is open-sourcing fastText, a library designed to help build scalable solutions for text representation and classification.",,false,4ydur0,,0,,false,1473058102,false,http://b.thumbs.redditmedia.com/TFNu61Gli0o8SdBoVjWsilWdKDmll1yByqy4Rn7qXig.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ydur0/facebook_is_opensourcing_fasttext_a_library/,t3_4ydur0,,false,,
1472669643,MachineLearning,vonnik,shop.oreilly.com,http://shop.oreilly.com/product/0636920035343.do,0,4,4,0,O'Reilly's Deep Learning: A Practitioner's Approach,,false,50iu7d,,0,,false,1473097599,false,http://a.thumbs.redditmedia.com/VJXde1o5RCZUZEKC9O22dMF-HOlwmbojTurAIEJusR4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50iu7d/oreillys_deep_learning_a_practitioners_approach/,t3_50iu7d,,false,,
1472337175,MachineLearning,darkconfidantislife,reddit.com,http://reddit.com/r/bvlccaffe,0,0,0,0,A subreddit for Caffe,,false,4zwlcl,,0,,false,1473085989,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zwlcl/a_subreddit_for_caffe/,t3_4zwlcl,,false,,
1471434123,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y5327/why_is_this_place_full_of_a_bunch_up_stuckup_jerks/,0,0,0,0,Why is this place full of a bunch up stuck-up jerks?,[deleted],false,4y5327,,0,,false,1473053662,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y5327/why_is_this_place_full_of_a_bunch_up_stuckup_jerks/,t3_4y5327,,false,,
1471386709,MachineLearning,[deleted],blogs.nvidia.com,https://blogs.nvidia.com/blog/2016/08/16/correcting-some-mistakes/,1,1,1,0,NVIDIA fires shots at Intel,[deleted],false,4y2a4u,,0,,false,1473052247,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y2a4u/nvidia_fires_shots_at_intel/,t3_4y2a4u,,false,,
1472082009,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zfjwl/starting_deep_learning_and_looking_for_good/,1,0,0,0,Starting deep learning and looking for good tutorials to start with,[removed],false,4zfjwl,,0,,false,1473077255,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zfjwl/starting_deep_learning_and_looking_for_good/,t3_4zfjwl,,false,,
1470705658,MachineLearning,wannabe_ee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wto7h/how_to_feature_scale_this_data/,6,0,0,0,How to feature scale this data,"Hi, I am working fraud detection using ENRON dataset. 

Attached are the boxplots of various features. 

http://imgur.com/a/NC5Jc


I am wondering how would you go on about feature scaling this data.
I tried min max scaling and my f1 score is low. However, when I normalize the data, my f1 score increases. 
My next step is to use ""robust scaler""

http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html#sklearn.preprocessing.robust_scale",false,4wto7h,,0,,false,1473029516,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wto7h/how_to_feature_scale_this_data/,t3_4wto7h,,false,,
1470319694,MachineLearning,mithrillion,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w4nxn/reinforcement_learning_qlearning_when_it_is_very/,7,1,1,0,Reinforcement learning (Q-learning) when it is very difficult to reach rewarding states?,"I understand that the gist of Q-learning is using value propagation to pass the reward of goal states to ""nearby"" states so the agent can come up with a policy that prefers a transition into these states. However when the agent is not able to consistently land on these goal states even on full-exploration mode (epsilon=1) for minutes at a time, there will be nothing valuable to propagate back and the algorithm is usually stuck. 

I am facing a problem similar to this: suppose you are teaching an agent to navigate in a maze, but the only reward is at the exit, and the agent usually cannot find the exit in reasonable time using just random walk. How am I supposed to train the agent without injecting too much hard-coded knowledge and making it too similar to A* search? I intuitively think if I start off by feeding relatively easy start states and enforce a step limit, and gradually increase the difficulty, it might help, but I am not certain about this? Any suggestions?",false,4w4nxn,,0,,false,1473016697,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w4nxn/reinforcement_learning_qlearning_when_it_is_very/,t3_4w4nxn,,false,,
1471437038,MachineLearning,meatshell,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y5afa/what_is_the_proper_way_to_annotate_your_image/,16,9,9,0,What is the proper way to annotate your image dataset?,"I'm working to annotate a specific dataset of traffic images that consist of a lot of motorbikes: [example](http://i.imgur.com/BE8pY12.png). I want to detect (or at least, localize) motorbikes in such traffic.
However, first, I need to annotate at least some frames. But since there are a lot of occlusions, I don't know how should I annotate.

Should I annotate the objects up to only where they are visible, like [this](http://i.imgur.com/HrcMz29.png).

Or should I guess where an object might extend to behind the occlusions, like [this](http://i.imgur.com/zj2gSN7.png).

I took a look at PASCAL VOC annotations of the ""person"" class and they chose to go with the first approach. I want to go with this but treating a head and a whole motorbike as the same class seems kinda wrong to me.",false,4y5afa,,0,,false,1473053764,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y5afa/what_is_the_proper_way_to_annotate_your_image/,t3_4y5afa,,false,,
1471010785,MachineLearning,wakkaflokka,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xdop3/nightmares_with_ubuntu_1604_cuda_cudnn_and/,30,20,20,0,"Nightmares with Ubuntu 16.04, CUDA, cuDNN, and Tensorflow - how do I get this to work?","I've spent substantial time trying to figure out how to get TensorFlow (either the Docker container, or locally in Anaconda by building from source) to work with my GPU (GTX 1080).

I've gotten CUDA 7.5 and TensorFlow 0.10 (local and Docker) installed successfully, but no matter what I do or what instructions I follow, I get errors when TensorFlow calls cuDNN (version 4 or version 5.1). They range depending on what I've done - ""could not find cudnnCreate in cudnn DSO"" is among the errors. After the error, the Python kernel dies.

I'm reaching out to the Reddit community for guidance on this, because I've killed too much time on this. How do I get Tensorflow, Docker or Anaconda, running successfully in Ubuntu 16.04 with my GPU?

UPDATE: Following either of these methods, I still can't get it to work:
https://devtalk.nvidia.com/default/topic/936429/-solved-tensorflow-with-gpu-in-anaconda-env-ubuntu-16-04-cuda-7-5-cudnn-/#4880949

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#optional-install-cuda-gpus-on-linux

UPDATE 2: For now I'm just using this Docker container: https://github.com/saiprashanths/dl-docker
Seems to work great when run with nvidia-docker. Thank you for all the comments!",false,4xdop3,,0,,false,1473039740,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xdop3/nightmares_with_ubuntu_1604_cuda_cudnn_and/,t3_4xdop3,,false,,
1470074166,MachineLearning,beijingspacetech,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vnt5w/what_is_the_best_way_to_use_gensim_for_similarity/,2,0,0,0,What is the best way to use Gensim for Similarity? Am I using indexes correctly?,"I started a project testing Gensim (HDP) and have been frustrated using the Gensim Indexes for Similarity/MatrixSimilarity queries. To my best current understanding, indexes are mostly just the order that you passed documents to the model. You must then save that list of documents to preserve the order for later when extracting similarity scores or tags.


Due to data cleaning, dropped documents and added documents, I find it rather difficult to keep track. Also, if I'm not 100% sure I got the two orders right, I have to start over.


I've been through the tutorials, and it looks like LDA might have some better index handling, but perhaps my process or methodology is off here. Hopefully you have some great advice! I would really just love it if I could also pass along a unique document ID.",false,4vnt5w,,0,,false,1473007874,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vnt5w/what_is_the_best_way_to_use_gensim_for_similarity/,t3_4vnt5w,,false,,
1470524775,MachineLearning,nex_jeb,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wi9ly/annotations_and_dataset_for_pixelwise_water/,1,5,5,0,Annotations and Dataset for pixelwise water segmentation ?,"Hello,
 I am willing to train a Neural Net for water segmentation for Aerial images. Are there any pixel annotations and dataset online ?

Thank you",false,4wi9ly,,0,,false,1473023689,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wi9ly/annotations_and_dataset_for_pixelwise_water/,t3_4wi9ly,,false,,
1470536372,MachineLearning,terrytangyuan,terrytangyuan.github.io,http://terrytangyuan.github.io/2016/08/06/tensorflow-not-just-deep-learning/,5,8,8,0,TensorFlow - Not Just for Deep Learning,,false,4wj067,,0,,false,1473024067,false,http://b.thumbs.redditmedia.com/oWpdLC7ROH9edCE85YmFgpa4f1rO0e5LoYB9-vf3GnI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wj067/tensorflow_not_just_for_deep_learning/,t3_4wj067,,false,,
1470648164,MachineLearning,Sawetwipes,classifiedads.com,https://www.classifiedads.com/cleaning/c43650v5zcb2,0,1,1,0,Wet-wipes &amp; Sanitizing wipes,,false,4wpafg,,0,,false,1473027284,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wpafg/wetwipes_sanitizing_wipes/,t3_4wpafg,,false,,
1470835756,MachineLearning,belangrijk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x242q/what_is_your_favorite_machine_learning_algorithm/,10,0,0,0,What is your favorite machine learning algorithm and why?,,false,4x242q,,0,,false,1473033800,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x242q/what_is_your_favorite_machine_learning_algorithm/,t3_4x242q,,false,,
1470841390,MachineLearning,jainadi341,blog.paralleldots.com,http://blog.paralleldots.com/technology/machine-learning-eli5/,0,0,0,0,Machine Learning Explained Like I'm Five,,false,4x2joc,,0,,false,1473034019,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x2joc/machine_learning_explained_like_im_five/,t3_4x2joc,,false,,
1472510873,MachineLearning,perecastor,youtu.be,https://youtu.be/fVdiWqUfn7Y,0,0,0,0,Play Video Games with Machine Learning,,false,507u24,,0,,false,1473091908,false,http://b.thumbs.redditmedia.com/o2ho1lIF5LjcmBAYtPkxXz80ViFZ_yV31NaRacz8gOo.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/507u24/play_video_games_with_machine_learning/,t3_507u24,,false,,
1471386928,MachineLearning,PM_YOUR_NIPS_PAPERS,blogs.nvidia.com,https://blogs.nvidia.com/blog/2016/08/16/correcting-some-mistakes/,41,152,152,0,NVIDIA fires shots at Intel,,false,4y2arm,,0,,false,1473052256,false,http://b.thumbs.redditmedia.com/K4uRp9z96_BhbX7gHwXghiVRoYw_Z2Mhxt38GFZm6bQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y2arm/nvidia_fires_shots_at_intel/,t3_4y2arm,,false,,
1471388209,MachineLearning,[deleted],plus.google.com,https://plus.google.com/111502245593180635268/posts/RVFUJeiXWaE,0,0,0,0,Apple Acquires Machine Learning Startup Turi For $200 Million,[deleted],false,4y2e82,,0,,false,1473052304,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y2e82/apple_acquires_machine_learning_startup_turi_for/,t3_4y2e82,,false,,
1470913018,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x7fni/when_are_nips_decisions_out/,0,0,0,0,When are NIPS decisions out?,[removed],false,4x7fni,,0,,false,1473036519,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x7fni/when_are_nips_decisions_out/,t3_4x7fni,,false,,
1470628574,MachineLearning,darkconfidantislife,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4woc06/explanation_of_neat_and_hyperneat/,8,0,0,0,Explanation of NEAT and HyperNEAT?,"Hey guys, I was wondering if there is any place where I could find an in-depth, but not a full-fledged paper, explanation of NEAT and HyperNEAT. Also, if anyone has the original paper that would be helpful as well :)
Thanks in advance. ",false,4woc06,,0,,false,1473026796,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4woc06/explanation_of_neat_and_hyperneat/,t3_4woc06,,false,,
1470326461,MachineLearning,sup6978,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w5865/when_is_andrew_ngs_machine_learning_yearning_book/,14,12,12,0,When is Andrew Ng's Machine Learning Yearning book draft supposed to be out?,"I signed up for it a while ago and I thought it was going to come out a week or a couple of weeks after that. ",false,4w5865,,0,,false,1473016988,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w5865/when_is_andrew_ngs_machine_learning_yearning_book/,t3_4w5865,,false,,
1471543455,MachineLearning,Gus_Bodeen,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ydr8x/ml_used_to_solve_ml/,14,12,12,0,ML used to solve ML,"Could machine learning be used to identify the ""best fit"" activation function for X number of hidden and output neurons with Y weightings?

For instance: To advance the use of ML, it will need to be approachable for the average business intelligence analyst. Could a website like [tensorflow playground](http://playground.tensorflow.org) automatically choose the most effective configuration settings before continuing on with recursion? ",false,4ydr8x,,0,,false,1473058051,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ydr8x/ml_used_to_solve_ml/,t3_4ydr8x,,false,,
1471368275,MachineLearning,pm_me_svms,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y0oio/deep_structured_energybased_models_for_anomaly/,1,6,6,0,"Deep structured energy-based models for anomaly detection, implementation?","https://arxiv.org/pdf/1605.07717v2.pdf

Has anyone successfully implemented this? I tried doing this in Theano/Lasagne with MNIST and I don't think I have done it correctly, even if the math is seemingly simple and straightforward. The authors don't seem to have any code available as reference.

I have a notebook here where I attempt to implement the energy model for MNIST:

https://github.com/christopher-beckham/cnn-energy-model/blob/master/train.ipynb

At the bottom of the notebook is a boxplot for the '3' digits vs the '7' digits, and you can see that while they have different energy distributions, there is not a clear separation between them. It gives me the feeling I haven't done something correctly... any ideas?

Side note: train.py is more up-to-date, and I have tried to make the network denoising as well by adding a GaussianNoiseLayer. Didn't seem to make a difference in the distribution of energies.",false,4y0oio,,0,,false,1473051436,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y0oio/deep_structured_energybased_models_for_anomaly/,t3_4y0oio,,false,,
1471573185,MachineLearning,sanosukesagara,arxiv.org,https://arxiv.org/abs/1608.02236,1,11,11,0,[1608.02236] Bootstrapping Face Detection with Hard Negative Examples,,false,4ygc4l,,0,,false,1473059349,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4ygc4l/160802236_bootstrapping_face_detection_with_hard/,t3_4ygc4l,,false,Research,
1470833072,MachineLearning,reworksophie,re-work.co,https://re-work.co/blog/deep-learning-andrej-karpathy-openai-recurrent-neural-networks-image-classification,0,1,1,0,"Visualizing &amp; Understanding Recurrent Neural Networks with Andrej Karpathy, OpenAI",,false,4x1xky,,0,,false,1473033707,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x1xky/visualizing_understanding_recurrent_neural/,t3_4x1xky,,false,,
1472040691,MachineLearning,Kiuhnm,github.com,https://github.com/mtomassoli/papers/blob/master/tensor_diff_calc.pdf,5,20,20,0,[tutorial] Matrix Differential Calculus with Tensors (for Machine Learning),,false,4zc8uv,,0,,false,1473075571,false,http://b.thumbs.redditmedia.com/YID9TOX-52aCG_mJNc0hQPJcDS1jlShdNOs4VuRHzhE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zc8uv/tutorial_matrix_differential_calculus_with/,t3_4zc8uv,,false,,
1472042095,MachineLearning,adeshpande3,adeshpande3.github.io,https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html,7,0,0,0,The 9 Deep Learning Papers You Need To Know About!,,false,4zcc7d,,0,,false,1473075618,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zcc7d/the_9_deep_learning_papers_you_need_to_know_about/,t3_4zcc7d,,false,,
1470063633,MachineLearning,[deleted],nlpers.blogspot.hr,http://nlpers.blogspot.hr/2016/07/a-quick-comment-on-structured-input-vs.html,2,3,3,0,A quick comment on structured input vs structured output learning - nlpers,,false,4vmwnm,,0,,false,1473007403,false,http://b.thumbs.redditmedia.com/_j4A5XwrDJM4O7HIqauBC1XKWFLtKNeHB68_tPZhPHY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vmwnm/a_quick_comment_on_structured_input_vs_structured/,t3_4vmwnm,,false,,
1470675975,MachineLearning,jonathan881,youtube.com,https://www.youtube.com/channel/UCReA3y5zzWH0XROkchmoqNA,1,1,1,0,Small YouTube channel dedicated to DN videos,,false,4wr6za,,0,,false,1473028251,false,http://b.thumbs.redditmedia.com/y95pPwRwUuxpEtcA7T3VqlBCJJP08oIAtr9lWfO67fU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wr6za/small_youtube_channel_dedicated_to_dn_videos/,t3_4wr6za,,false,,
1470335291,MachineLearning,gwulfs,github.com,https://github.com/facebookresearch/fastText,12,30,30,0,facebookresearch/fastText: Library for fast text representation and classification.,,false,4w6015,,0,,false,1473017387,false,http://b.thumbs.redditmedia.com/UWmwvgJwx_xpz-nvGu_9PIFEMjEuB3J2OOMmNJYR_Sg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w6015/facebookresearchfasttext_library_for_fast_text/,t3_4w6015,,false,,
1471126969,MachineLearning,gwern,arxiv.org,https://arxiv.org/abs/1606.09282,17,70,70,0,"""Learning without Forgetting"", Li &amp; Hoiem 2016",,false,4xlc1l,,0,,false,1473043634,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4xlc1l/learning_without_forgetting_li_hoiem_2016/,t3_4xlc1l,,false,Research,
1471462661,MachineLearning,vonnik,medium.com,https://medium.com/autonomous-agents/how-to-train-your-neuralnetwork-for-wine-tasting-1b49e0adff3a,0,11,11,0,How to Train a Neural Network for Wine Tasting?,,false,4y7npj,,0,,false,1473054958,false,http://b.thumbs.redditmedia.com/s1R12p8xuekNHNb8iVphSjD4fb1L_4EvodP02R4b7po.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y7npj/how_to_train_a_neural_network_for_wine_tasting/,t3_4y7npj,,false,,
1470690858,MachineLearning,Mr__Christian_Grey,stackoverflow.com,http://stackoverflow.com/questions/38838456/tensorflow-back-prop-valueerror-setting-an-array-element-with-a-sequence,0,0,0,0,Tensorflow back_prop ValueError: setting an array element with a sequence,,false,4wsjgz,,0,,false,1473028939,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wsjgz/tensorflow_back_prop_valueerror_setting_an_array/,t3_4wsjgz,,false,,
1472079748,MachineLearning,cptncrnch,kickstarter.com,https://www.kickstarter.com/projects/thewolfe/the-wolfe-supercharge-your-laptop,6,0,0,0,External GPU Wolfe Pro runs on an NVIDIA GTX 970 GPU - 1664 cores operating at 1050MHz (Kickstarter),,false,4zfdn8,,0,,false,1473077165,false,http://a.thumbs.redditmedia.com/zzqCTRptMRxAg-rLYgUGCuqqPnKXmtvQzdTGisi04F8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zfdn8/external_gpu_wolfe_pro_runs_on_an_nvidia_gtx_970/,t3_4zfdn8,,false,,
1470226734,MachineLearning,[deleted],enaible.com,http://www.enaible.com,4,0,0,0,enaible - Artificial Intelligence for Image Analysis! Beta request available.,[deleted],false,4vy6v5,,0,,false,1473013301,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vy6v5/enaible_artificial_intelligence_for_image/,t3_4vy6v5,,false,,
1472417965,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/501dvv/overfitting_on_training_set_improves_validation/,5,0,0,0,Overfitting on training set improves validation set performance.,[deleted],false,501dvv,,0,,false,1473088499,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/501dvv/overfitting_on_training_set_improves_validation/,t3_501dvv,,false,,
1472053047,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zd6vi/given_a_document_corpus_that_has_already_been/,4,3,3,0,"Given a document corpus that has already been vectorized and a query that consists of words that do not exist in the corpus, how do you calculate cosine similarity?",[deleted],false,4zd6vi,,0,,false,1473076049,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zd6vi/given_a_document_corpus_that_has_already_been/,t3_4zd6vi,,false,,
1472098182,MachineLearning,Konikukore,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zgp5y/koniku_neurogrammer_developers_econference_2728/,0,1,1,0,"Koniku Neurogrammer Developers e-Conference, 27-28 Aug.",[removed],false,4zgp5y,,0,,false,1473077837,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zgp5y/koniku_neurogrammer_developers_econference_2728/,t3_4zgp5y,,false,,
1470784005,MachineLearning,nospoko,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wz3in/artificial_intelligence_learning_to_play/,0,1,1,0,Artificial intelligence learning to play progressive metal,[removed],false,4wz3in,,0,,false,1473032278,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wz3in/artificial_intelligence_learning_to_play/,t3_4wz3in,,false,,
1471743875,MachineLearning,funnyornotletitrot,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ysh35/mobile_sensor_data_prediction/,4,0,0,0,Mobile Sensor Data Prediction,"I am a programmer who now has to do predictions based on sensor data collected using mobile devices.

So my problem broken into simple statements will be:

- Collect Sensor Data
- Train Something? (What?) //My lack of Data Science foundations cuts me here.
- Predict something using real-time sensor data based on previous training.
- So how can this be done? Can it be done using Particle Swarm Optimization, Monte-Carlo simulations etc.? Please note that I have 0 data science background, any suggestion will really be of help.

What would be the easiest way to do, if I want to do all the 3 steps in the device itself (as that is the main challenge!). Here are the data set tables that I might be looking at are Accelerometer, gyro, proximity, compass, barometer:

    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Sensor              | Description                                                                                                                                                                         |
    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | TYPE_ACCELEROMETER  | Measures the acceleration force in m/s2 that is applied to a device on all three physical axes (x, y, and z), including the force of gravity.                                       |
    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | TYPE_GYROSCOPE      | Measures a device's rate of rotation in rad/s around each of the three physical axes (x, y, and z).                                                                                 |
    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | TYPE_PROXIMITY      | Measures the proximity of an object in cm relative to the view screen of a device. This sensor is typically used to determine whether a handset is being held up to a person's ear. |
    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | TYPE_PRESSURE       | Measures the ambient air pressure in hPa or mbar.                                                                                                                                   |
    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | TYPE_MAGNETIC_FIELD | Measures the ambient geomagnetic field for all three physical axes (x, y, z) in μT.                                                                                                 |
    +---------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I know, this is quite diverse and I have to use either or any combination of this to predict the phones location (bag, pocket etc.). What would be your suggestions?

Thanks.",false,4ysh35,,0,,false,1473065532,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ysh35/mobile_sensor_data_prediction/,t3_4ysh35,,false,,
1471581485,MachineLearning,iamquah,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ygy9n/speed_of_an_autoencoder_in_dimensionality/,2,0,0,0,Speed of an autoencoder in dimensionality reduction? Or more generally speed of a Neural net compared to other dimensionality reduction methods like SVD,"Hey all,

So I'm not sure if I'm imagining it but I think that I read once about how training of a neural net can be slow, but once you've trained it, predictions are fast? 

So I was talking to someone about dimensionality reduction of datasets with extremely large dimensions and the person I was talking to said that an AE will never be faster than SVD. I understand that in terms of speed there are many metrics (training time, prediction time, and so forth) but can anyone point me to any material that concretely shows that one method is faster than the other?

Also, what is the general consensus about trying to interpret the reduced dimensions produced by an AE? Is it meaningful in any way or is it just noise that only makes sense to the model? the famous [visualizing and understanding Conv nets](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) paper came to mind but I'm not sure if the argument can be carried over?",false,4ygy9n,,0,,false,1473059659,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ygy9n/speed_of_an_autoencoder_in_dimensionality/,t3_4ygy9n,,false,,
1472240836,MachineLearning,jewdai,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zqhl2/machine_learning_sites_that_just_jump_straight/,10,0,0,0,Machine Learning sites that just jump straight into the math and save the description for later.,"I'm running into dozens of description of machine learning algorithms without the substance and just the description. 

I know what the fuck a neural network is, I just want to see the math and algorithm behind calculating it. I don't need the umpteenth time describing it like a brain cell. 

Any good resources that cut out the bullshit and get to the point. (some bullshit is allowed only after the mathematical calculation point is shared)",false,4zqhl2,,0,,false,1473082848,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zqhl2/machine_learning_sites_that_just_jump_straight/,t3_4zqhl2,,false,,
1471917643,MachineLearning,doomie,arxiv.org,http://arxiv.org/abs/1608.06019,5,15,15,0,Domain Separation Networks,,false,4z450l,,0,,false,1473071445,false,default,t5_2r3gv,false,three,googlebrain,false,false,,/r/MachineLearning/comments/4z450l/domain_separation_networks/,t3_4z450l,Google Brain,false,Research,
1470809910,MachineLearning,mixmachinery,mixmachinery.com,http://www.mixmachinery.com/news/How-to-choose-powder-mixing-vessel.html,1,1,1,0,How to choose powder mixing vessel?,,false,4x0sib,,0,,false,1473033133,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0sib/how_to_choose_powder_mixing_vessel/,t3_4x0sib,,false,,
1471674586,MachineLearning,ssreekanth2000,wolframcloud.com,https://www.wolframcloud.com/objects/user-e426755a-0ecd-4ac1-83c7-c66b8e539b28/Image%20Classifier,9,4,4,0,Satellite image classifier I made. Any suggestions?,,false,4ynodu,,0,,false,1473063088,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ynodu/satellite_image_classifier_i_made_any_suggestions/,t3_4ynodu,,false,,
1470291118,MachineLearning,evc123,scaledml.org,http://scaledml.org/2016/slides/ilya.pdf,11,36,36,0,Recent Progress in Generative Modeling - Ilya Sutskever @ OpenAI,,false,4w34fa,,0,,false,1473015907,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w34fa/recent_progress_in_generative_modeling_ilya/,t3_4w34fa,,false,,
1470915245,MachineLearning,reworksophie,youtube.com,https://www.youtube.com/watch?v=InYNSzVblZQ,0,1,1,0,"Video interview with deep learning expert Yoshua Bengio, at the RE•WORK Deep Learning Summit, Boston 2016",,false,4x7jk7,,0,,false,1473036574,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x7jk7/video_interview_with_deep_learning_expert_yoshua/,t3_4x7jk7,,false,,
1470657789,MachineLearning,riomus,sparkling.ml,http://sparkling.ml,1,2,2,0,Library extending Spark GraphX with features that can be useful for research purposes but not only.,,false,4wpsl9,,0,,false,1473027538,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wpsl9/library_extending_spark_graphx_with_features_that/,t3_4wpsl9,,false,,
1470393296,MachineLearning,NeatMonster,youtu.be,https://youtu.be/jAQNiL3o5lU?t=3,3,11,11,0,Update: Evolving blob creatures using Neural Networks,,false,4w9rul,,0,,false,1473019316,false,http://b.thumbs.redditmedia.com/htrnKGGpWklB8KzG35Pv9U27wMC3QnOINZt5IMNylzk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w9rul/update_evolving_blob_creatures_using_neural/,t3_4w9rul,,false,,
1471807026,MachineLearning,_rusht,rushtehrani.com,https://www.rushtehrani.com/post/using-kubernetes-api/,1,1,1,0,Using Kubernetes API Go SDK to Launch a Jupyter Notebook,,false,4ywju1,,0,,false,1473067597,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ywju1/using_kubernetes_api_go_sdk_to_launch_a_jupyter/,t3_4ywju1,,false,,
1472555325,MachineLearning,marcjschmidt,academictorrents.com,http://academictorrents.com/browse.php?cat=6,1,78,78,0,Academic Torrents - meanwhile with tons of datasets.,,false,50akaa,,0,,false,1473093297,false,http://b.thumbs.redditmedia.com/LEPfzBQRneHiEXMUKo0u_JWegLfLIiXAZsfJtOmw1PU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50akaa/academic_torrents_meanwhile_with_tons_of_datasets/,t3_50akaa,,false,,
1470764016,MachineLearning,amplifier_khan,gab41.lab41.org,https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.jzw0hmaqs,0,1,1,0,Deep Compression: 3 Ways to Compress a Neural Network,,false,4wxeho,,0,,false,1473031416,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wxeho/deep_compression_3_ways_to_compress_a_neural/,t3_4wxeho,,false,,
1472568150,MachineLearning,DanielHendrycks,news.berkeley.edu,https://news.berkeley.edu/2016/08/29/center-for-human-compatible-artificial-intelligence/,1,25,25,0,UC Berkeley launches Center for Human-Compatible Artificial Intelligence,,false,50bgi6,,0,,false,1473093758,false,http://a.thumbs.redditmedia.com/a4fibYDtDUg45xXOMmwO2sGe1f6zvtci_2RlZujc7v0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50bgi6/uc_berkeley_launches_center_for_humancompatible/,t3_50bgi6,,false,,
1471024194,MachineLearning,jackerfrinandis,alemitegreasepump.blogspot.in,http://alemitegreasepump.blogspot.in/2016/08/alemite-grease-pump-ideal-solution-for.html,0,1,1,0,Alemite Grease Pump – An Ideal Solution For Varied Lubrication Needs,,false,4xetdu,,0,,false,1473040322,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xetdu/alemite_grease_pump_an_ideal_solution_for_varied/,t3_4xetdu,,false,,
1471630917,MachineLearning,ronzohar,youtube.com,https://www.youtube.com/watch?v=OQvNfxnV1WY,35,90,90,0,Automatic video colorization using deep learning for Lukas Graham's 7 years,,false,4ykf5s,,0,,false,1473061423,false,http://b.thumbs.redditmedia.com/4qin_zhAPb9mLH8IhVMxQl7sDCwzN3rsX2Z0N2gEK6M.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ykf5s/automatic_video_colorization_using_deep_learning/,t3_4ykf5s,,false,,
1471386069,MachineLearning,bbsome,nextplatform.com,http://www.nextplatform.com/2016/08/16/intel-ssf-optimizations-boost-machine-learning/,0,0,0,0,If intel could bring down the prize a bit this can have high impact on gpu usage,,false,4y289z,,0,,false,1473052221,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y289z/if_intel_could_bring_down_the_prize_a_bit_this/,t3_4y289z,,false,,
1471160237,MachineLearning,[deleted],sebastianraschka.com,http://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html,0,1,1,0,"Model evaluation, model selection, and algorithm selection in machine learning",[deleted],false,4xn90r,,0,,false,1473044613,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xn90r/model_evaluation_model_selection_and_algorithm/,t3_4xn90r,,false,,
1472471609,MachineLearning,newbornking999,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/504ms0/what_is_the_good_books_for_pre_bishop_and_machine/,0,1,1,0,what is the good books for pre- Bishop and machine learning books by murphy,[removed],false,504ms0,,0,,false,1473090196,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/504ms0/what_is_the_good_books_for_pre_bishop_and_machine/,t3_504ms0,,false,,
1470410273,MachineLearning,cypherx,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4waw0g/questions_about_convolutional_rnns/,6,10,10,0,Questions about Convolutional RNNs,"I have a question about a few models I've seen but haven't understood completely. If you're classifying sequences of varying lengths then it seems reasonable to (1) embed sequence elements in a vector space (2) perform convolutions to capture local structure (3) feed the sequence of convolutional activations to an RNN to capture longer-range interactions.

Are there any good papers or tutorials which discuss these kinds of models in detail? 

Specifically I'm hoping to answer some of the following:

1) Are the convolutions typically 1D (applied along only one embedding dimension) or 2D (mixing all the components of embedding vectors)?

2) Is there any reason to preserve convolutional activations which overlap only part of the sequence (i.e. border_mode=""same"")? 

3) Is it useful to add explicit start and stop symbols to each sequence (which would then have their own embedded representations)? 

4) If using convolutions of multiple widths with a ""valid"" border mode then the number of timesteps from each convolution size will differ. How can these activation sequences of differing lengths be used as inputs to the same RNN layer? Should each conv size have its own downstream RNN, with a merge of outputs afterward?

5) Purely a detail of implementation, but how can convolutional RNNs be implemented in Keras when the convolution classes don't support masking (and thus seem to require that every input be of the same maximal length). 
",false,4waw0g,,0,,false,1473019887,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4waw0g/questions_about_convolutional_rnns/,t3_4waw0g,,false,,
1471631471,MachineLearning,mayank1123,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ykgum/is_a_course_on_real_analysis_important_for/,27,15,15,0,Is a course on Real Analysis important for research in Machine Learning ?,"Pardon me for the long post but i would appreciate any help whatsoever. 
I am currently a Masters Student in Computer Vision/machine learning and I am thinking of applying for Phd this December. Somebody suggested me to take a course on Real Analysis and then maybe after joining the PhD follow it up with functional analysis this semester for 2 reasons: 
1) It might show that I am serious about research in ML.
2) It might improve my chances of getting in a decent school. 

Recent papers in Computer Vision eg. most from CVPR 2016 ( something like this http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Noh_Image_Question_Answering_CVPR_2016_paper.pdf ) do not require much of mathematical understanding other than some basic concepts of deep learning. This might change in a couple of years and the research might require more theoretical understanding. eg. http://arxiv.org/abs/1602.04485

I can read about this stuff on my own. I have tried it and it works. 

My question is : Should I invest money in the course ($4000) to improve my chances for a good Phd ? I am on a student loan. 

Thank you for your patience :) ",false,4ykgum,,0,,false,1473061447,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ykgum/is_a_course_on_real_analysis_important_for/,t3_4ykgum,,false,,
1471384458,MachineLearning,coffeecoffeecoffeee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y23hw/do_you_prefer_caret_or_mlr_as_a_generalized_r/,10,1,1,0,Do you prefer Caret or MLR as a generalized R machine learning framework?,"I'm about to run a bunch of machine learning experiments and decided to use caret.  However I recently discovered mlr, which seems to have some nice features, like including costs, optimizing multiple hyperparameters at the same time, and dealing with class imbalance.  Which do you guys tend to prefer?",false,4y23hw,,0,,false,1473052153,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y23hw/do_you_prefer_caret_or_mlr_as_a_generalized_r/,t3_4y23hw,,false,,
1471847563,MachineLearning,john_philip,octoparse.com,http://www.octoparse.com/tutorial/how-to-extract-data-from-airbnb/,2,0,0,0,How to Extract Data from Airbnb?,,false,4yz9ci,,0,,false,1473068959,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yz9ci/how_to_extract_data_from_airbnb/,t3_4yz9ci,,false,,
1471418812,MachineLearning,erkaman,erkaman.github.io,https://erkaman.github.io/regl-cnn/src/demo.html,13,38,38,0,I implemented GPU-accelerated Digit Recognition with WebGL!,,false,4y4bd8,,0,,false,1473053275,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y4bd8/i_implemented_gpuaccelerated_digit_recognition/,t3_4y4bd8,,false,,
1471852361,MachineLearning,vvpreetham,medium.com,https://medium.com/autonomous-agents/part-2-error-analysis-the-wild-west-algorithms-to-improve-neuralnetwork-accuracy-6121569e66a5#.946v9qvtr,2,1,1,0,Part-2: Error Analysis — The Wild West. Algorithms to Improve #NeuralNetwork Accuracy and break from Overfitting.,,false,4yzgz5,,0,,false,1473069065,false,http://b.thumbs.redditmedia.com/BakktT52TVquU10r4731m4p6sFlGluMgHQmz8xRru-E.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yzgz5/part2_error_analysis_the_wild_west_algorithms_to/,t3_4yzgz5,,false,,
1472472211,MachineLearning,JKSamir,hackerrank.com,https://www.hackerrank.com/machine-learning-codesprint?utm_source=kaggle&amp;utm_medium=jobposting&amp;utm_campaign=machine-learning-codesprint,1,7,7,0,HackerRank's Machine Learning CodeSprint,,false,504o1b,,0,,false,1473090214,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/504o1b/hackerranks_machine_learning_codesprint/,t3_504o1b,,false,,
1472645521,MachineLearning,beamsearch,arxiv.org,http://arxiv.org/abs/1608.08225,14,40,40,0,[1608.08225] Why does deep and cheap learning work so well?,,false,50gurv,,0,,false,1473096578,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/50gurv/160808225_why_does_deep_and_cheap_learning_work/,t3_50gurv,,false,Research,
1471354303,MachineLearning,colorizrr,itunes.apple.com,https://itunes.apple.com/us/app/colorize-automatically-colorize/id1137245384?mt=8,1,5,5,0,Automatically Colorize Black &amp; White Photos using Artificial Intelligence (iOS App),,false,4xzhvc,,0,,false,1473050834,false,http://b.thumbs.redditmedia.com/sZwouNZntix2kgRRJImjOdlFInPyH9gH0_uc06hxyqE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xzhvc/automatically_colorize_black_white_photos_using/,t3_4xzhvc,,false,,
1471287098,MachineLearning,ConspiracyLurr,jaan.io,https://jaan.io/unreasonable-confusion/,12,44,44,0,Explainer of variational autoencoders from a neural nets and Bayesian perspective - would love to get feedback!,,false,4xv5b5,,0,,false,1473048626,false,http://a.thumbs.redditmedia.com/n3mjxx91JCkxUFPNmo8C3sII9-pNLrP_AhGPqu9ba18.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xv5b5/explainer_of_variational_autoencoders_from_a/,t3_4xv5b5,,false,,
1471886922,MachineLearning,swentso,youtube.com,https://www.youtube.com/watch?v=TiPes04cK-A,2,4,4,0,"[Youtube] A Look at the Original Roots of Artificial Intelligence, Cognitive Science, and Neu",,false,4z1o5b,,0,,false,1473070194,false,http://b.thumbs.redditmedia.com/qB7c3nv0qGaygmDH6dkPf1y2fWn-mVrnZn9Ipqfq5IE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z1o5b/youtube_a_look_at_the_original_roots_of/,t3_4z1o5b,,false,,
1472062124,MachineLearning,alxndrkalinin,research.googleblog.com,https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html,3,61,61,0,Text summarization with TensorFlow | Google Research Blog,,false,4zdx8i,,0,,false,1473076421,false,http://a.thumbs.redditmedia.com/vn9ro9fL3zOIIO6r-0iT3wJLIQ_9ixeXkq6toM63_u4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zdx8i/text_summarization_with_tensorflow_google/,t3_4zdx8i,,false,,
1470248395,MachineLearning,LeavesBreathe,github.com,https://github.com/LeavesBreathe/tensorflow_with_latest_papers#minimal-gated-unit-recurrent-neural-network,0,1,1,0,Minimal Gated Unit For RNN -- Implemented in TensorFlow,,false,4vzyqh,,0,,false,1473014268,false,http://b.thumbs.redditmedia.com/VJ3ULb8XPZLolp7DAkjefrNpN2E-LplSlvokitZLtgE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vzyqh/minimal_gated_unit_for_rnn_implemented_in/,t3_4vzyqh,,false,,
1470250414,MachineLearning,bcaulfield,blogs.nvidia.com,https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/,1,2,2,0,"What’s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?",,false,4w05bl,,0,,false,1473014360,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w05bl/whats_the_difference_between_artificial/,t3_4w05bl,,false,,
1471552243,MachineLearning,alexeyr,arxiv.org,https://arxiv.org/abs/1608.04980,0,1,1,0,Mollifying Networks,,false,4yel9n,,0,,false,1473058472,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yel9n/mollifying_networks/,t3_4yel9n,,false,,
1471466969,MachineLearning,benhamner,blog.kaggle.com,http://blog.kaggle.com/2016/08/17/making-kaggle-the-home-of-open-data/,6,131,131,0,Today Kaggle Launched its Open Data Platform. Publish your data for the community to explore,,false,4y82cr,,0,,false,1473055162,false,http://a.thumbs.redditmedia.com/NWaJNcrtR8uduxjOd4j1EmZptIGdKdXP8Z2FexmOrb0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y82cr/today_kaggle_launched_its_open_data_platform/,t3_4y82cr,,false,,
1472581378,MachineLearning,jonbruner,oreilly.com,https://www.oreilly.com/ideas/what-are-bots-heres-the-background,3,9,9,0,New O'Reilly podcast on AI-driven chatbots,,false,50clot,,0,,false,1473094371,false,http://b.thumbs.redditmedia.com/oxsHBPBlf9YMrSc2YbqS24v4DwzxVTVzTdccjTVqK4s.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50clot/new_oreilly_podcast_on_aidriven_chatbots/,t3_50clot,,false,,
1471902696,MachineLearning,7sider,youtube.com,https://www.youtube.com/watch?v=D5tDubyXLrQ,1,1,1,0,GopherCon 2016 - Go for Data Science,,false,4z308g,,0,,false,1473070873,false,http://b.thumbs.redditmedia.com/Csau8lnN6rNOtu_wu5NFPczur2ds8JYrYRnNd7CWsMk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z308g/gophercon_2016_go_for_data_science/,t3_4z308g,,false,,
1471875687,MachineLearning,[deleted],nextplatform.com,http://www.nextplatform.com/2016/08/22/intel-tweaking-xeon-phi-deep-learning/,0,1,1,0,Why Intel is Tweaking Xeon Phi Supercomputing Chip for Deep Learning,[deleted],false,4z0qtq,,0,,false,1473069719,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z0qtq/why_intel_is_tweaking_xeon_phi_supercomputing/,t3_4z0qtq,,false,,
1472090785,MachineLearning,crashedand,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zg75m/learning_to_rank/,0,0,0,0,Learning to rank,[removed],false,4zg75m,,0,,false,1473077585,false,default,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4zg75m/learning_to_rank/,t3_4zg75m,,false,Discusssion,
1470798817,MachineLearning,nhrahi_iut,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x05t1/what_are_the_state_of_the_art_deep_unsupervised/,0,1,1,0,What are the state of the art deep unsupervised learning algorithms?,[removed],false,4x05t1,,0,,false,1473032816,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x05t1/what_are_the_state_of_the_art_deep_unsupervised/,t3_4x05t1,,false,,
1472092831,MachineLearning,OldeElk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zgcg4/discussion_some_problems_met_when_using/,1,0,0,0,[Discussion] Some problems met when using TensorFlow RNN on different datasets,[removed],false,4zgcg4,,0,,false,1473077659,false,default,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4zgcg4/discussion_some_problems_met_when_using/,t3_4zgcg4,,false,Discussion,
1471472303,MachineLearning,m09az,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y8jve/i_want_to_apply_unsupervised_learning_through_my/,0,1,1,0,"I want to apply unsupervised learning through my customer data in to examine if there are any patterns or particular offers that attract them, any suggestions?",[removed],false,4y8jve,,0,,false,1473055417,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y8jve/i_want_to_apply_unsupervised_learning_through_my/,t3_4y8jve,,false,,
1472327764,MachineLearning,infosecds,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zvwq2/going_from_1_to_2_hidden_layers_in_neural_net/,0,1,1,0,Going from 1 to 2 hidden layers in neural net increases cost?,[removed],false,4zvwq2,,0,,false,1473085637,false,default,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4zvwq2/going_from_1_to_2_hidden_layers_in_neural_net/,t3_4zvwq2,,false,Discusssion,
1471535258,MachineLearning,dynerthebard,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yczg7/numerical_precision_with_respect_to_ml/,6,0,0,0,Numerical Precision with respect to ML?,"Hey there, 

Was wondering if anyone could point me to some papers regarding the affects of precision in ML research. Specifically interested in the accuracy needed (8, 16, 32 bit / float vs signed, etc) when doing large SVMs or neural nets. 

Thanks!",false,4yczg7,,0,,false,1473057658,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yczg7/numerical_precision_with_respect_to_ml/,t3_4yczg7,,false,,
1471971127,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z7gk9/can_one_implement_svd_in_theano_using_gpus/,8,3,3,0,Can one implement SVD in theano using GPUs?,"In theano, the functions `svd` and `_svd` both run on CPUs. 

https://github.com/Theano/theano/blob/cacac4554206c61ebc1f0ca7de86e5ed2a1e240a/theano/tensor/nlinalg.py#L603-L641


Why exactly is this? Is there a ""GPU"" way to run SVD on extremely large matrices? (As a side question, is there a size limit?)",false,4z7gk9,,0,,false,1473073134,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z7gk9/can_one_implement_svd_in_theano_using_gpus/,t3_4z7gk9,,false,,
1471122992,MachineLearning,79c4a06fbba64629867a,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xl1tk/what_is_the_benefit_of_crossentropy_loss_against/,18,10,10,0,What is the benefit of cross-entropy loss against a simple euclidean/least-squares loss?,"Both types of loss functions should essentially generate a global minimum in the same place. So if I had some magical algorithm that could magically find the global minimum perfectly, it wouldn't matter which loss function I use.

But for practical purposes, like training neural networks, people always seem to use cross entropy loss. What's the benefit of this? What does this look like mathematically that makes it appealing?

Apologies if this is an easily answered question, feel free to send me to the right resource!",false,4xl1tk,,0,,false,1473043490,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xl1tk/what_is_the_benefit_of_crossentropy_loss_against/,t3_4xl1tk,,false,,
1470608542,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wn1qt/what_unsupervised_learning_algorithms_exist_to/,32,32,32,0,What unsupervised learning algorithms exist to infer the number of clusters in data?,"Let's say you have a density plot of data in 2D (or even 1D). Surely there are algorithms which infer the number of clusters which exist in the data without users having to explicitly set this number (e.g. like with traditional k-means). 

What exists for this problem? ",false,4wn1qt,,0,,false,1473026136,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wn1qt/what_unsupervised_learning_algorithms_exist_to/,t3_4wn1qt,,false,,
1470348639,MachineLearning,NotAHomeworkQuestion,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w74ms/softmax_regression_vs_multinomial_logistic/,8,8,8,0,Softmax regression vs multinomial logistic regression: is there a difference?,"I hear these two names used interchangeably but there seems to be an important difference: 

Softmax regression generates a score for each of the K classes by taking the inner product of the input features with the class-specific parameters before normalizing to get probabilities for each class.

Multinomial logistic regression does something similar but only has parameters for the first K-1 classes, taking advantage of the fact that the resulting probabilities must sum to 1.

Thus, why would anyone ever use softmax when they seem to be getting at the same thing but multinomial logistic does it with fewer parameters, thus (I'm assuming) reducing the variance of our estimates?",false,4w74ms,,0,,false,1473017966,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w74ms/softmax_regression_vs_multinomial_logistic/,t3_4w74ms,,false,,
1471333824,MachineLearning,john_philip,analyticsvidhya.com,https://www.analyticsvidhya.com/blog/2016/08/practicing-machine-learning-techniques-in-r-with-mlr-package/,0,16,16,0,Practicing Machine learning techniques in R with MLR Package,,false,4xyddg,,0,,false,1473050267,false,http://b.thumbs.redditmedia.com/x9641jzc7XsuutVmqnUHWknz1D_Qjftr3L1sC52BdHU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xyddg/practicing_machine_learning_techniques_in_r_with/,t3_4xyddg,,false,,
1472387807,MachineLearning,t_broad,longnow.org,http://longnow.org/essays/richard-feynman-connection-machine/,9,191,191,0,Richard Feynman and The Connection Machine,,false,4zz7n9,,0,,false,1473087332,false,http://b.thumbs.redditmedia.com/-RaYuUimzCiNFPWpkWZIyoOyWr0z26KPtTa1S6JLjik.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zz7n9/richard_feynman_and_the_connection_machine/,t3_4zz7n9,,false,,
1471433160,MachineLearning,mixmachinery,youtube.com,https://www.youtube.com/attribution_link?a=Jv8Xbad3KLs&amp;u=%2Fwatch%3Fv%3DCcuIuIQT4KU%26feature%3Dshare,1,1,1,0,Does JCT Machinery has rtv silicone rubber production mixer in stock?,,false,4y50w1,,0,,false,1473053631,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y50w1/does_jct_machinery_has_rtv_silicone_rubber/,t3_4y50w1,,false,,
1470829818,MachineLearning,nb410,getrevue.co,https://www.getrevue.co/profile/nathanbenaich/issues/news-in-artificial-intelligence-and-machine-learning-july-august-24589,0,2,2,0,News in artificial intelligence and machine learning [July/August],,false,4x1qox,,0,,false,1473033611,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x1qox/news_in_artificial_intelligence_and_machine/,t3_4x1qox,,false,,
1470057197,MachineLearning,halfeatenscone,colinmorris.github.io,http://colinmorris.github.io/blog/dreaming-rbms,2,61,61,0,Dreaming of names with restricted Boltzmann machines,,false,4vmf88,,0,,false,1473007151,false,http://b.thumbs.redditmedia.com/rbuCHXwk47zxmRsIbIQ316P0CgJbq_X4ZEe3LFwp3MA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vmf88/dreaming_of_names_with_restricted_boltzmann/,t3_4vmf88,,false,,
1472030747,MachineLearning,italartworld001,denonpu.blogspot.in,http://denonpu.blogspot.in/2016/08/applications-of-pu-machine-that.html,0,1,1,0,An efficient PU machine such as CPDS 1000 (Constant Pressure Dispensing System) has a container yield of 750 board feet and uses 120â€™of hose for better reach and excellent supply of foam from the air compressor. These PU machines play a key role in supporting the country’s economic sum.,,false,4zbpe9,,0,,false,1473075301,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zbpe9/an_efficient_pu_machine_such_as_cpds_1000/,t3_4zbpe9,,false,,
1470325940,MachineLearning,MaxTalanov,bayareadlschool.org,http://www.bayareadlschool.org/,0,4,4,0,Deep learning school Stanford,,false,4w56j2,,0,,false,1473016964,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w56j2/deep_learning_school_stanford/,t3_4w56j2,,false,,
1471623936,MachineLearning,jackerfrinandis,lubricationpump.bcz.com,http://lubricationpump.bcz.com/2016/08/18/3-key-attributes-to-find-an-ideal-distributor-of-lubrication-products/,0,1,1,0,3 Key Attributes To Find An Ideal Distributor Of Lubrication Products,,false,4yjux3,,0,,false,1473061134,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yjux3/3_key_attributes_to_find_an_ideal_distributor_of/,t3_4yjux3,,false,,
1472186232,MachineLearning,[deleted],research.facebook.com,https://research.facebook.com/blog/learning-to-segment/,0,1,1,0,"Facebook Research released the code of DeepMask, SharpMask and MultiPath Network training",[deleted],false,4zmuto,,0,,false,1473080990,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zmuto/facebook_research_released_the_code_of_deepmask/,t3_4zmuto,,false,,
1470753565,MachineLearning,[deleted],nextplatform.com,http://www.nextplatform.com/2016/08/08/deep-learning-chip-upstart-set-take-gpus-task/,41,14,14,0,Deep Learning Chip Upstart Takes GPUs to Task,[deleted],false,4wwhui,,0,,false,1473030953,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wwhui/deep_learning_chip_upstart_takes_gpus_to_task/,t3_4wwhui,,false,,
1472169402,MachineLearning,[deleted],i.redd.it,https://i.redd.it/bf3nlxos9mhx.png,0,1,1,0,New Computer Vision dataset - SpaceNet,[deleted],false,4zloee,,0,,false,1473080385,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zloee/new_computer_vision_dataset_spacenet/,t3_4zloee,,false,,
1471528888,MachineLearning,oatmealfurnace,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ycf0i/understanding_coding_practices_in_machine_learning/,0,1,1,0,Understanding coding practices in Machine Learning,[removed],false,4ycf0i,,0,,false,1473057373,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ycf0i/understanding_coding_practices_in_machine_learning/,t3_4ycf0i,,false,,
1471547296,MachineLearning,Deepspeech,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ye4e7/in_quest_of_cofounders_for_machine_learning/,0,1,1,0,In quest of co-founders for Machine learning startup,[removed],false,4ye4e7,,0,,false,1473058236,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ye4e7/in_quest_of_cofounders_for_machine_learning/,t3_4ye4e7,,false,,
1470039771,MachineLearning,xristos_forokolomvos,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vlhic/stateofart_in_probabilistic_classification/,1,2,2,0,State-of-art in probabilistic classification?,"I've been studying Machine Learning for quite a while but up to now I never had to dive into the probabilistic part. Can someone point me to reading material on that?

I'm working on a project and am expected to improve upon a Naive Bayes classifier, any help would be much appreciated!",false,4vlhic,,0,,false,1473006661,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vlhic/stateofart_in_probabilistic_classification/,t3_4vlhic,,false,,
1470639072,MachineLearning,metacurse,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wovnn/what_exactly_is_the_history_of_the_field_of/,9,0,0,0,What exactly is the history of the field of Artificial Intelligence?,"The curiosity in this is provoked by these observations:

* Hinton during his acceptance speech commented on old AI that was logic and new AI that is basically Neural Networks

* Yann LeCun saying that probabilistic AI people endured criticism similar to NN researchers but ultimately won

So I'm interested to know what exactly this old AI approach was, what are it's disadvantages and how have we  have overcome it.

If you know of any fragment of information that would give a clearer picture regarding this, please share.

PS: not trusting Wikipedia with this after I saw the horrible state of some DL related pages. ",false,4wovnn,,0,,false,1473027073,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wovnn/what_exactly_is_the_history_of_the_field_of/,t3_4wovnn,,false,,
1471520095,MachineLearning,bbsome,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ybroo/survey_which_ml_libraries_you_use/,22,5,5,0,Survey: Which ML Libraries you use?,"So I'm buliding a small list of many different ML libraries, but don't want to miss out too many (sure I will do some). So far I have frameworks[UPDATED]:
Theano,
Tensorflow,
Torch7,
MXNet,
Caffe,
CNTK,
Deeplearning4j,
Nervana neon,
Sickit-learn,
Apache Spark (mllib),
MlPack,
Vowpal Wabbit,
H20,
XGBoost,
dllib,
sofia-ml,
JSAT,
Deep ExcelNet

And libraries based on some of them[UPDATED]:
Lasagne,
Keras,
Blocks,
Tflearn,
Nvidia DIGITS,
Elephas,
Sparkling Water,



Anything else which you are using and think needs attention please feel free post.",false,4ybroo,,0,,false,1473057048,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ybroo/survey_which_ml_libraries_you_use/,t3_4ybroo,,false,,
1471438095,MachineLearning,anonymousTestPoster,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y5dei/generative_models_with_constraints/,6,7,7,0,Generative models with constraints.,"With generative models we can generate new ""fantasy"" input data as we please. Is it possible at all to have a finer tuned control over what range of data can be generated? 

For example imagine a learning scenario where I teach a model to associate a shape to a number. So the recognition model goes shape -&gt; a number, e.g. square -&gt; 4. But if I want to go the other way: number -&gt; shape, how can I guarantee that my model won't generate some random shape with crazy 'wobbly' edges, or a shape without closed edges. Ie I don't want nonsensical surfaces / shapes. Is there some way to prune / train a generative model to enforce some sort of guarantee? ",false,4y5dei,,0,,false,1473053806,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y5dei/generative_models_with_constraints/,t3_4y5dei,,false,,
1471700977,MachineLearning,Qwexi,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yp47e/question_about_which_math_electives_i_should_take/,21,18,18,0,Question about which math electives I should take.,"Hey I'm an undergrad cs major and I'm getting a minor in mathematics. The areas I'm most interested in are machine learning / deep learning / ai and robotics. I have a year left and I've planned out for the most parts which classes I'm taking, but I recently found out the classes needed to take a real analysis course and an algorithms course aren't anywhere as high as I thought they'd be. Not only are these classes I'd personally like to take but I thought they could look pretty well on a transcript to grad school especially since in my school those are both graduate level courses. The problem is that I don't have enough room in my schedule to just add them I'd have to replace. I was going to take graph theory and differential equations, but I wouldn't mind switching out these two for the real analysis course and the algorithms class. Could you guys point me in the right direction? Which would generally look better on a transcript? Or would it not really make a difference?",false,4yp47e,,0,,false,1473063817,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yp47e/question_about_which_math_electives_i_should_take/,t3_4yp47e,,false,,
1470694867,MachineLearning,Tokukawa,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wsv4e/i_am_struggling_to_understand_the_difference/,4,4,4,0,I am struggling to understand the difference between max pooling and RoI max pooling.,"I am reading the fast-RCNN paper of Girshick. Although I can grasp the big picture, I can't understand the details of the implementation. Who can help me?",false,4wsv4e,,0,,false,1473029103,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wsv4e/i_am_struggling_to_understand_the_difference/,t3_4wsv4e,,false,,
1472081317,MachineLearning,ih4cku,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zfhyr/how_to_calculate_the_total_number_of_paths_for_a/,3,0,0,0,How to calculate the total number of paths for a labelling in CTC?,"In `Chapter 7.3 Forward-backward algorithm` of Alex Graves's [thesis](https://www.cs.toronto.edu/~graves/preprint.pdf), he gives a formula for calculating the total number of paths for a labelling,

&gt; ... more precisely, for a length T input sequence and a length U labelling, 

&gt; there are 2 ^ {T −U^ 2+U(T −3)} * 3^ {(U−1)(T −U)−2} paths.


![](http://i.imgur.com/mrug6jm.png)  

Take a simple example with input length T=4, labelling length U=3. 

Say the labelling for the input sequence is `abc`, we can list all the possible paths (`-` represents a `blank`):

&gt; -abc

&gt; a-bc

&gt; aabc

&gt; ab-c

&gt; abbc

&gt; abc-

&gt; abcc


For this example, there are **7** paths in total. 

However, using Alex's formula with T=4 and U=3, the result is 2^-2 * 3^0 = **1/4**, not even an integer.

Where am I wrong?",false,4zfhyr,,0,,false,1473077227,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zfhyr/how_to_calculate_the_total_number_of_paths_for_a/,t3_4zfhyr,,false,,
1472170317,MachineLearning,adamwkraft,businesswire.com,http://www.businesswire.com/news/home/20160825005368/en/DigitalGlobe-CosmiQ-Works-NVIDIA-Amazon-Web-Services,2,10,10,0,New Computer Vision dataset - SpaceNet,,false,4zlqsx,,0,,false,1473080422,false,http://b.thumbs.redditmedia.com/bibM0u1TdSuUZz-pl9UJwPvCxkM0oOOJYjRZxZKO-pA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zlqsx/new_computer_vision_dataset_spacenet/,t3_4zlqsx,,false,,
1470296472,MachineLearning,koormoosh,stats.stackexchange.com,http://stats.stackexchange.com/questions/228205/expectation-of-an-unbiased-estimator-under-variational-inference-setting,0,2,2,0,Expectation of an unbiased estimator (under variational inference setting),,false,4w3eu0,,0,,false,1473016057,false,http://b.thumbs.redditmedia.com/sKCks5VJPpRM5wPziuEeT2KEv5rM2XuQuq0e9pJQuRk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w3eu0/expectation_of_an_unbiased_estimator_under/,t3_4w3eu0,,false,,
1470916085,MachineLearning,ernesttg,deepomatic.com,http://www.deepomatic.com/#/blog/text-regression-for-click-through-rate-prediction-,4,0,0,0,How often will people click on your link? You won't believe how this deep-learning scientist solved this problem!!! (marketers hate him...),,false,4x7l36,,0,,false,1473036596,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x7l36/how_often_will_people_click_on_your_link_you_wont/,t3_4x7l36,,false,,
1472543036,MachineLearning,mixmachinery,mixmachinery.com,http://www.mixmachinery.com/news/JCT-hot-melt-glue-mixing-machine.html,1,1,1,0,How to choose the suitable hot melt glue machine?,,false,509ywt,,0,,false,1473092995,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/509ywt/how_to_choose_the_suitable_hot_melt_glue_machine/,t3_509ywt,,false,,
1470552271,MachineLearning,Dawny33,datascience.stackexchange.com,http://datascience.stackexchange.com/q/13266/11097,6,3,3,0,Help me in understanding the math behind SVM,,false,4wjttn,,0,,false,1473024489,false,http://a.thumbs.redditmedia.com/y4UNiP8rZ5wylSlK8esHvSGI1n4n3sRsnyEFUAUzBy0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wjttn/help_me_in_understanding_the_math_behind_svm/,t3_4wjttn,,false,,
1471862278,MachineLearning,virene,responsiblemachines.wordpress.com,https://responsiblemachines.wordpress.com/2016/06/04/creating-a-base-architecture-for-sensor-data/,0,3,3,0,"To be able to render perfect knowledge, where perfect knowledge can be defined as availability of all data in the environment in order to generate accurate intelligence by incorporating every parameter (both visible and hidden) in the environment.",,false,4yzxaz,,0,,false,1473069292,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yzxaz/to_be_able_to_render_perfect_knowledge_where/,t3_4yzxaz,,false,,
1472032575,MachineLearning,italartworld001,slideserve.com,http://www.slideserve.com/KateLeo/applications-of-pu-machine-that-contribute-to-country-s-economy,0,1,1,0,An efficient PU machine such as CPDS 1000 (Constant Pressure Dispensing System) has a container yield of 750 board feet and uses 120â€™of hose for better reach and excellent supply of foam from the air compressor. These PU machines play a key role in supporting the country’s economic sum.,,false,4zbspc,,0,,false,1473075347,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zbspc/an_efficient_pu_machine_such_as_cpds_1000/,t3_4zbspc,,false,,
1471004541,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1608.03287,0,25,25,0,[1608.03287] Deep vs. shallow networks : An approximation theory perspective,,false,4xd8la,,0,,false,1473039509,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4xd8la/160803287_deep_vs_shallow_networks_an/,t3_4xd8la,,false,Research,
1470231459,MachineLearning,cbil360,i.redd.it,https://i.redd.it/1sym3yoc76dx.jpg,0,0,0,0,Machine Learning: A New Paradigm in Data Analytics,,false,4vyilg,,0,,false,1473013478,false,http://a.thumbs.redditmedia.com/mgI0HU9IwAIVLNxD57gqRI8Ehsy9LILmUk4Cta0BOd4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vyilg/machine_learning_a_new_paradigm_in_data_analytics/,t3_4vyilg,,false,,
1470849489,MachineLearning,gwulfs,ttvand.github.io,https://ttvand.github.io/Winning-approach-of-the-Facebook-V-Kaggle-competition/,0,13,13,0,Facebook V: Predicting Check Ins,,false,4x38dq,,0,,false,1473034377,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x38dq/facebook_v_predicting_check_ins/,t3_4x38dq,,false,,
1470938449,MachineLearning,J4ck-,tech.iheart.com,https://tech.iheart.com/mapping-the-world-of-music-using-machine-learning-part-1-9a57fa67e366#.y1sau57w3,0,27,27,0,Mapping the World of Music Using Machine Learning: Part 1,,false,4x9czl,,0,,false,1473037515,false,http://b.thumbs.redditmedia.com/2JngvxkT4oo29gPzgWSepNg8AXnGQpFlZyGNfnIaF6c.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x9czl/mapping_the_world_of_music_using_machine_learning/,t3_4x9czl,,false,,
1470780601,MachineLearning,edkeens,github.com,https://github.com/janivanecky/Artistic-Style,0,13,13,0,Simple Tensorflow implementation of artistic style transfer,,false,4wyu1r,,0,,false,1473032143,false,http://b.thumbs.redditmedia.com/RI79TQO0DqQzvjjZ_iZS1u58bHy5p_QyF_7ydq-9-3c.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wyu1r/simple_tensorflow_implementation_of_artistic/,t3_4wyu1r,,false,,
1470173974,MachineLearning,amplifier_khan,insightdatascience.com,http://insightdatascience.com/blog/thesauropodus.html,0,2,2,0,Thesauropod.us: Building a Podcast Recommendation Algorithm,,false,4vv0dc,,0,,false,1473011646,false,http://b.thumbs.redditmedia.com/T6scMKUC1pSTUwOOKveqSBleaT2vMAP3XWQcosaQCRw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vv0dc/thesauropodus_building_a_podcast_recommendation/,t3_4vv0dc,,false,,
1470675410,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wr58u/activation_functions/,13,10,10,0,Activation functions,[deleted],false,4wr58u,,0,,false,1473028226,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wr58u/activation_functions/,t3_4wr58u,,false,,
1471899749,MachineLearning,[deleted],nextplatform.com,http://www.nextplatform.com/2016/08/22/intel-tweaking-xeon-phi-deep-learning/,1,0,0,0,Why Intel is Tweaking Xeon Phi Supercomputing Chip for Deep Learning,[deleted],false,4z2rph,,0,,false,1473070754,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z2rph/why_intel_is_tweaking_xeon_phi_supercomputing/,t3_4z2rph,,false,,
1470166623,MachineLearning,[deleted],github.com,https://github.com/KnHuq/Dynamic-Tensorflow-Tutorial/blob/master/Tensorboard/Tensorboard.ipynb,0,0,0,0,Simple Example of using tensorboard,[deleted],false,4vud01,,0,,false,1473011313,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vud01/simple_example_of_using_tensorboard/,t3_4vud01,,false,,
1470642403,MachineLearning,technewtech,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wp13l/how_microsoft_is_making_big_impact_with_machine/,9,3,3,0,How Microsoft Is Making Big Impact with Machine Learning?,[removed],false,4wp13l,,0,,false,1473027150,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wp13l/how_microsoft_is_making_big_impact_with_machine/,t3_4wp13l,,false,,
1470812498,MachineLearning,damnedSpirit,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x0wwt/anyone_interested_in_doing_beginner_level_machine/,0,1,1,0,Anyone interested in doing beginner level Machine Learning / Image processing research project for learning?,[removed],false,4x0wwt,,0,,false,1473033194,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x0wwt/anyone_interested_in_doing_beginner_level_machine/,t3_4x0wwt,,false,,
1472620340,MachineLearning,curryeater259,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50fkjn/andrew_ng_machine_learning_course_help/,4,1,1,0,Andrew Ng Machine Learning course help?,"Hey Guys,

what's up. I'm currently working through Andrew Ng's Machine Learning course on Coursera and I'm on week 4 (starting Neural Networks). This has been happening on all the programming assignments, but I'll watch all the videos, understand everything, take the quiz and pass, but then on the programming assignments I'll get completely stuck and have no idea where to start. I really don't want to cheat and google the answers, so I was wondering if you knew any links to any good neural network implementations in MATLAB. I searched online, but couldn't really find any ground-up implementations, most of the code I saw just used machine learning libraries for all the main stuff. Thanks guys.",false,50fkjn,,0,,false,1473095912,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50fkjn/andrew_ng_machine_learning_course_help/,t3_50fkjn,,false,,
1470109112,MachineLearning,EsportsinaNutshell,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vqjn0/trying_to_incorporate_machine_learning_into/,8,0,0,0,Trying to incorporate machine learning into assessing credit risk.,"Hello,
My friend is trying to incorporate machine learning into his company's proprietary credit scoring system to assess credit risk. Do you recommend going to a site like Kaggle and making it a contest or are there consulting machine learning firms that he can go to, to figure out a solution?",false,4vqjn0,,0,,false,1473009313,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vqjn0/trying_to_incorporate_machine_learning_into/,t3_4vqjn0,,false,,
1470202298,MachineLearning,rulerofthehell,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vwynk/what_are_some_good_pretrained_rnn_models_for/,8,17,17,0,What are some good pre-trained RNN models for speech recognition?,"I'm asking for some good (like VGG16 for image processing equivalent for speech and audio) models, if those models aren't available, are there any good research paper I should read to know more about it?",false,4vwynk,,0,,false,1473012651,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vwynk/what_are_some_good_pretrained_rnn_models_for/,t3_4vwynk,,false,,
1470065620,MachineLearning,gr8ape,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vn2hf/model_validation_selection_with_neural_net_is/,0,0,0,0,Model validation &amp; selection with neural net is failing to choose close to the best model.,"Hi all, I am facing a frustrating issue where my model validation &amp; selection seems to be very off-track. 

I have some sequential sales data (monthly aggregation) for a set of products and stores, for which I have features for both the products and stores, and some features representing the time.

I tried several validation schemes, but the one that worked generally well with random forests was sequential validation.
With this same setup, using random forests, validation error is quite correlated with test error (for most of the error metrics), and I can select a model based on validation that performs reasonably well on the test set.

With neural nets, validation error seems almost uncorrelated (spearman correlation of error metrics on validation set vs test set ~=0.1-0.2 vs 0.7-0.8 with random forests) with test error, and although I can select models that seem to have comparable performance to the random forests, I can never select the best ones, which pretty consistently outperform the random forests.

I am not sure how to fix or work around this issue. Could be due to the fact that the neural output is more variable than random forests? Seems like a bug? I guess part of this is due the non-stationary data distribution... Any insights appreciated, I can give more details on the architecture if needed.

edit: I am sure all the downvoters have the solution or some genius insight",false,4vn2hf,,0,,false,1473007488,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vn2hf/model_validation_selection_with_neural_net_is/,t3_4vn2hf,,false,,
1470231360,MachineLearning,SimulatedAnnealing,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vyich/should_l2_regularization_be_corrected_for_scale/,3,5,5,0,Should L2 regularization be corrected for scale?,"I am training a deep neural network using cross entropy loss and L2 regularization, so the final cost function looks something like this:

E=−1/N ∑i cross_entropy(xi,yi) + λ ∑j ∑k ∑l (w_j,k,l)**2

where the first term is the cross entropy over classes (averaged over the size of the training set) and the second term is the sum of squared weights involved in the network (w_j,k,l is the weight from k-th unit in j-th layer to l-th unit in (j+1)-th layer), and λ is a regularization strength parameter.

My question is: won't the number of layers and units affect the scale of the regularization term ? Therefore, wouldn't it make more sense to normalize the second term by the number of weights (i.e., replacing λ / (Nlayers * Nunits * Nunits) for λ).

Unfortunately, I've not found any reference about this. I've just found in Bengio's paper [1] (weight decay subsection) that they recommend to scale according to the number of mini-batches in each epoch (which I do not really see the reason why).

[1] Practical Recommendations for Gradient-Based Training of Deep Architectures",false,4vyich,,0,,false,1473013474,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vyich/should_l2_regularization_be_corrected_for_scale/,t3_4vyich,,false,,
1471538045,MachineLearning,Slyferr,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yd8tk/software_development_or_machine_learning/,4,0,0,0,Software Development or Machine Learning?,"Ok, so I know it's not fair to ask for opinions in the Machine Learning subreddit... but I hope that some of you were in my same position.
I'm studying Computer Engineering, and for the last 2 years I've been developing Android Apps just for fun. I like it. I've done a couple of projects with my friends that made me feel like I've accomplished something on my own. However I'm also attracted to new technologies, so in the last months I've attended a Machine Learning MOOCs (the one from Standford on Coursera). I'm relatively comfortable with the level of mathematics required, and I'm eager to learn more. 
Given the state of mobile app development (I feel like the market will be saturated in the next decade), would it be wise to follow a career in Machine Learning?

Sorry for my english. It's not my native language.",false,4yd8tk,,0,,false,1473057789,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yd8tk/software_development_or_machine_learning/,t3_4yd8tk,,false,,
1471376043,MachineLearning,SIGKDDawards2016,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y1dsw/in_the_sigkdd_2016_the_awards_seem_to_be_a_fix/,0,1,1,0,In the SIGKDD 2016 the awards seem to be a fix...,"In the SIGKDD 2016 the awards seem to be a fix...

The Test of Time Award went to Christos Faloutsos, the committee was run by Wei Wang.

The Service Award went to Wei Wang, the committee included Christos Faloutsos.

Does anyone think this is a bit ‘iffy’?
---

Faloutsos also won the best paper award. I cant find a committee list online yet, but if you had to guess? 


X. Wu
",false,4y1dsw,,0,,false,1473051793,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y1dsw/in_the_sigkdd_2016_the_awards_seem_to_be_a_fix/,t3_4y1dsw,,false,,
1471309389,MachineLearning,MaxTalanov,twitter.com,https://twitter.com/fchollet/status/765212287531495424,18,153,153,0,Latest popularity ranking of Deep Learning frameworks,,false,4xww1v,,0,,false,1473049515,false,http://b.thumbs.redditmedia.com/dIyRM9ilq-WKuWSXcAGaBZG_5521HcGinas7bbg9rWc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xww1v/latest_popularity_ranking_of_deep_learning/,t3_4xww1v,,false,,
1471573451,MachineLearning,harrism,devblogs.nvidia.com,https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/,4,22,22,0,End-to-End Deep Learning for Self-Driving Cars,,false,4ygcxk,,0,,false,1473059360,false,http://b.thumbs.redditmedia.com/vWBAkglCW4gP--5uAtE9WpWr5N2xS9K3VzheVXuNftQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ygcxk/endtoend_deep_learning_for_selfdriving_cars/,t3_4ygcxk,,false,,
1470203541,MachineLearning,sphinx-solution,sphinx-solution.com,http://www.sphinx-solution.com/blog/machine-learning-a-new-paradigm-in-data-analytics/,1,1,1,0,Machine Learning: A New Paradigm in Data Analytics,,false,4vx0wi,,0,,false,1473012683,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vx0wi/machine_learning_a_new_paradigm_in_data_analytics/,t3_4vx0wi,,false,,
1471862091,MachineLearning,reworksophie,re-work.co,https://re-work.co/blog/summer-special-offer-discount-on-all-tickets-to-all-summits-2016,0,1,1,0,"20% off all tickets to RE•WORK events inc. Machine Intelligence, Deep Learning, Autonomous Vehicles, locations inc NY, SF, London, Amsterdam, Singapore &amp; more!",,false,4yzwz7,,0,,false,1473069288,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yzwz7/20_off_all_tickets_to_rework_events_inc_machine/,t3_4yzwz7,,false,,
1471601453,MachineLearning,sphinx-solution,sphinx-solution.com,http://www.sphinx-solution.com/blog/how-retail-industry-can-utilize-machine-learning#15ZQW,0,1,1,0,How Retail Industry Can Utilize Machine Learning?,,false,4yi3pb,,0,,false,1473060240,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yi3pb/how_retail_industry_can_utilize_machine_learning/,t3_4yi3pb,,false,,
1471347856,MachineLearning,john_philip,aryehoffman.com,http://aryehoffman.com/reference/structured-objects-approach/,0,0,0,0,Structured Approach,,false,4xz289,,0,,false,1473050616,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xz289/structured_approach/,t3_4xz289,,false,,
1470834829,MachineLearning,reworksophie,re-work.co,https://re-work.co/blog/where-are-they-now-deep-learning-startups-metamind-orbeus-emotient-acquisitions,0,1,1,0,A.I. Acquisitions: The Evolving Deep Learning Landscape,,false,4x21rx,,0,,false,1473033766,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x21rx/ai_acquisitions_the_evolving_deep_learning/,t3_4x21rx,,false,,
1472413281,MachineLearning,p8donald,ieva.rocks,http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/,0,0,0,0,XGBoost tutorial with the Iris Dataset,,false,5010vo,,0,,false,1473088307,false,http://b.thumbs.redditmedia.com/_7DTgOxJRJ1jCR6SDMWpGf4p80bLMAyzBF8vOKQsQDc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/5010vo/xgboost_tutorial_with_the_iris_dataset/,t3_5010vo,,false,,
1471714970,MachineLearning,abdsc,datasciencecentral.com,http://www.datasciencecentral.com/profiles/blogs/6448529:BlogPost:459267,0,1,1,0,AI vs Deep Learning vs Machine Learning,,false,4yq6rj,,0,,false,1473064371,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yq6rj/ai_vs_deep_learning_vs_machine_learning/,t3_4yq6rj,,false,,
1470440161,MachineLearning,sbc1906,geekwire.com,http://www.geekwire.com/2016/exclusive-apple-acquires-turi-major-exit-seattle-based-machine-learning-ai-startup/,19,53,53,0,"Apple acquires Machine Learning startup Turi (formerly Dato, GraphLab)",,false,4wdbwi,,0,,false,1473021143,false,http://a.thumbs.redditmedia.com/obhqdkWIjfR_sW4CINl7x4AlNojqFfkZGrRDxts5du0.jpg,t5_2r3gv,false,two,,false,false,,/r/MachineLearning/comments/4wdbwi/apple_acquires_machine_learning_startup_turi/,t3_4wdbwi,,false,News,
1470487189,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wfq6l/do_classifiers_classify_using_the_decision/,2,0,0,0,Do classifiers classify using the decision surface or do they actually calculate the class for each item?,[deleted],false,4wfq6l,,0,,false,1473022376,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wfq6l/do_classifiers_classify_using_the_decision/,t3_4wfq6l,,false,,
1470070812,MachineLearning,HeshamElhalawani,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vnihc/computational_precision_medicine_challenge/,0,1,1,0,Computational Precision Medicine Challenge- Develop your own algorithm to the best radiomics signature for oropharynx cancers (OPC),[removed],false,4vnihc,,0,,false,1473007720,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vnihc/computational_precision_medicine_challenge/,t3_4vnihc,,false,,
1470679337,MachineLearning,Helix_Hoenikker,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wrimi/ml_in_wolfram_language/,0,1,1,0,ML in Wolfram Language,[removed],false,4wrimi,,0,,false,1473028414,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wrimi/ml_in_wolfram_language/,t3_4wrimi,,false,,
1471937545,MachineLearning,Throwaway34448,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5b8r/unsupervised_learning_doesnt_make_sense/,15,1,1,0,Unsupervised Learning doesn't make sense?,"I don't get unsupervised learning. In the sense that I don't see why it is more desirable than reinforcement learning.

An unsupervised learning system will learn to represent the data in some way that maximises an objective function that is only dependent on the data. Big woop. Why is that important - surely the only important aspect is how that representation can be applied.

What we really need is reinforcement learning which is much more powerful - that might include systems like autoencoders - but the key part is the reinforcement learning. 

Unsupervised learning just seems to me like a useful piece of a larger puzzle, rather than some holy grail.

Am I missunderstanding something?",false,4z5b8r,,0,,false,1473072033,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z5b8r/unsupervised_learning_doesnt_make_sense/,t3_4z5b8r,,false,,
1472305289,MachineLearning,Schlagv,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zu9bk/cifar10_how_to_reach_95_i_can_only_get_85/,16,0,0,0,Cifar10: how to reach 95% ? I can only get 85%,"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html

When I try to implement the papers from this page, I get results between 80% and 86%. All-convolutional-net is a very clear and simple paper, but when I do exactly the same, I get 86%, not 95%. And I even think that this is without data augmentation and multiple models.

I tried data augmentation with random crop + flip (no contrast), different initializations, different gradient descent methods and many other things. And I cannot even reach 90%.

I tried to implement other papers, and same thing, I am usually stuck between 75-85%.

What am I doing wrong ?",false,4zu9bk,,0,,false,1473084783,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zu9bk/cifar10_how_to_reach_95_i_can_only_get_85/,t3_4zu9bk,,false,,
1471945355,MachineLearning,FCM_BR,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5o3g/machine_learning_for_industrial_processes/,5,7,7,0,Machine Learning for Industrial Processes,"I am an engineer and enterprenuer specialized in information systems for manufacturing companies. Recently I have did an on-line course where I could learn the basics of Machine Learning using Microsoft Azure ML platform and I believe it has a big potential to be applied on predictive analysis for manufacturing companies. Today most of the manufacturing companies have systems that generate a huge amount of data. Unstructured data such as time series events and measurements from automation and control system and Structured data from relational databases. I planning to provide a new type of services for my customers. The basis of these services would consist of consulting services, to help them to identify opportunities of application of ML to improve their businesses combined with ML algorithms running in the Cloud (which they will be able to use in a As-A-Service model). My idea is the customer to provide the data to have the ML models trained (by providing XML files) and after the model has been trained they will be able to use the trained models in the cloud to make predictions important areas of the industrial process. 

Do you believe that this is a doable application. Is this approach already being applied in other areas? Am I having a too simplified view of ML?
",false,4z5o3g,,0,,false,1473072228,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z5o3g/machine_learning_for_industrial_processes/,t3_4z5o3g,,false,,
1470763162,MachineLearning,isaacgerg,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wxbsn/anyone_willing_to_share_ubiome_data/,3,0,0,0,Anyone willing to share uBiome data?,"I have ubiome data on myself (gut) over time.  I would like to start building a dataset by which others submit their biomes and I match them to other users with similar biomes.

There is limited on ubiome's github.  I would be willing to share any methods / data I receive.",false,4wxbsn,,0,,false,1473031376,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wxbsn/anyone_willing_to_share_ubiome_data/,t3_4wxbsn,,false,,
1472497574,MachineLearning,Sig_Luna,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/506p37/face_recognition_in_videos/,3,2,2,0,Face Recognition in videos,Hey guys. I'm currently trying to make a face recognition in videos with TensorFlow and I'm a little bit stuck. I googled for some inspiration but did not find a single thing on the subject. Does anyone of you know a good ressource of Face Recognition in videos?,false,506p37,,0,,false,1473091326,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/506p37/face_recognition_in_videos/,t3_506p37,,false,,
1471451210,MachineLearning,Chobeat,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y6k35/what_about_a_bad_machine_learning_in_the_guise_of/,36,71,71,0,"What about a ""Bad Machine Learning"" in the guise of /r/badhistory or /r/badphilosphy?","I just got this idea reading a thread on the subreddit that should not be named by any data scientist (/r/Futurology, obviously) and I thought that there should be a place like /r/badhistory or /r/badphilosophy where we could report and ridicule the most imaginative bullshit about ""AI revolution"", singularity, cognitive computing and machine learning concepts in general.
",false,4y6k35,,0,,false,1473054402,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y6k35/what_about_a_bad_machine_learning_in_the_guise_of/,t3_4y6k35,,false,,
1470156543,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vtfn9/against_what_other_models_could_i_benchmark_the/,1,6,6,0,Against what other models could I benchmark the performance of a Markov Random Field?,"I know this is an abstract question. 

However, I am using a MRF with multiple hidden states, and then I am simply searching for the maximum a posteriori (MAP) solution for MRF. 

What other models are similar or useful is this sort of situation? ",false,4vtfn9,,0,,false,1473010813,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vtfn9/against_what_other_models_could_i_benchmark_the/,t3_4vtfn9,,false,,
1471641676,MachineLearning,ill-logical,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yldbh/lua_and_python_bad_for_deployment/,34,0,0,0,Lua and Python bad for deployment?,"http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html

suggests that Lua is never used for deployment, and Python-based solutions are generally rewritten in another language.

I wanted to get others' opinions on this issue. Is this true in your experience?",false,4yldbh,,0,,false,1473061906,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4yldbh/lua_and_python_bad_for_deployment/,t3_4yldbh,,false,Discusssion,
1471649779,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ym1kt/convnets_vs_lstms_for_text_classification/,25,11,11,0,ConvNets vs LSTMs for text classification,"http://arxiv.org/abs/1606.01781 argues heavily for the use of ConvNets for text classification (e.g. sentiment prediction) by presenting resuls using ConvNets on some huge datasets.

However, I haven't seen anyone test the performance of LSTM's on these datasets. Have anyone tested them in depth? What are your thoughts - are LSTM's inferior on these kind of tasks?",false,4ym1kt,,0,,false,1473062249,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ym1kt/convnets_vs_lstms_for_text_classification/,t3_4ym1kt,,false,,
1472448779,MachineLearning,tpois,tpois.com,http://tpois.com/items/search?q=&amp;l=&amp;search=Search,0,1,1,0,Tpois - SELL YOUR SURPLUS INDUSTRIAL GOODS,,false,503idi,,0,,false,1473089613,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/503idi/tpois_sell_your_surplus_industrial_goods/,t3_503idi,,false,,
1470811754,MachineLearning,hkotadia1,hkotadia.com,http://hkotadia.com/archives/5751,0,1,1,0,5 Easy &amp; Quick Reads on Machine Learning,,false,4x0vom,,0,,false,1473033178,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0vom/5_easy_quick_reads_on_machine_learning/,t3_4x0vom,,false,,
1471584705,MachineLearning,downtownslim,arxiv.org,https://arxiv.org/abs/1606.07536,1,7,7,0,[1606.07536] Coupled Generative Adversarial Networks,,false,4yh5xj,,0,,false,1473059765,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4yh5xj/160607536_coupled_generative_adversarial_networks/,t3_4yh5xj,,false,Research,
1471274668,MachineLearning,ArnuP,blog.dominodatalab.com,https://blog.dominodatalab.com/reproducible-research-using-domino/,0,19,19,0,Reproducible research in computational sciences,,false,4xu2ld,,0,,false,1473048079,false,http://b.thumbs.redditmedia.com/BhvdLdadk4IIrX4xel30eCxY_Qt_yGL5uv-qghi_sBQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xu2ld/reproducible_research_in_computational_sciences/,t3_4xu2ld,,false,,
1470672241,MachineLearning,perceptron01,research.googleblog.com,https://research.googleblog.com/2016/08/meet-parseys-cousins-syntax-for-40.html,12,36,36,0,"Meet Parsey's Cousins: Syntax for 40 languages, plus new SyntaxNet capabilities",,false,4wqv6l,,0,,false,1473028084,false,http://a.thumbs.redditmedia.com/hqw2VC6HZJlZ9Fm9ykmJ_wkks2v4IVgMhAzH0Zc9WN8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wqv6l/meet_parseys_cousins_syntax_for_40_languages_plus/,t3_4wqv6l,,false,,
1471881179,MachineLearning,madraf,youtube.com,https://www.youtube.com/watch?v=vItpcksfOcE&amp;feature=youtu.be,0,9,9,0,A bit of fun with YOLO (darknet/pjreddie.com credits).,,false,4z16d2,,0,,false,1473069938,false,http://b.thumbs.redditmedia.com/MRVnpAwfLnJt7ZP15dgDI_I41pb43Vl9Vnbb3k-z6Is.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z16d2/a_bit_of_fun_with_yolo_darknetpjreddiecom_credits/,t3_4z16d2,,false,,
1470763919,MachineLearning,amplifier_khan,machinelearningmastery.com,http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/,0,1,1,0,How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras - Machine Learning Mastery,,false,4wxe6z,,0,,false,1473031412,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wxe6z/how_to_grid_search_hyperparameters_for_deep/,t3_4wxe6z,,false,,
1470335276,MachineLearning,smerity,github.com,https://github.com/krasch/presentations/blob/master/pydata_Berlin_2016.pdf,0,8,8,0,What every data scientist should know about data anonymization,,false,4w5zz8,,0,,false,1473017386,false,http://b.thumbs.redditmedia.com/8jhxbVHEU8sGnBbmGKms33Ex7TxnpmELJviohUMHrZY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w5zz8/what_every_data_scientist_should_know_about_data/,t3_4w5zz8,,false,,
1470238568,MachineLearning,physixer,en.wikipedia.org,https://en.wikipedia.org/wiki/Subitizing,1,3,3,0,TIL the concept of 'subitizing': What is the SOTA? Anyone working on this?,,false,4vz3i3,,0,,false,1473013796,false,http://b.thumbs.redditmedia.com/5Cem73wktbWszLTZT56WlpaNRrrZjzDO6R5CcHxgOwk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vz3i3/til_the_concept_of_subitizing_what_is_the_sota/,t3_4vz3i3,,false,,
1471133760,MachineLearning,[deleted],i.redd.it,https://i.redd.it/3e6mnpn8q8fx.jpg,0,2,2,0,Why you shouldn't ask your mom to go to academic conferences...,[deleted],false,4xls3a,,0,,false,1473043862,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xls3a/why_you_shouldnt_ask_your_mom_to_go_to_academic/,t3_4xls3a,,false,,
1470888757,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x67ym/can_someone_explain_how_trees_are_created_in_rfxgb/,1,0,0,0,Can someone explain how trees are created in RF/XGB?,[deleted],false,4x67ym,,0,,false,1473035906,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x67ym/can_someone_explain_how_trees_are_created_in_rfxgb/,t3_4x67ym,,false,,
1471161502,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xnauo/how_can_a_learning_environmenthigh_school_college/,0,1,1,0,"How can a learning environment(high school, college) benefit from a machine learning project?",[deleted],false,4xnauo,,0,,false,1473044639,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xnauo/how_can_a_learning_environmenthigh_school_college/,t3_4xnauo,,false,,
1472222921,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zoyl4/dimensionality_reduction_methods_is_it_possible/,5,4,4,0,Dimensionality reduction methods: Is it possible to automatically interpret the new reduced dimensions?,[deleted],false,4zoyl4,,0,,false,1473082063,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zoyl4/dimensionality_reduction_methods_is_it_possible/,t3_4zoyl4,,false,,
1470897982,MachineLearning,gempomme,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x6qm2/why_is_there_no_pretrained_model_for_timeseries/,0,1,1,0,Why is there no pre-trained model for timeseries (signals)? (Deep Learning),[removed],false,4x6qm2,,0,,false,1473036168,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x6qm2/why_is_there_no_pretrained_model_for_timeseries/,t3_4x6qm2,,false,,
1470245998,MachineLearning,FWD0341,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vzqvf/help_understanding_architecture_of_autoencoder/,0,1,1,0,Help understanding architecture of autoencoder,[removed],false,4vzqvf,,0,,false,1473014156,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vzqvf/help_understanding_architecture_of_autoencoder/,t3_4vzqvf,,false,,
1471074708,MachineLearning,thundergolfer,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xi6p7/78_of_the_ama_google_brain_team_have_phds_34_have/,25,15,15,0,"78% of the AMA Google Brain team have PHDs. 34% have a Masters, and 95% have a Bachelors.","There was a pile of un-upvoted questions about education in the AMA , and I was curious enough to get the numbers on the team in the AMA. Hopefully this is a satisfying enough answer.

There was one member who had 2 Masters, 1 PHD, and a Bachelor. One other member, Chris Olah, only finished high school. ",false,4xi6p7,,0,,false,1473042036,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4xi6p7/78_of_the_ama_google_brain_team_have_phds_34_have/,t3_4xi6p7,,false,Discusssion,
1471781604,MachineLearning,godspeed_china,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yukno/can_i_average_n_multinomiallogistic_regression/,13,2,2,0,can I average N multinomial-logistic regression model?,"I have a large dataset. I split them to 10 batches and fit a multinomial-logistic regression model for each batch.   
The question is can I simply average the weights (with a multiplier sqrt(N)) to obtain a grand model?  
Since it is a linear model, I guess it shall be ok, right?",false,4yukno,,0,,false,1473066598,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yukno/can_i_average_n_multinomiallogistic_regression/,t3_4yukno,,false,,
1471105105,MachineLearning,Knoxvillefox,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xjqaj/advice_needed_learning_machine_learning_for/,4,4,4,0,Advice Needed: Learning Machine Learning for Computational Biology and Pharmaceutical Research,"Hi Everyone! I hope that I'm able to make this post as concise as possible so as not to waste your collective time. Thank you in advance for taking the time to read through it :D



I'm a masters student studying biomedical sciences. I majored in physics with a minor in mathematics. The year I graduated, physics funding was still down from the recession, so I did some coffee shop type jobs and took a break before ultimately being accepted into a masters program. I'm familiar with programming in Python and I'm just learning the fundamentals of Java. I have a good handle on calculus, basic physics, linear algebra, and applied statistics.



I want to take Udacity's nanodegree in machine learning, and in general, I would like to specialize in cross disciplinary computational biology. There's no way to know for sure if it will help my career, but it's interesting to me nonetheless. I'm currently doing little refreshers on java and linear algebra, but I'm finding that my physics education was completely devoid of **conditional probability**, and **stochastic analysis**. I reel at the sight of equations of the form P(x|a) = P(x)P(a|x = sum(xi)) &lt;-for the record, this probably is a meaningless equation and I'm aware of that.



There are three books I'm considering on the subject:

[An Introduction to Probability and Stochastic Methods](https://www.amazon.com/Introduction-Probability-Stochastic-Processes-Mathematics/dp/0486490998/ref=sr_1_12?ie=UTF8&amp;qid=1471103376&amp;sr=8-12&amp;keywords=probability+dover)

[Principles of Statistics](https://books.google.com/books?id=dh24EaSrmBkC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=twopage&amp;q&amp;f=false)

[Probability: An Introduction](https://www.amazon.com/Probability-Introduction-Samuel-Goldberg/dp/0486652521/ref=sr_1_9?ie=UTF8&amp;qid=1471103376&amp;sr=8-9&amp;keywords=probability+dover)

Which of these books make the most sense to rent from the library and start digging through? Or is there some other method of learning this information that I haven't considered yet?



Any other advice would be appreciated if you have some ideas! For the record I'm not that smart of a guy, but I'm generally decent with theory. My main goals in my career is to be able to work hard in a field I feel at least mildly adept at and be able to help contribute to our knowledge of biology (and if I make a little bit of money that would be cool too).



**Edit: Oh, and thanks again!**

**Edit 2: Some more mistakes**",false,4xjqaj,,0,,false,1473042821,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xjqaj/advice_needed_learning_machine_learning_for/,t3_4xjqaj,,false,,
1470332465,MachineLearning,darkconfidantislife,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w5r5p/psa_gtx_titan_x_back_in_stock/,0,0,0,0,PSA: GTX Titan X back in stock,Title,false,4w5r5p,,0,,false,1473017261,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w5r5p/psa_gtx_titan_x_back_in_stock/,t3_4w5r5p,,false,,
1470435455,MachineLearning,hyperqube12,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wczj7/two_naive_questions/,7,4,4,0,Two naive questions.,"So I've read the wiki, I've read pretty much all the nice blog posts about machine learning, I do have somewhat of an idea of what the field means and what people do within it....somewhat. 

Here's the short version of my background: PhD in quantum information, some programming skills, very interested in AI. 

Now for my naive series of questions: 

1) As a beginner, having a million sources to start with is more harmful than helpful. I've found a few nice books that look to get you up and running writing code and using libraries (scikit-learn, tensorflow, etc.). However, they are really shallow on the explanation part. I am looking for a solid book to start with, and, from what I've managed to find so far, there are two choices: [Murphy's Book](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr_1_1?ie=UTF8&amp;qid=1470435013&amp;sr=8-1&amp;keywords=machine+learning+a+probabilistic+perspective), which some people have told me is THE textbook in the field, and [Marsland's Book](https://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1466583282/ref=sr_1_1?ie=UTF8&amp;qid=1470435027&amp;sr=8-1&amp;keywords=machine+learning+algorithmic+perspective), which, as the name says, is oriented towards writing actual code (indeed, in Murphy's book I did not see a single line of code or even pseudocode).

Thus, the question, Murphy introduces you to a ton of statistics, but is it a good place to start ? I was thinking of doing Murphy first and then Marsland, once I have some understanding of what is actually going on.


2) It might sound funny that I place this as a second question, but is ML the best place to start learning about AI ? Are there any AI specific books that would be of use?

Well, that's about it, looking forward to reading your opinions. ",false,4wczj7,,0,,false,1473020968,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wczj7/two_naive_questions/,t3_4wczj7,,false,,
1472599934,MachineLearning,quickhull,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50e4u9/am_i_missing_something_which_theoretical/,2,1,1,0,Am I missing something? Which theoretical principles should I learn or relearn?,"Hello everybody,

this might sound like a stupid question and I'm sorry if I'm posting this in the wrong subreddit. 

Before I go into details, let me briefly let me give you a little bit of background information about myself. I'm a pretty fresh PhD student in the field of data analysis (few months in). I did my both my bachelor and master in the field in computer science with a focus on the areas of data mining/visualization/machine learning. In my bachelor my task was to identify similar gene expression patterns (using existing and some self defined similarity measures) and for my master thesis I compared documents using combinations of multiple available modalities like text, images, metadata, etc. Now I'm at the beginning of my PhD and although I already had one small accepted publications, I somehow can't shake off feeling like a fraud sometimes. 

In principle, I know how the techniques I'm using work, how I can apply them and how I can combine them to get the desired results.  But for me it seems, that the more papers and books I read, the more lost I feel. There are so many different niches, so many different theoretical backgrounds and so many different combinations of techniques, that I sometimes don't know if a given or self-constructed approach if or why an approach is the right or even the best one. Sometimes I think that I'm missing some fundamental basics, that would make sense of everything and put everything into it's correct place. However I don't know what I'm missing? When I'm dealing with computer vision I think that I should deepen my grasp on linear algebra, when dealing with networks or graphs it seems that my graph theory skills are lacking, same with text, timeseries, spatio-temporal problems, statistics, etc.

I'm aware that a huge part of my PhD is research and learning, but I can't help but feeling overwhelmed by diversity and scope of ... well somehow everything. That is why I'm now asking you now. Is it normal to feel that way? Do you have any tips or advice you could give me? Is there some core fundamental knowledge, that would help me to make some sense of everything thats going on in the machine learning domain?

Sorry for the long read, but I'm really looking forward to your answers and I would be really grateful to any advice you could give a new PhD student.",false,50e4u9,,0,,false,1473095159,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50e4u9/am_i_missing_something_which_theoretical/,t3_50e4u9,,false,,
1470809521,MachineLearning,LazyOptimist,stuhlmueller.org,https://stuhlmueller.org/papers/preferences-aaai2016.pdf,0,3,3,0,"Learning the Preferences of Ignorant, Inconsistent Agents",,false,4x0rtt,,0,,false,1473033123,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4x0rtt/learning_the_preferences_of_ignorant_inconsistent/,t3_4x0rtt,,false,Research,
1471846607,MachineLearning,TechwellJoanna,youtube.com,https://www.youtube.com/attribution_link?a=ecCvw8uc9YI&amp;u=%2Fwatch%3Fv%3D8tBHecViMKU%26feature%3Dshare,1,0,0,0,Corrugated Panel Punching Machine,,false,4yz7p8,,0,,false,1473068937,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yz7p8/corrugated_panel_punching_machine/,t3_4yz7p8,,false,,
1470052013,MachineLearning,Hidden_dreamz,youtube.com,https://www.youtube.com/watch?v=Jg4sl7WO13w,0,0,0,0,Syria Dreaming,,false,4vm2kt,,0,,false,1473006972,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vm2kt/syria_dreaming/,t3_4vm2kt,,false,,
1470654509,MachineLearning,pilooch,github.com,https://github.com/dmorr-google/wiki-reading,1,24,24,0,"Wiki-Reading, a novel large-scale task",,false,4wpm0v,,0,,false,1473027447,false,http://a.thumbs.redditmedia.com/Du5EFtED8PE49lsbSvSAP1Vl7YA57QJlbIOvszEfIT0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wpm0v/wikireading_a_novel_largescale_task/,t3_4wpm0v,,false,,
1470997009,MachineLearning,elephant612,arxiv.org,https://arxiv.org/abs/1607.03474,13,16,16,0,Recurrent Highway Networks achieve SOTA on PennTreebank word level language modeling,,false,4xctvk,,0,,false,1473039300,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4xctvk/recurrent_highway_networks_achieve_sota_on/,t3_4xctvk,,false,Research,
1470217228,MachineLearning,desu-no,chainer.org,http://chainer.org,1,9,9,0,Chainer: A flexible framework of neural networks,,false,4vxoyi,,0,,false,1473013031,false,http://b.thumbs.redditmedia.com/fbpv65iWHIjp7DMb4Bmlykqr5y2EWMRTSAiQICvoivk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vxoyi/chainer_a_flexible_framework_of_neural_networks/,t3_4vxoyi,,false,,
1472145708,MachineLearning,Zenfinch,code.facebook.com,https://code.facebook.com/posts/561187904071636,3,73,73,0,Facebook AI Research Team Open Source DeepMask and SharpMask,,false,4zjo40,,0,,false,1473079359,false,http://a.thumbs.redditmedia.com/kYHxdBAcnlgeSWbWVTX4O1YbBCj3iHeS2VXA4ORAnq0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zjo40/facebook_ai_research_team_open_source_deepmask/,t3_4zjo40,,false,,
1470512913,MachineLearning,theoszymk,8fractals.com,http://8fractals.com/#/fractal/Machine_Learning/155/,0,0,0,0,"Mapping out the Machine Learning landscape on Fractal (Applications, tutorials, models ..)",,false,4whfes,,0,,false,1473023249,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4whfes/mapping_out_the_machine_learning_landscape_on/,t3_4whfes,,false,,
1472670106,MachineLearning,abstractcontrol,cloud.google.com,https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow,32,298,298,0,How a Japanese cucumber farmer is using deep learning and TensorFlow,,false,50ivm9,,0,,false,1473097620,false,http://b.thumbs.redditmedia.com/LGCRfDhbymlWeB_jidNCZfYRpnDaWbOK8XywJbpDEGM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50ivm9/how_a_japanese_cucumber_farmer_is_using_deep/,t3_50ivm9,,false,,
1472059983,MachineLearning,allenleein,medium.com,https://medium.com/open-intelligence/recommended-resources-for-learning-ai-3ab4023cfa85#.wh0ml3fan,0,1,1,0,Recommended Resources for Learning AI (1),,false,4zdqcp,,0,,false,1473076324,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zdqcp/recommended_resources_for_learning_ai_1/,t3_4zdqcp,,false,,
1471463250,MachineLearning,siddkotwal,oreilly.com,https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime?twitter=@bigdata,13,59,59,0,A technique to explain the predictions of any machine learning classifier.,,false,4y7ppw,,0,,false,1473054986,false,http://b.thumbs.redditmedia.com/ahKnwE-xJd3e_Y_ExzZ0C7DLezI8uo9_D01hD3rOCQs.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y7ppw/a_technique_to_explain_the_predictions_of_any/,t3_4y7ppw,,false,,
1472582379,MachineLearning,LegionOfPie,onlinelibrary.wiley.com,http://onlinelibrary.wiley.com/doi/10.1002/qua.24890/abstract,4,2,2,0,Constructing high-dimensional neural network potentials: A tutorial review,,false,50cosv,,0,,false,1473094416,false,http://b.thumbs.redditmedia.com/2nhSVL5eDP6op0OO7ETSON4ilYCdcXxiGYNbHF3O-Ys.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50cosv/constructing_highdimensional_neural_network/,t3_50cosv,,false,,
1471909250,MachineLearning,adamnemecek,seor.gmu.edu,http://seor.gmu.edu/~klaskey/papers/Laskey_FOBL.pdf,1,5,5,0,First-Order Bayesian Logic,,false,4z3ied,,0,,false,1473071127,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z3ied/firstorder_bayesian_logic/,t3_4z3ied,,false,,
1472029625,MachineLearning,[deleted],userworkstech.blogspot.in,http://userworkstech.blogspot.in/2016/05/building-customer-loyalty-with.html,0,1,1,0,Building customer loyalty with artificial intelligence,[deleted],false,4zbnft,,0,,false,1473075274,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zbnft/building_customer_loyalty_with_artificial/,t3_4zbnft,,false,,
1470011185,MachineLearning,arobella,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vjvs9/hard_negative_mining_clarification_creating_new/,0,1,1,0,Hard Negative Mining Clarification: Creating New Training Batches?,[removed],false,4vjvs9,,0,,false,1473005820,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vjvs9/hard_negative_mining_clarification_creating_new/,t3_4vjvs9,,false,,
1470682503,MachineLearning,mohaukachi,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wrt1w/journalism_in_local_government_is_dwindling_could/,2,0,0,0,"Journalism in local government is dwindling, could machine learning fill the gaps?",[removed],false,4wrt1w,,0,,false,1473028562,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wrt1w/journalism_in_local_government_is_dwindling_could/,t3_4wrt1w,,false,,
1472546939,MachineLearning,long_dist_researcher,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50a59j/independent_researcher_with_financial_issues_need/,8,17,17,0,Independent researcher with financial issues - need advice regarding publishing,"I am an independent researcher (not aligned with any university or lab) and actively research on the side. One of my papers was accepted in a tier-one conference workshop (ICML, NIPS, KDD etc) and it was a huge hassle to be able to go there and present because of finances.

My next project is reaching the stage of publication and I was wondering what I could do to avoid the hassle from last time.

Maybe I should go for quality journals? (yes, there are advantages of being present at the conference and exchanging ideas -- but its something I can't afford). Do you know any examples?

What are other other avenues I could pursue to get my research validated but at the same time not take a hit financially?

Field of current research is: Social data mining, networks, spatio-temporal.

EDIT: Is arvix a good option for validation?",false,50a59j,,0,,false,1473093085,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50a59j/independent_researcher_with_financial_issues_need/,t3_50a59j,,false,,
1472055110,MachineLearning,wintermute93,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zdcpj/unsupervised_noisy_audio_segmentation/,2,1,1,0,Unsupervised noisy audio segmentation,"ML novice here, sorry for the nontechnical question and thanks in advance for any suggestions! I have a collection of audio recordings tnat are mostly noise, but occasionally contain short bursts of meaningful signals. Think something like several minutes of radio static punctuated by occasional speech/music, or a recording of ambient highway noise with occasional car horns, etc. The content and timing of the hidden signal bursts is unknown, and typically has fairly low SNR. I'm looking for a ML approach to automatically slicing the recording into periods of noise and periods of signal activity. I know that the DSP community has done a lot of work on denoising, but at the moment I'm more interested in putting together an unsupervised learning approach to recognizing the presence of real signals, rather than some combination of filters and stuff to recover a clean signal from a noisy one. Unsupervised is key here, because labeled versions of the data I'm working with does not exist.

As a toy example to get started, I generated a bunch of sample data that's just a few minutes of white noise added to a signal that alternates every few seconds between silence and a pure tone. Taking the FFT and running k-means with k=2 on the result works very well in this simple case, even with very weak signals, since there's a clear difference between time points with equal intensity at all frequencies and time points with much higher intensity at the signal frequency, but I don't think something that simple will work when there might be more than one type of signal present, or when the signal present has more complicated frequency content.

If you look at the spectrogram of the types of recordings I'm working with, they will look like a mostly uniform background of empty space with some small colored blobs scattered around. Detecting those blobs really seems like something that should be easy to solve with machine learning as an image segmentation or object recognition process, but I'm really not sure how to approach it. Something with CNNs? Autoencoders? Whenever I try searching for relevant techniques, I keep getting results that are for hugely more complex problems -- searching for how to recognize objects in images gives you stuff like ImageNet, for example.

tldr: if you wanted to use machine learning to take [something like this image as input](http://i.imgur.com/nu1BXzL.jpg)  and output ""I see five blobs, with centers (x1,y1),...,(x5,y5)"", or put a bounding box around everything it thinks is a non-background object, or better yet, ""I see four of one type of blob with centers (x1,y1),...,(x4,y4) and one of another type of blob with center (x5,y5)"", how would you go about doing that in an unsupervised way?

Thanks!",false,4zdcpj,,0,,false,1473076131,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zdcpj/unsupervised_noisy_audio_segmentation/,t3_4zdcpj,,false,,
1470685528,MachineLearning,Greendogo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ws2u0/are_services_like_aws_still_usable_for_dcnns/,11,0,0,0,Are services like AWS still usable for DCNNs?,"I'm not referring to Amazon's abysmal cards (they're currently still using K520's which are dirt slow), but rather are Neural Networks, anymore, going to get any benefit from distributed compute resources?  I've heard that a lot of the time now Neural Network training is bandwidth limited, so even if I could rent out a hundred GPU instances from Amazon, I'd still be better off having an on-site GPU box with a few good cards ~~in SLI~~.

What are your experiences with this?",false,4ws2u0,,0,,false,1473028703,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ws2u0/are_services_like_aws_still_usable_for_dcnns/,t3_4ws2u0,,false,,
1471986808,MachineLearning,omniagogacious,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z8t6i/question_about_probabilistic_derivation_of/,1,2,2,0,question about probabilistic derivation of least-squares cost function,"I'm reading through Andrew Ng's CS229 notes, and I'm having trouble understanding how he made the connection between the two probability densities that you can see here: http://imgur.com/a/6coCw.

For reference, his notes are http://cs229.stanford.edu/notes/cs229-notes1.pdf, top of page 12.

I understand that epsilon = y - theta*x. I don't understand how he jumps from one probability density to the other. Please let me know where to post this question if this is the wrong place to post. Thanks!",false,4z8t6i,,0,,false,1473073825,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z8t6i/question_about_probabilistic_derivation_of/,t3_4z8t6i,,false,,
1471911198,MachineLearning,zhongwenxu,plus.google.com,https://plus.google.com/+VincentVanhoucke/posts/8T7DSJhGY3u,10,196,196,0,Google Brain released two large datasets for robotics research,,false,4z3nmo,,0,,false,1473071201,false,http://b.thumbs.redditmedia.com/IMekXFAR0FiVjmp99DipJZSA7mZeAd5PVuNIEd_OU6Q.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z3nmo/google_brain_released_two_large_datasets_for/,t3_4z3nmo,,false,,
1471919745,MachineLearning,efavdb,efavdb.com,http://efavdb.com/model-selection/,0,2,2,0,Hyperparameter sample size dependence,,false,4z4aeo,,0,,false,1473071519,false,http://b.thumbs.redditmedia.com/Cygxo3M21buBDWXKiDVAeIzahb_UcsPUEmgKK5wELqY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z4aeo/hyperparameter_sample_size_dependence/,t3_4z4aeo,,false,,
1470635951,MachineLearning,iamkeyur,experfy.com,https://www.experfy.com/blog/how-to-become-a-data-scientist-part-3-3,0,1,1,0,"How to Become a Data Scientist, Part 3",,false,4woq1q,,0,,false,1473026994,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4woq1q/how_to_become_a_data_scientist_part_3/,t3_4woq1q,,false,,
1471795666,MachineLearning,shash273,github.com,https://github.com/pinkeshbadjatiya/trust-inference-API,0,1,1,0,Open source library for Trust Inference in Social Media,,false,4yvk7d,,0,,false,1473067096,false,http://a.thumbs.redditmedia.com/gpM3V7A6ztuCNcTFtJnd3eQn8pUR_SoO_tef7ZHTm00.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yvk7d/open_source_library_for_trust_inference_in_social/,t3_4yvk7d,,false,,
1472540331,MachineLearning,sealince,kefid.com,http://www.kefid.com/v3/Solution/Ore_beneficiation/,0,1,1,0,Ore beneficiation plant - Machinery,,false,509twz,,0,,false,1473092926,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/509twz/ore_beneficiation_plant_machinery/,t3_509twz,,false,,
1470663909,MachineLearning,vladdione,indiegogo.com,https://www.indiegogo.com/projects/cowarobot-r1-the-first-and-only-robotic-suitcase-travel-technology--2#/,0,0,0,0,COWAROBOT: The First and Only Robotic Suitcase,,false,4wq7rj,,0,,false,1473027752,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wq7rj/cowarobot_the_first_and_only_robotic_suitcase/,t3_4wq7rj,,false,,
1471879883,MachineLearning,ajmooch,nips.cc,https://nips.cc/Conferences/2016/AcceptedPapers,33,73,73,0,NIPS 2016 Accepted Papers,,false,4z12jr,,0,,false,1473069885,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z12jr/nips_2016_accepted_papers/,t3_4z12jr,,false,,
1472564041,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1603.05953,2,7,7,0,[1603.05953] Katyusha: The First Direct Acceleration of Stochastic Gradient Methods,,false,50b4th,,0,,false,1473093588,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/50b4th/160305953_katyusha_the_first_direct_acceleration/,t3_50b4th,,false,Research,
1470850251,MachineLearning,hX3S,ujjwalkarn.me,https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/,1,40,40,0,An Intuitive Explanation of Convolutional Neural Networks,,false,4x3aoq,,0,,false,1473034411,false,http://a.thumbs.redditmedia.com/uOR0PVJ7CksmjmTmZkicIAnnGD9EzKUW6NMrR-AHQA8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x3aoq/an_intuitive_explanation_of_convolutional_neural/,t3_4x3aoq,,false,,
1472228285,MachineLearning,StagaDish,github.com,https://github.com/stagadish/NNplusplus,16,14,14,0,I implemented an easy-to-use and self-contained Neural Net for C++. Any Feedback is welcomed!,,false,4zpf4f,,0,,false,1473082300,false,http://b.thumbs.redditmedia.com/-NcMN6HDDiD06Pen_9razYRT20MQg8iN4vJTbk4fOHQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zpf4f/i_implemented_an_easytouse_and_selfcontained/,t3_4zpf4f,,false,,
1472599327,MachineLearning,alotofdata,megaface.cs.washington.edu,http://megaface.cs.washington.edu/participate/challenge2.html,2,7,7,0,"New face data: 672K identities, 4.7M photos",,false,50e38n,,0,,false,1473095137,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50e38n/new_face_data_672k_identities_47m_photos/,t3_50e38n,,false,,
1471652151,MachineLearning,[deleted],digitaltrends.com,http://www.digitaltrends.com/cool-tech/machine-learning-poverty-map/,1,1,1,0,Machine learning ‘poverty map’ could help aid get to the right places in Africa,[deleted],false,4ym7u2,,0,,false,1473062339,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ym7u2/machine_learning_poverty_map_could_help_aid_get/,t3_4ym7u2,,false,,
1470616510,MachineLearning,iorefusk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wnllx/clean_simple_wikipedia_dataset/,1,2,2,0,Clean Simple Wikipedia dataset?,[removed],false,4wnllx,,0,,false,1473026417,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wnllx/clean_simple_wikipedia_dataset/,t3_4wnllx,,false,,
1470240028,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vz7xo/groups_working_on_machine_learning_with_focus_con/,0,1,1,0,Groups working on machine learning with focus con bayesian nonparametric model,[removed],false,4vz7xo,,0,,false,1473013864,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vz7xo/groups_working_on_machine_learning_with_focus_con/,t3_4vz7xo,,false,,
1471065186,MachineLearning,TuringsEgo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xhque/how_much_ram_does_googlenet_take_to_run/,10,0,0,0,How much ram does googlenet take to run?,"I am trying to decide what gpu to get, an 8gb, 12gb, or 24gb, and I can't seem to find out how much ram the larger deep neural networks seem to require.",false,4xhque,,0,,false,1473041810,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xhque/how_much_ram_does_googlenet_take_to_run/,t3_4xhque,,false,,
1472365971,MachineLearning,feedthecreed,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zyb5p/are_densenets_and_resnets_specific_to_vision/,3,12,12,0,Are DenseNets and ResNets specific to vision problems? Do these ideas work for fully connected networks?,Lots of exciting things have been happening in convnets and vision lately. Has anyone extended ResNets/DenseNets to non-vision tasks? Or does this simply not work? Is there a speech or NLP version of ResNets?,false,4zyb5p,,0,,false,1473086870,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4zyb5p/are_densenets_and_resnets_specific_to_vision/,t3_4zyb5p,,false,Discusssion,
1471578063,MachineLearning,foxh8er,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ygpme/what_is_on_the_cutting_edge_of_anomaly_detection/,5,4,4,0,What is on the cutting edge of anomaly detection today?,"I've mostly been reading and using clustering methods or One-Class SVM's, but I was wondering if there are any new methods within the last ~2 years that appear to work better. 

My data is in the form of spatial relationships if that helps any. ",false,4ygpme,,0,,false,1473059536,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ygpme/what_is_on_the_cutting_edge_of_anomaly_detection/,t3_4ygpme,,false,,
1472278251,MachineLearning,jstaker7,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zt0yg/question_what_is_the_intuition_for_when_to_use/,4,1,1,0,[Question] What is the intuition for when to use larger convolutional kernels,"Although this question in some form has been asked in many places, I still have yet to read an answer or explanation that grants intuition for when it is appropriate to use larger kernels in a convolutional neural network. It seems like smaller kernels like 3x3 are all the rage these days, and I'll often hear things like, ""it allows for more expressive power"" or ""can pick up on finer details"" but this seems opposite to me. Shouldn't larger kernels always have greater expressive power (assuming same number of layers and at a greater computational cost, of course)?

For example, let's say I'd like to discriminate between very small features, such as detecting computer typeface. Should this warrant larger or smaller kernels?

As a quick test I built a 1 layer convolutional auto encoder and as far as I can tell, lower loss and better output from visual inspection of the decoded images are highly correlated with larger filters. So this seems to support my intuition that information is lost after pooling that no matter how many 3x3s we stack with the additional non-linear transformations, it will never be able to capture the information we could have learned if we used a larger filter. Would love some confirmation on this, though.",false,4zt0yg,,0,,false,1473084148,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zt0yg/question_what_is_the_intuition_for_when_to_use/,t3_4zt0yg,,false,,
1472100735,MachineLearning,saseptim,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zgunx/speech_recognition_with_gru_issue_with_short/,4,4,4,0,speech recognition with gru - issue with short samples,"I am training networks for speech recognition in the ""style"" of Deep Speech 2, where I have a few convolution layers, then bi-directional gru, and then the CTC loss. I train on data of which the bulk is close to 16 seconds per sample. I am finding that the success rate on samples that are less than 2 seconds is significantly worse than the success rate on samples whose durations are 3-16 seconds. Any theories as to the reason or suggestions for a solution?",false,4zgunx,,0,,false,1473077914,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zgunx/speech_recognition_with_gru_issue_with_short/,t3_4zgunx,,false,,
1471596443,MachineLearning,NicolasGuacamole,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yhtif/daphne_kollers_pgm_course/,17,18,18,0,Daphne Koller's PGM course,"Soon, Daphne Koller's course on probabilistic graphical models is starting again. I've recently become interested in this area, and will be doing the course once it comes out.

I was wondering if anybody from this sub also intends to do it, and if they would like to form an online study-group around it. Supposedly it is a very challenging course.

[Link here.](https://www.coursera.org/learn/probabilistic-graphical-models)

edit: Looks like there is already a (dead) sub for the class. I propose we work out of there. /r/pgmclass",false,4yhtif,,0,,false,1473060097,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yhtif/daphne_kollers_pgm_course/,t3_4yhtif,,false,,
1472215790,MachineLearning,godspeed_china,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zof9g/tricks_that_convert_unsupervised_learning_to/,26,17,17,0,tricks that convert unsupervised learning to supervised learning,"label is expensive; unsupervised learning data is everywhere; supervised learning is mature and powerful. Thus it is favored to convert unsupervised learning to supervised learning using some tricks.  Here, I can give two examples:  
(1) artificially generate a negative sample. eg. you have a lot of english text, and you could generate a random text or a random permuted text as negative sample. Now you can train a classifier to classify ""good“ text against ""bad"" text. It raises some meaningful score to your unlabeled text.  
(2) try to predict part of your data with other parts of your data. eg. word2vector try to predict a word with its local content.  
  
The purpose of this post is to collect more tricks from you guys.  Thus teach me more tricks if possible. :-)  
",false,4zof9g,,0,,false,1473081786,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zof9g/tricks_that_convert_unsupervised_learning_to/,t3_4zof9g,,false,,
1471022684,MachineLearning,mt37,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xeokr/how_would_you_deal_with_multiple_concepts_in_a/,3,1,1,0,"How would you deal with multiple ""concepts"" in a convnet ?","I'm trying to make a model for image segmentation (fcn-like), but I have also implemented a second model for the question ""is there even a thing of interest at all"".

My question is : should these be two distinct models, or can they share the early convolutions that reduce the dimensions - then one 'head' would feed into a fully connected graph for Y/N questions and the other into a series of 'deconv' for the pixelwise prediction.

Essentially is it at all possible (and good ?) to have multiple losses that are being optimized concurrently within one network.",false,4xeokr,,0,,false,1473040254,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xeokr/how_would_you_deal_with_multiple_concepts_in_a/,t3_4xeokr,,false,,
1471713706,MachineLearning,danielcanadia,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yq38d/create_embeddings_with_tensorflow/,1,1,1,0,Create Embeddings with Tensorflow,"Hi everyone,

I want to create a deep neural network who's first layer maps words to vector embeddings. Essentially: Y = Wx + b where Y is the corresponding word embedding and x is the one-hot encoded vector for a word.

However, I found feeding such a large x as a placeholder for each word full of zero makes the model extremely slow, so I was wondering if there are any alternatives aside from running word2vec in C and then just feeding the already embedded word vectors in.",false,4yq38d,,0,,false,1473064320,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yq38d/create_embeddings_with_tensorflow/,t3_4yq38d,,false,,
1471628944,MachineLearning,larshh,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yk9oa/what_have_been_the_most_useful_recent/,9,6,6,0,What have been the most useful recent developments for neural networks in different areas?,"With dozens of papers uploaded on Arxiv every day it can be hard to see the bigger picture. Especially you might not work in all areas (ConvNets / RNNs / generative models / ...) at the same time. What are the most important corner stones over the last year in each of those areas? I'll start with
&amp;nbsp;

ConvNets:
-ResNets
&amp;nbsp;

RNNs:
-Attention
-Memory
-Neural Turing machines
...though which is really the one that stuck?
&amp;nbsp;

Generative models:
-Variational Auto-encoders (sorry that's older than a year but seems to just have gained traction recently)
-Generative Adversarial Models
-Pixel RNN for image generation
&amp;nbsp;

Reinforcement learning:
-??
&amp;nbsp;

Model Compression:
-Dark knowledge
-??
&amp;nbsp;

Optimization Algorithms:
-ADAM
&amp;nbsp;

Please add areas so long they are general",false,4yk9oa,,0,,false,1473061344,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yk9oa/what_have_been_the_most_useful_recent/,t3_4yk9oa,,false,,
1472511739,MachineLearning,lostwhitewalker,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/507wjn/questionhow_to_split_dataset_into_training_test/,2,1,1,0,"[Question]How to split dataset into training, test, validation using scikit?","I have the following dataset in a csv file. p_i_j value of pixel in row i, column j with value 0 or 1. 
[![I have the following dataset][1]][1]


  [1]: http://i.stack.imgur.com/Q9K59.png

I read the csv file using pandas.
     
     df = pd.read_csv('letter.csv')

When I print df.values i get the following results

    [['o' 0 0 ..., 0 0 0]
     ['m' 0 0 ..., 0 0 0]
     ['m' 0 0 ..., 0 0 0]
     ...,
     ['i' 0 0 ..., 1 0 0]
     ['a' 0 0 ..., 0 0 0]
     ['l' 0 1 ..., 1 1 0]]

SciKit Learn has a method for splitting dataset.

    sklearn.corss_validation.train_test_split(X,Y, test_size,random_stat)

How do I use SciKit Learn to split my dataset? What will replace `X` and `Y`?
",false,507wjn,,0,,false,1473091944,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/507wjn/questionhow_to_split_dataset_into_training_test/,t3_507wjn,,false,,
1471372805,MachineLearning,thegrenadines,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y1394/deep_learning_question_concatenating_the_output/,14,5,5,0,Deep learning question - concatenating the output of multiple layers of a neural net?,"I'm sure I read somewhere about a feature extraction approach where, instead of using the just soft output of a neural net as the features, this was supplemented with the output of an earlier layer of the same net. So representations of larger dimensionality were produced - the features consisted of a concatenation of the final layer, and an intermediate layer. Is anyone aware of this/have a reference for something similar to this? Thanks",false,4y1394,,0,,false,1473051642,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y1394/deep_learning_question_concatenating_the_output/,t3_4y1394,,false,,
1472683377,MachineLearning,bbsome,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50k01y/variational_dropout_for_rnns_sigma/,2,1,1,0,Variational Dropout for RNNs - sigma?,"Link to paper: http://arxiv.org/pdf/1512.05287v3.pdf

So it seems to me that the authors are assuming that sigma is so negligibly small that they can ignore it (as from what I've saw from the implementation and that they are ""masking"" the input). Is this correct? ",false,50k01y,,0,,false,1473098196,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50k01y/variational_dropout_for_rnns_sigma/,t3_50k01y,,false,,
1472175873,MachineLearning,llSourcell,youtu.be,https://youtu.be/cdLUzrjnlr4,3,4,4,0,Build a Recurrent Neural Net in 5 Min,,false,4zm5rw,,0,,false,1473080635,false,http://a.thumbs.redditmedia.com/-C3ks-P_XaK_dUoar9OOAqKHnW-ZGd_81cqbYtHfzS8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zm5rw/build_a_recurrent_neural_net_in_5_min/,t3_4zm5rw,,false,,
1470287799,MachineLearning,put_up_your_dukes,arstechnica.com,http://arstechnica.com/gadgets/2016/08/ibm-phase-change-neurons/,0,1,1,0,IBM creates world's first artificial phase-change neurons,,false,4w2y1p,,0,,false,1473015817,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w2y1p/ibm_creates_worlds_first_artificial_phasechange/,t3_4w2y1p,,false,,
1470740483,MachineLearning,Mrarthurachance,youtube.com,https://www.youtube.com/attribution_link?a=gsPRZUv1S5A&amp;u=%2Fwatch%3Fv%3DH3pOTtdTf20%26feature%3Dshare,3,0,0,0,Michael Jackson AI,,false,4wvm1v,,0,,false,1473030503,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wvm1v/michael_jackson_ai/,t3_4wvm1v,,false,,
1471595865,MachineLearning,ookwrd,tokyohackathon.wordpress.com,https://tokyohackathon.wordpress.com/,8,7,7,0,"Registrations open for Tokyo Conscious Machine Hackathon (Sept 3, 2016)",,false,4yhse0,,0,,false,1473060083,false,http://b.thumbs.redditmedia.com/J-CKgVvm8ce_CAxQW5oUWj9V55k3Z5Nio_XGQUYtI8Y.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yhse0/registrations_open_for_tokyo_conscious_machine/,t3_4yhse0,,false,,
1470403144,MachineLearning,bdamos,ydwen.github.io,http://ydwen.github.io/papers/WenECCV16.pdf,2,10,10,0,[ECCV 2016] A Discriminative Feature Learning Approach for Deep Face Recognition,,false,4wacaf,,0,,false,1473019604,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wacaf/eccv_2016_a_discriminative_feature_learning/,t3_4wacaf,,false,Research,
1470225322,MachineLearning,NeatMonster,youtu.be,https://youtu.be/pAqrSM3drxw?t=7,34,147,147,0,Evolving blob creatures using Neural Networks,,false,4vy3vb,,0,,false,1473013256,false,http://b.thumbs.redditmedia.com/3o7LE-AfngExaxK_22oHpJWpAsLV5Q5zDrl53234V2g.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vy3vb/evolving_blob_creatures_using_neural_networks/,t3_4vy3vb,,false,,
1472053546,MachineLearning,jackerfrinandis,motorlubrication.hatenadiary.com,http://motorlubrication.hatenadiary.com/entry/2016/08/24/015346,0,1,1,0,Hydraulics and Pneumatics – The Difference,,false,4zd86d,,0,,false,1473076068,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zd86d/hydraulics_and_pneumatics_the_difference/,t3_4zd86d,,false,,
1472665154,MachineLearning,bahidev,youtube.com,https://www.youtube.com/attribution_link?a=RjW8BPMWEIA&amp;u=%2Fwatch%3Fv%3DgWmRkYsLzB4%26feature%3Dshare,0,0,0,0,The jobs we'll lose to machines -- and the ones we won't | Anthony Goldb...,,false,50ifx6,,0,,false,1473097397,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50ifx6/the_jobs_well_lose_to_machines_and_the_ones_we/,t3_50ifx6,,false,,
1470082025,MachineLearning,obayes,news.mit.edu,http://news.mit.edu/2016/seymour-papert-pioneer-of-constructionist-learning-dies-0801,3,74,74,0,"Professor Emeritus Seymour Papert, pioneer of constructionist learning, dies at 88",,false,4voi3u,,0,,false,1473008243,false,http://b.thumbs.redditmedia.com/Q1qYU5_F4BDviCqHzGUJ9Jt3OUK40FyAiYGDu6Th0dk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4voi3u/professor_emeritus_seymour_papert_pioneer_of/,t3_4voi3u,,false,,
1470345396,MachineLearning,amplifier_khan,adeshpande3.github.io,https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/,1,2,2,0,A Beginner's Guide To Understanding Convolutional Neural Networks,,false,4w6uqi,,0,,false,1473017824,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w6uqi/a_beginners_guide_to_understanding_convolutional/,t3_4w6uqi,,false,,
1470608182,MachineLearning,EsportsinaNutshell,machinelearninginanutshell.com,http://www.machinelearninginanutshell.com/index.php/2016/08/07/20/,0,0,0,0,Apple buys Turi for $200M | Machine Learning in a Nutshell,,false,4wn0tl,,0,,false,1473026122,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wn0tl/apple_buys_turi_for_200m_machine_learning_in_a/,t3_4wn0tl,,false,,
1470437678,MachineLearning,gwern,azure.microsoft.com,https://azure.microsoft.com/en-us/blog/azure-n-series-preview-availability/,60,88,88,0,MS expands cloud GPU offerings: K80s at $0.56/hour,,false,4wd5kn,,0,,false,1473021053,false,http://b.thumbs.redditmedia.com/s4-zy1oNCkuitp5_esl0_pBH2_Niq-mx30GtEVKDT_A.jpg,t5_2r3gv,false,two,,false,false,,/r/MachineLearning/comments/4wd5kn/ms_expands_cloud_gpu_offerings_k80s_at_056hour/,t3_4wd5kn,,false,News,
1470244220,MachineLearning,[deleted],i.redd.it,https://i.redd.it/hri6jkwd97dx.png,0,1,1,0,Gradient Descent concept explained with a story,[deleted],false,4vzl52,,0,,false,1473014073,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vzl52/gradient_descent_concept_explained_with_a_story/,t3_4vzl52,,false,,
1471523002,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ybyi0/numerical_optimization/,0,1,1,0,Numerical Optimization,[removed],false,4ybyi0,,0,,false,1473057144,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ybyi0/numerical_optimization/,t3_4ybyi0,,false,,
1472141362,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zja35/research_on_generating_text/,5,4,4,0,Research on generating text?,[removed],false,4zja35,,0,,false,1473079161,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zja35/research_on_generating_text/,t3_4zja35,,false,,
1470168997,MachineLearning,Danny_Champion,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vukse/comparing_mlr_and_rf/,0,1,1,0,Comparing MLR and RF,[removed],false,4vukse,,0,,false,1473011424,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vukse/comparing_mlr_and_rf/,t3_4vukse,,false,,
1472163926,MachineLearning,Throwaway_DCNN,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zl91b/putting_together_a_podcast_educational_youtube/,0,1,1,0,"Putting together a podcast + educational YouTube videos on A.I. philosophy, Machine learning business applications, industry news, and some neuroscience.",[removed],false,4zl91b,,0,,false,1473080167,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zl91b/putting_together_a_podcast_educational_youtube/,t3_4zl91b,,false,,
1470723875,MachineLearning,RobotCaleb,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wurui/is_this_a_good_learning_set/,1,0,0,0,Is this a good learning set?,"I found this site by chance today and I noticed that the images all have a (usually) correct phrase to describe the image contents. Knowing nothing about machine learning, I thought maybe they'd be handy for classifying (that word is probably overloaded in ML?) other images based on the contents of these ones. It's probably too small a sample set to do anything with. I'm probably speaking nonsense. Anyway, the site: http://psbattle.co/",false,4wurui,,0,,false,1473030077,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wurui/is_this_a_good_learning_set/,t3_4wurui,,false,,
1471501397,MachineLearning,MiksLus,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yaprf/look_alike_machine_learning_algorithm/,4,0,0,0,Look alike machine learning algorithm?,"Hi, all!
I am seeking a advice on how to tackle  a business project we are currently having in our company.
We are a growing fintech company currently having our business in 8 countries. What we would like to do is to base our decision in which country to go next, completely on data. Meaning that we would like to build such an algorithm that outputs us the country to which we should be expanding next.
Problem is that it is a small sample problem and applying something like regression would not really be sufficient solution in my opinion. Plus we have 8 countries which we consider to be good and 0 countries which we can tell by our experience to be bad.
We have many good ideas on feature engineering side but we struggle choosing the right algo. The only thing that came into my mind was to try to make clusters and hope that some new country appears to be in a same cluster that good countries are in.

Do you have any suggestions or ideas, dear redditers? :)",false,4yaprf,,0,,false,1473056519,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yaprf/look_alike_machine_learning_algorithm/,t3_4yaprf,,false,,
1471337752,MachineLearning,Rauau,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xyk06/how_can_i_improve_my_nlp_task/,4,1,1,0,How can I Improve my NLP Task?,"I know this is very unconventional (nonsense)

I have a Feed Forward Neural Network that Predicts the next Character based on the last n Characters. I tried values of 4,5, 7 and 12 for n.  A Character is represented as a 28-dimensional vector, where 27 are one-Hot, the 
27th is space and the 28th is shift. shift + space are newline. 

I took Pride and Prejudice as training data.

KL divergence is my error measure and RMSProp with first and second order momentum and standard parameters 
is my weight update algorithm.

I tried batch sizes of 40,100, 400 and 1000.

After every step, i've put out 100 Characters of generated text.

The only network architecture that didn't end up to always putting out the same character or putting out Spaces was sine activations in all hidden layers and sigmoid in the output layer. The first layer after input was a wide layer with more nodes than input nodes. Then 4-5 layers, decreasing in size.

With a test training set only consisting of ""LOLOLOLOLOLO..."", it converged after 3 training steps with n = 4 and batch size 40, but diverges after that and never converges again. With the Pride and Prejudice set, it puts out total gibberish, no words whatsoever. The classification error is decreasing, but plateaus after ~100 training steps. It does model some probabilities, for example, a vocal is always followed by a consonant, there are never more than 3 consonants in a row, word length is realistic, shift is low most the time, but it hasn't learned that it should only be high after a space character. Also, i think the words sound somehow english. 

I can't provide output text atm because i'm not at home, i can do so in 7 Hours.

My intention behind the sine is that the Bias is far more easy to train because of the periodicity of the sine Function. I already trained some sine networks on MNIST and image regression, and got very good results with both of them. 

Is there any work on Sliding-Window Feed-Forward Neural Networks used for character-level text synthesis? ",false,4xyk06,,0,,false,1473050359,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xyk06/how_can_i_improve_my_nlp_task/,t3_4xyk06,,false,,
1470220318,MachineLearning,stua8992,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vxugk/help_with_inference_on_graphical_models/,1,2,2,0,Help with inference on graphical models,"Hi all,

I'm trying to create a model similar in certain ways to Microsoft's TrueSkill. I've been trying to understand the update equations shown in [their paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2007/01/NIPS2006_0688.pdf), and I'm having some difficulties.

Now I've derived the first two rows of update equations shown in Table 1, but I can't get very far on the third in which x is a linear combination of y. I'm not sure if I'll even be able to ask the right questions, but maybe someone can provide some insight.

So as I understand it, the sum-product algorithm says:

     [; m_{f\rightarrow x} = \int ... \int I(x = a^T y) \prod_i m_{y_i\rightarrow f} dy_1...dy_n ;]

I'm not sure how the identity function really fits here. Is it supposed to be treated like some kind of delta function which removes all of the integrals in a sense? If so is I actually a probability density function which is infinite when the equality is satisfied and 0 elsewhere?

Alternatively is it really the identity function, and that basically just forces the integration to be performed only where the equality is satisfied. If that is the case how exactly are the multiple integrals dealt with?

Once the integrals are sorted I take it that each message can be treated like a Gaussian, and so the product is just a matter of doing the algebra to find the result. 

I'm really not sure if I'm on the right track so any guidance will be much appreciated. For instance am I right in thinking that the integrals should disappear before I have to deal with the product or is that mistaken? Thanks very much
",false,4vxugk,,0,,false,1473013115,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vxugk/help_with_inference_on_graphical_models/,t3_4vxugk,,false,,
1472486332,MachineLearning,insider_7,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/505pd5/tools_to_increase_productivity_in_ml_research/,15,28,28,0,Tools to increase productivity in ML research,"Most of us use Pycharm + LaTeX and our preferred ML library to do research. But what other tools usually help you to increase the productivity of your research? At any stage, from organising literature (e.g.  Mendeley) to use Scripts to automate repetitive tasks.
",false,505pd5,,0,,false,1473090783,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/505pd5/tools_to_increase_productivity_in_ml_research/,t3_505pd5,,false,,
1470845582,MachineLearning,Jollyhrothgar,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x2wjo/neural_networks_and_linear_transformation/,6,0,0,0,Neural Networks and Linear Transformation,"Hi Folks,

I've dealt with baysian methods in machine learning in the past, and now I find myself working a lot with neural networks in learning models. Specifically, multi-layer neural networks and recurrant neural networks.

One metaphor I see again and again, is that classic diagram with 'circles' as neurons, with lines connecting each layer of neurons such that the network is fully connected, the lines themselves representing the weight of the signal from one neuron to the next.

As a learning exercise, I'm working to write a neural network from scratch, and then train it to do some kind of regression and classification. I'm going to start with a single-layer neural network.

Now, my understanding, mathematically, is that the 'best' representation of a single layer neural network is actually just a linear transformation. If we take the simple example of a vector of values, mapped to a vector of outputs, in the same dimensional space, we can represent a neural network like so:

$$w\_{2}A(w\_{1}x) = y$$

Where $x$ is an n dimensional column vector, $y$ is an n dimensional column vector, $w\_{1}$ is a matrix of weights (k by n), A() is a sigmoid function, and $w\_{2}$ is a another matrix of weights transforming the activation 'vector' $w\_{2}x$ back into the output space.

My understanding is that $w\_{2}x$ represents the activation signal sent out by each neuron (of which there are k, in this case), and each 'neuron' is basically represented by the thing that outputs that weight, which is essentially the dot-product of a row of the $w\_{1}$ matrix and the $x$ vector.

The process of training a neural network involves calculating the cost function as the difference between the calculated $y$ value and the true $y$ value, using the residual, and then applying gradient descent on the weights of the network.

So, how's my understanding? I'd like to read more on this, but I don't see many resources take a linear algebra approach which connects back to the standard neuronal metaphor. Any thoughts or suggestions? As a first project, I'd like to write a simple neural network to get the 'best fit line' to a 2D data set.

Thanks!",false,4x2wjo,,0,,false,1473034203,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x2wjo/neural_networks_and_linear_transformation/,t3_4x2wjo,,false,,
1471543726,MachineLearning,WingedAshur6,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yds90/qeustion_about_svm_with_quadratic_programming/,2,1,1,0,Qeustion about SVM with Quadratic programming,"Hi all!

I am currently looking into SVM's as a method for separation of data.


 I understand an implication for creating the quadratic form and passing it into a quadratic programming function however there are a few things that I dont understand.

Starting with a linearly separable dataset:

1.) with the quadratic form, there is this:
sum(i=1,M sum(j=1,M yi*yj*ai*aj*xi_TRANS*xj))

I understand that to find the alphas, or ""a"" in the above, you have to first run the quadratic programming methods using constraints. However, what is j? and what is i? if I have a dataset using x as an nx2 matrix of points to place in a 2-D space, and y being an nx1 matrix containing labels of each point, how does all of that information fit into creating the quadratic coefficient matrix?

2.) are there any sources one may have that has a walkthrough to create an SVM (excluding the quadratic programming -- I plan on using MATLAB's quadprog for that) from beginning with a dataset/labels and ending with calculating the decision boundary and plotting it?

If I am understanding the concepts I have learned correctly, NON-linear separation would just include using the kernel trick in addition, am I correct? just wanted to get a confirmation on that from someone who has had experience with this.

Thanks for reading, and thanks in advance to all!!",false,4yds90,,0,,false,1473058066,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yds90/qeustion_about_svm_with_quadratic_programming/,t3_4yds90,,false,,
1470775163,MachineLearning,theredmirror,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wydpq/machine_learning_for_law_ideas_for_a_thesis/,6,4,4,0,Machine Learning for Law? Ideas for a thesis,"Hello,

I am an undergraduate student (Law) and I have a beginner background on CS (I did some courses and coding on Python, R, Racket, CLisp...).

I would like to know if you have ideas for applications and research topics in Law that use Machine Learning? I have to write my thesis in a year, so I would like to do something unusual and tech-related.

Thanks!",false,4wydpq,,0,,false,1473031911,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wydpq/machine_learning_for_law_ideas_for_a_thesis/,t3_4wydpq,,false,,
1470244307,MachineLearning,Fragore,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vzler/how_do_i_do_backpropagation_with_convolutional/,5,0,0,0,How do I do backpropagation with Convolutional Neural Networks?,"Hi guys! I need your help! I am trying to code a Convolutional Neural Network with:

- 1 convolutional layer

- 1 activation layer (takes the max between 0 and the value)

- 1 max_pooling layer

- 1 fully connected layer

- 1 sigmoid activation layer.



But I don’t know how to implement the back propagation algorithm. I think I made to obtain how to update the weights of the fully connected layer, but now? Moreover I know that in order to do it I must backprop at the end of the forward propagation of all the trainig set, but then how do I update the weights given that I need the value obtained for each training element?
",false,4vzler,,0,,false,1473014076,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vzler/how_do_i_do_backpropagation_with_convolutional/,t3_4vzler,,false,,
1470796158,MachineLearning,ufoym,arxiv.org,https://arxiv.org/pdf/1607.08438v1.pdf,1,3,3,0,Faceless Person Recognition,,false,4wzzhu,,0,,false,1473032726,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wzzhu/faceless_person_recognition/,t3_4wzzhu,,false,,
1470887721,MachineLearning,c3534l,togelius.blogspot.com,https://togelius.blogspot.com/2016/08/algorithms-that-select-which-algorithm.html,0,1,1,0,Algorithms that select which algorithm should play which game,,false,4x65pk,,0,,false,1473035875,false,http://b.thumbs.redditmedia.com/Vml2bW6x8F15vngylnIj_NdvxmrQliCnTo9GGm7tTyg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x65pk/algorithms_that_select_which_algorithm_should/,t3_4x65pk,,false,,
1472631945,MachineLearning,Alexey75,newscientist.com,https://www.newscientist.com/article/mg23130882-300-move-over-silicon-machine-learning-boom-means-we-need-new-chips/,0,1,1,0,Move over silicon: Machine learning boom means we need new chips,,false,50g5eo,,0,,false,1473096211,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50g5eo/move_over_silicon_machine_learning_boom_means_we/,t3_50g5eo,,false,,
1472028040,MachineLearning,wederer42,youtube.com,https://www.youtube.com/watch?v=TkR0GdW2fDE,5,0,0,0,My new GTX 1060 beeps when running machine learning algorithms? Anybody got an idea what that could be?,,false,4zbkqk,,0,,false,1473075237,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zbkqk/my_new_gtx_1060_beeps_when_running_machine/,t3_4zbkqk,,false,,
1471687030,MachineLearning,john_philip,washingtonpost.com,https://www.washingtonpost.com/news/the-intersect/wp/2016/08/19/98-personal-data-points-that-facebook-uses-to-target-ads-to-you,0,1,1,0,Perosnal Data points that facebook uses to target ads to you,,false,4yob3m,,0,,false,1473063408,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yob3m/perosnal_data_points_that_facebook_uses_to_target/,t3_4yob3m,,false,,
1471600908,MachineLearning,rohitoberoi,intellipaat.com,https://intellipaat.com/interview-question/mahout-interview-questions/,0,1,1,0,Machine Learning Algorithms Exposed by Mahout,,false,4yi2kd,,0,,false,1473060224,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yi2kd/machine_learning_algorithms_exposed_by_mahout/,t3_4yi2kd,,false,,
1472036309,MachineLearning,alexeyr,deliprao.com,http://deliprao.com/archives/187,2,3,3,0,Synthetic Gradients: Cool or Meh?,,false,4zbzkw,,0,,false,1473075443,false,http://b.thumbs.redditmedia.com/mfsdXc6w5en1x8PI0Nhpg7tkIbQEr6R823dt867rvUE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zbzkw/synthetic_gradients_cool_or_meh/,t3_4zbzkw,,false,,
1471872268,MachineLearning,perone,blog.christianperone.com,http://blog.christianperone.com/2016/08/jit-native-code-generation-for-tensorflow-computation-graphs-using-python-and-llvm/,9,33,33,0,JIT native code generation for TensorFlow computation graphs using Python and LLVM,,false,4z0i82,,0,,false,1473069585,false,http://b.thumbs.redditmedia.com/hHlMfZerFKrXIpAqHLVMifgSML7huMmjuER368YK6hY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z0i82/jit_native_code_generation_for_tensorflow/,t3_4z0i82,,false,,
1470062215,MachineLearning,gicht,github.com,https://github.com/matthiasplappert/keras-rl,25,190,190,0,keras-rl: A library for state-of-the-art deep reinforcement learning,,false,4vmsmy,,0,,false,1473007346,false,http://b.thumbs.redditmedia.com/EzqBlO7w3XuqhL9T0IerCuP2KJZSzGmxMyA89cGHcPg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vmsmy/kerasrl_a_library_for_stateoftheart_deep/,t3_4vmsmy,,false,,
1470934171,MachineLearning,dharma-1,quora.com,https://www.quora.com/How-are-Tensor-methods-used-in-deep-learning,0,0,0,0,How are Tensor methods used in deep learning?,,false,4x8z0w,,0,,false,1473037318,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x8z0w/how_are_tensor_methods_used_in_deep_learning/,t3_4x8z0w,,false,,
1471293693,MachineLearning,elisebreda,blog.yhat.com,http://blog.yhat.com/posts/olympic-medals.html,2,11,11,0,Transforming Olympics Data into GIFs with Matplotlib.pyplot and Images2gif,,false,4xvpdw,,0,,false,1473048909,false,http://b.thumbs.redditmedia.com/Tuv2SXKyvvAJi5NAiWBKvx2wfB3VolKHDbIlYuv_Owg.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xvpdw/transforming_olympics_data_into_gifs_with/,t3_4xvpdw,,false,,
1472060475,MachineLearning,adamnemecek,cs.ru.nl,http://www.cs.ru.nl/B.Jacobs/PAPERS/cat-prob-th.pdf,2,8,8,0,A Categorical Basis for Conditional Probability,,false,4zdrvw,,0,,false,1473076345,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zdrvw/a_categorical_basis_for_conditional_probability/,t3_4zdrvw,,false,,
1472588445,MachineLearning,amplifier_khan,gab41.lab41.org,https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7#.1hvvynnpk,0,1,1,0,Flexible Image Tagging with Fast0Tag,,false,50d7g7,,0,,false,1473094683,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50d7g7/flexible_image_tagging_with_fast0tag/,t3_50d7g7,,false,,
1472230232,MachineLearning,[deleted],arxiv.org,https://arxiv.org/abs/1603.06277v3,0,1,1,0,[1603.06277v3] Composing graphical models with neural networks for structured representations and fast inference,[deleted],false,4zplai,,0,,false,1473082387,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zplai/160306277v3_composing_graphical_models_with/,t3_4zplai,,false,,
1470783912,MachineLearning,[deleted],gudoshnikov.blogspot.com,http://gudoshnikov.blogspot.com/2016/08/funny-recognition.html,0,0,0,0,An example of how Google image search misunderstands you,[deleted],false,4wz38x,,0,,false,1473032274,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wz38x/an_example_of_how_google_image_search/,t3_4wz38x,,false,,
1472081238,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zfhr3/starting_deep_learning_and_looking_for_good/,0,1,1,0,starting deep learning and looking for good tutorials to start with,[deleted],false,4zfhr3,,0,,false,1473077224,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zfhr3/starting_deep_learning_and_looking_for_good/,t3_4zfhr3,,false,,
1471837715,MachineLearning,robot_t1,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yyr0i/how_to_get_a_job_in_machine_learning_as_an/,0,1,1,0,How to get a job in machine learning as an undergrad ?,[removed],false,4yyr0i,,0,,false,1473068703,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yyr0i/how_to_get_a_job_in_machine_learning_as_an/,t3_4yyr0i,,false,,
1471347505,MachineLearning,deepzeon,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xz1j2/scientific_american_steven_e_shladovers_article/,0,0,0,0,Scientific american Steven E. Shladover's article,[removed],false,4xz1j2,,0,,false,1473050606,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xz1j2/scientific_american_steven_e_shladovers_article/,t3_4xz1j2,,false,,
1471977827,MachineLearning,jbmouret,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z81tz/postdoc_position_humanoid_locomotion_machine/,0,1,1,0,Post-doc position: humanoid locomotion/ machine learning / optimal control theory,[removed],false,4z81tz,,0,,false,1473073437,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z81tz/postdoc_position_humanoid_locomotion_machine/,t3_4z81tz,,false,,
1470177130,MachineLearning,Alirezag,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vv98c/autoencoder_visulization_question/,5,3,3,0,AutoEncoder visulization question,[removed],false,4vv98c,,0,,false,1473011772,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vv98c/autoencoder_visulization_question/,t3_4vv98c,,false,,
1471395525,MachineLearning,FR_STARMER,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y2x6h/is_cuda_implementation_required_to_take_advantage/,15,0,0,0,Is CUDA implementation required to take advantage of GPU processing or are there ways to general purpose processing / training on GPUs without it?,Specifically talking about AWS GPU EC2 instances. I'm not in the mood to dive into the APIs for writing a CUDA implementation of my program. Do I still see a significant boost over CPU training if I switch over to a heavy GPU instance? How do I push my processing to a GPU?,false,4y2x6h,,0,,false,1473052570,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y2x6h/is_cuda_implementation_required_to_take_advantage/,t3_4y2x6h,,false,,
1470706686,MachineLearning,tsmith5151,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wtow8/data_science_or_computer_science_degree/,8,0,0,0,Data Science or Computer Science Degree?,"I would love to get some feedback on a decision I'm having to make within the next couple of days. I've been accepted into two different M.S. programs, one in Data Science and the other in Computer Science. My previous BS/MS is both in engineering, but I do have programming experience (Python). 

My interest is in Machine Learning and data analytics. Therefore, what is the perception in the tech industry with someone who has D.S. degree as opposed to C.S.? It's a relatively new degree so I'm a little hesitant going the data route and concerned what will the demand be for Data Scientist in the next 10 years or so. Is C.S. the safest choice? Any feedback/advice would be greatly appreciated!",false,4wtow8,,0,,false,1473029526,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wtow8/data_science_or_computer_science_degree/,t3_4wtow8,,false,,
1472458306,MachineLearning,hoosierpride1,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/503z36/is_it_possible_to_make_a_flood_prediction_method/,11,3,3,0,Is it possible to make a flood prediction method with LandSat images?,If its possible can you kindly direct me to a solved example or case study?,false,503z36,,0,,false,1473089858,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/503z36/is_it_possible_to_make_a_flood_prediction_method/,t3_503z36,,false,,
1471333677,MachineLearning,fuckinghelldad,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xyd3b/has_anyone_used_a_binary_tree_architecture_in_a/,3,0,0,0,Has anyone used a binary tree architecture in a neural net?,"[Mhaskar and Poggio (2016, Figure 3)](http://arxiv.org/pdf/1608.03287v1.pdf) have a diagram of one, but they didn't use it for a practical application or provide citations showing anyone else who did.",false,4xyd3b,,0,,false,1473050263,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xyd3b/has_anyone_used_a_binary_tree_architecture_in_a/,t3_4xyd3b,,false,,
1471167486,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xnj77/differences_between_loss_functions_for_multilabel/,4,0,0,0,Differences between loss functions for multi-label classification?,"I have a classification problem involving 100 binary labels that I want to classify. It seems that the two main options for the loss function are:

1. Taking the mean over the binary cross-entropy loss across each label (https://en.wikipedia.org/wiki/Binary_entropy_function)
2. Using the Hamming loss (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html)

I'm not interesting in using the zero_one_loss (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html) as I'll very rarely have perfect classifications across all 100 binary labels.

Based on your experiences what are the pros/cons of using either the binary cross-entropy or the Hamming loss for a multi-label problem?",false,4xnj77,,0,,false,1473044755,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xnj77/differences_between_loss_functions_for_multilabel/,t3_4xnj77,,false,,
1471955576,MachineLearning,FFiJJ,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z67z7/comparison_of_machine_learning_libraries/,5,2,2,0,Comparison of machine learning libraries,"I assume most of you don't write your algorithms from 0 and instead use various ml libraries. 
I myself have used mostly dlib (due to the newby friendly documentation and the fact that it has both a C++ and a python API so I can switch language and still use the same concepts).

Recently, I've been thinking about trying out some projects with tensorFlow.

However, I've seen no articles whatsoever that deal with the subject of comparing the various libraries... and there are a lot of them. Even the ones I've seen seem quite outdated.

Is there any good resource for deciding which library to pick ? Do you guys just go with the flow and use whatever is more familiar/used by your team ? Do you do extensive research before choosing a library ? If so, what is your thought-process when doing so, what makes a library ""fit"" for a project ?",false,4z67z7,,0,,false,1473072506,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z67z7/comparison_of_machine_learning_libraries/,t3_4z67z7,,false,,
1472062467,MachineLearning,SwampyPk,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zdy4f/help_computational_time_viola_jones_object/,2,0,0,0,HELP - Computational Time (Viola Jones Object Detection),"Hey.

So I'm a little worried and I was wondering if anyone could shed some light on this.

I've written half the base training algorithm (for only two features hence half) for the Viola Jones Object Detection Framework and I've done the math and it seems like it's going to take 58 minutes at its current state to train it's self on 1 image. Considering I would like to use something in the region of 100 images. That means it's going to take 100 hours!! 

This can't be right. Can someone explain or give be a ball park time frame for what should be expected? 

Thanks!",false,4zdy4f,,0,,false,1473076433,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zdy4f/help_computational_time_viola_jones_object/,t3_4zdy4f,,false,,
1470690315,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wsht2/how_to_use_em_to_infer_the_position_mean_and/,3,0,0,0,"How to use EM to infer the position, mean, and variance for each Gaussian in this pdf---unsupervised GMM?","I have the following data, which when plotted as a histogram, are a mixture of Gaussians:

http://imgur.com/a/TCgZ1

I would like to write an algorithm that would infer

(1) the number of ""peaks"" or normal distributions in this data (i.e. three in the above)
(2) the mean/position of each gaussian
(3) the variance of each Gaussian
(4) the height of each Gaussian/peak

However, I do not know a prior the number of Gaussians in the dataset. Sometimes, there could be four:
http://imgur.com/a/aHjMb

or maybe seven:
http://imgur.com/a/sFcPU


If this is an application of the EM algorithm, the latent variables are mean \mu, variance \sigma, and the parameter is k, the number of peaks? As I don't know the number of clusters a priori, is this a different problem? 

It seems like I'm using density estimation first, and then EM to infer the parameters (i.e. the information of each peak). What solutions exist for this problem? 
",false,4wsht2,,0,,false,1473028916,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wsht2/how_to_use_em_to_infer_the_position_mean_and/,t3_4wsht2,,false,,
1470657478,MachineLearning,mixmachinery,youtube.com,https://www.youtube.com/attribution_link?a=r4QwTWDEM7U&amp;u=%2Fwatch%3Fv%3DpHSvka2l4ss%26feature%3Dshare,1,1,1,0,What is silicone filling manufacturing process,,false,4wprwu,,0,,false,1473027529,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wprwu/what_is_silicone_filling_manufacturing_process/,t3_4wprwu,,false,,
1472316144,MachineLearning,compsens,nuit-blanche.blogspot.fr,http://nuit-blanche.blogspot.fr/2016/08/paris-machine-learning-newsletter.html,0,9,9,0,"Paris Machine Learning Meetup Newsletter, Summer 2016",,false,4zv0f0,,0,,false,1473085175,false,http://b.thumbs.redditmedia.com/IZK7acLJrMeDAm4Nl-V9k_JR0yOTbLDc2w4dy5VXfUo.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zv0f0/paris_machine_learning_meetup_newsletter_summer/,t3_4zv0f0,,false,,
1471891219,MachineLearning,hyperqube12,youtube.com,https://www.youtube.com/watch?v=oYbVFhK_olY,2,2,2,0,New Deep Learning course on YouTube,,false,4z21pa,,0,,false,1473070387,false,http://b.thumbs.redditmedia.com/lywwOX-Hm7NIB9lnXCzje5ad_zRBdgNbBJYDfzk0yno.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z21pa/new_deep_learning_course_on_youtube/,t3_4z21pa,,false,,
1472665286,MachineLearning,KatieContract,blog.contractroom.com,http://blog.contractroom.com/a-brief-history-of-artificial-intelligence,0,1,1,0,A short history of artificial intelligence,,false,50igcq,,0,,false,1473097403,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50igcq/a_short_history_of_artificial_intelligence/,t3_50igcq,,false,,
1470092261,MachineLearning,amplifier_khan,pappubahry.com,http://pappubahry.com/misc/rectangles/,6,10,10,0,Simple R algorithm to find the Rectangularness of Countries,,false,4vpblt,,0,,false,1473008673,false,http://b.thumbs.redditmedia.com/hVxLx2fMIKSY4cBfSDsI9ZndQSjEN0hTe8MftUtS5tA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vpblt/simple_r_algorithm_to_find_the_rectangularness_of/,t3_4vpblt,,false,,
1471728273,MachineLearning,Valiox,arxiv.org,http://arxiv.org/pdf/1608.05148.pdf,28,65,65,0,Full Resolution Image Compression with Recurrent Neural Networks,,false,4yr9w2,,0,,false,1473064922,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yr9w2/full_resolution_image_compression_with_recurrent/,t3_4yr9w2,,false,,
1472645578,MachineLearning,[deleted],github.com,https://github.com/arranger1044/awesome-spn,0,1,1,0,Awesome Sum-Product Networks,[deleted],false,50gux4,,0,,false,1473096579,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50gux4/awesome_sumproduct_networks/,t3_50gux4,,false,,
1471465172,MachineLearning,[deleted],blog.kaggle.com,http://blog.kaggle.com/2016/08/17/making-kaggle-the-home-of-open-data/,0,1,1,0,Today Kaggle Launched its Open Data Platform. launch Kaggle's #opendata platform. Publish your data for the community to explore,[deleted],false,4y7w7c,,0,,false,1473055076,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y7w7c/today_kaggle_launched_its_open_data_platform/,t3_4y7w7c,,false,,
1471733783,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yrpz0/did_kaggle_get_hacked/,0,1,1,0,Did Kaggle get hacked?,[deleted],false,4yrpz0,,0,,false,1473065148,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yrpz0/did_kaggle_get_hacked/,t3_4yrpz0,,false,,
1471828302,MachineLearning,bentzy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yy5fs/knowledge_db_extraction_supervised_learning_form/,0,1,1,0,Knowledge db extraction supervised learning form semi structured documents tools or libraries,[removed],false,4yy5fs,,0,,false,1473068403,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yy5fs/knowledge_db_extraction_supervised_learning_form/,t3_4yy5fs,,false,,
1471442094,MachineLearning,aahdee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y5p4x/best_resources_for_a_newcomer/,1,0,0,0,Best resources for a newcomer?,[removed],false,4y5p4x,,0,,false,1473053969,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y5p4x/best_resources_for_a_newcomer/,t3_4y5p4x,,false,,
1471921739,MachineLearning,rodrinkus,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z4fem/first_mnist_results_for_a_sparse_distributed/,21,6,6,0,"First MNIST results for a sparse distributed representation (SDR) based model, Sparsey","First results of Neurithmic Systems's Sparsey on MNIST (http://www.sparsey.com/MNIST_Results.html).  To my knowledge, Sparsey is the first SDR-based model to achieve near-SOA results, 90%, on a (significant subset of) a widely accepted benchmark, MNIST.  Sparsey uses Hebbian learning, binary units, binary weights, and no gradients or MCMC-type sampling, and is thus computationally far more efficient than RBM- or ConvNet-based Deep Learning approaches.",false,4z4fem,,0,,false,1473071590,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z4fem/first_mnist_results_for_a_sparse_distributed/,t3_4z4fem,,false,,
1472300585,MachineLearning,Kiuhnm,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zu08d/definition_of_generalization_error/,7,5,5,0,Definition of generalization error,"I'm reading the [DLBook](http://www.deeplearningbook.org/). I'm a little confused about the following part:

&gt; The generalization error typically follows a U-shaped curve when plotted as
a function of one of the hyperparameters, as in ﬁgure 5.3. At one extreme, the
hyperparameter value corresponds to low capacity, and generalization error is high
because training error is high. This is the underﬁtting regime. At the other extreme,
the hyperparameter value corresponds to high capacity, and the generalization
error is high because the gap between training and test error is high.

According to [wikipedia](https://en.wikipedia.org/wiki/Generalization_error):

&gt; The generalization error is the difference between the expected and empirical error. This is the difference between error on the training set and error on the underlying joint probability distribution.

This means that in underfitting regime the generalization error should be low. I think the authors of that book are using a non-standard definition of generalization error. Do you agree?",false,4zu08d,,0,,false,1473084654,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zu08d/definition_of_generalization_error/,t3_4zu08d,,false,,
1471528875,MachineLearning,munchler,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ycez5/qlearning_with_neural_networks/,6,2,2,0,Q-Learning with Neural Networks,"I'm trying to implement Q-Learning with an artificial neural network approximating the Q(S,A) function. I've found two different articles that say that two distinct forward passes through the neural net are required on each iteration:

* https://www.nervanasys.com/demystifying-deep-reinforcement-learning/

* http://outlace.com/Reinforcement-Learning-Part-3/

This doesn't seem right to me, since it doesn't seem to conform to the basic Q-Learning algorithm, which requires only a single evaluation of Q(S,A) on each iteration. Am I missing something?
",false,4ycez5,,0,,false,1473057373,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ycez5/qlearning_with_neural_networks/,t3_4ycez5,,false,,
1472397390,MachineLearning,decoder007,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zzsbq/trained_lstmrnn_on_mahabharata_longest_poem_and/,16,10,10,0,"Trained LSTM|RNN on Mahabharata (longest poem) and it gave me this, awesome kid !","ततो स्यामास्तु ते सर्वे सहादीः क्रप्य वापि मम |
महीपदे महात्मानः किञ्चित्तं वृष्णीन्नरप्रनां पाण्डवः ||१५||
स्माना भवद्य तद्चिन्ती प्राप्ता नरभिरुजिरः |
सानुचर्भ्यं च पुत्रान्ता स्वर्तं तत्र ||२०||

* Trained on : http://sanskritdocuments.org/mirrors/mahabharata/unic/mbh18_sa.html (~140 k characters) 
* Keras|Theano implementation of RNN's. Find a detailed explanation on Karpathy's [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). 
* Understood the no. of characters/word, no. of word/line patterns.
* Got the relative position of terminal characters like visarga (colon). The terminal, beginning, overall letter frequencies matches with the complete training set. [Zipf's frequencies are preserved].
* Got the idea that there are two lines per verse. 
* Also got the ""labelling"" after each verse. 
* Many words have proper meanings. 
* Sentences do not mean anything as of now. 

",false,4zzsbq,,0,,false,1473087650,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zzsbq/trained_lstmrnn_on_mahabharata_longest_poem_and/,t3_4zzsbq,,false,,
1472493970,MachineLearning,Zeekawla99ii,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/506diy/what_exactly_is_a_tpu/,7,10,10,0,What exactly is a TPU?,"https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html

From my understanding, this is simply a marketing name for a device which is physically a GPU but tailored for TensorFlow. 

Is that correct?",false,506diy,,0,,false,1473091152,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/506diy/what_exactly_is_a_tpu/,t3_506diy,,false,,
1470593195,MachineLearning,feedthecreed,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wlx5k/survey_the_verdict_on_layer_normalization/,19,19,19,0,"Survey, the verdict on layer normalization?","It's been well over 2 weeks since the layer normalization paper came out (https://arxiv.org/pdf/1607.06450v1.pdf), surely we have results by now ;)  

Has anyone seen any drastic gains over batch normalization? 

I haven't seen any drastic improvements for my supervised learning tasks, but I also haven't seen that much improvement with batch normalization either.",false,4wlx5k,,0,,false,1473025555,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4wlx5k/survey_the_verdict_on_layer_normalization/,t3_4wlx5k,,false,Discusssion,
1471118984,MachineLearning,gabrielgoh,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xkri5/what_is_magic_about_the_gram_matrix_matching/,5,8,8,0,what is magic about the gram matrix matching which makes artistic style transfer work?,"I know it's sensible because it discards spatial information about the image, but it seems like there's something special about the gram matrix which captures a picture's artistic fingerprint. Does this trick apply in more general settings?",false,4xkri5,,0,,false,1473043344,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xkri5/what_is_magic_about_the_gram_matrix_matching/,t3_4xkri5,,false,,
1471893047,MachineLearning,nharada,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z27j2/convnets_on_audio_or_other_1d_signals/,1,0,0,0,ConvNets on audio or other 1d signals,"ConvNets have found some success in 1D, such as text classification. However, I don't see many reports of people successfully using them on raw audio or other waveform data. Generally it seems people convert to spectrogram or mel-frequency cepstrum first, and then use a regular 2D architecture. **Has anyone seen ConvNets used for raw 1D data like audio?** I suspect maybe at high sampling rates the noise over short time scales is too much for the ConvNet to deal with, but it's possible I've missed something.",false,4z27j2,,0,,false,1473070468,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z27j2/convnets_on_audio_or_other_1d_signals/,t3_4z27j2,,false,,
1470431843,MachineLearning,istareatscreens,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wcpfp/trying_to_find_the_name_of_this_book/,2,0,0,0,Trying to find the name of this book,"Hi,

The wikipedia page on Backpropagation has a link to a book that I am really keen to find the name of.

The link is the one just above the title 'Modes of learning' on the wikipedia page, labelled' ""The Back Propogation Algorithm  Page 20""

the link is to: https://web.archive.org/web/20150317210621/https://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf

I would really appreciate it if someone could tell me the name of this book/publication as I found it very easy to read and understand and would like to read the rest of it.

",false,4wcpfp,,0,,false,1473020821,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wcpfp/trying_to_find_the_name_of_this_book/,t3_4wcpfp,,false,,
1470204778,MachineLearning,italartworld001,denonpu.com,http://www.denonpu.com/Products/PU_machines/Low_Pressure_Series/2014/0323/28.html,0,1,1,0,"The polyurethane foam insulation spray is used to apply to buildings in order to have great insulation against air, noise, mold and moss.",,false,4vx37k,,0,,false,1473012718,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vx37k/the_polyurethane_foam_insulation_spray_is_used_to/,t3_4vx37k,,false,,
1471515905,MachineLearning,greymatter-analytics,arc.applause.com,https://arc.applause.com/2016/08/17/gartner-hype-cycle-2016-machine-learning/#.V7VBgKS_mUg.google_plusone_share,0,1,1,0,Machine Learning at the height of the S curve?,,false,4ybilz,,0,,false,1473056922,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ybilz/machine_learning_at_the_height_of_the_s_curve/,t3_4ybilz,,false,,
1470830304,MachineLearning,marcjschmidt,youtube.com,https://www.youtube.com/watch?v=qS8qhzXRQWE,20,58,58,0,"New free deep learning platform now available, with Keras model designer // AETROS",,false,4x1rma,,0,,false,1473033623,false,http://b.thumbs.redditmedia.com/2WcnAXB9pg3G0IceGzmz-gjityr1XDy3SD3YEu8yL0s.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x1rma/new_free_deep_learning_platform_now_available/,t3_4x1rma,,false,,
1471799974,MachineLearning,superfunny,economist.com,http://www.economist.com/news/finance-and-economics/21705329-governments-have-much-gain-applying-algorithms-public-policy,0,1,1,0,Machine Learning and Public Policy,,false,4yvxey,,0,,false,1473067284,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yvxey/machine_learning_and_public_policy/,t3_4yvxey,,false,,
1471284852,MachineLearning,short_vix,jmlr.org,http://jmlr.org/papers/volume17/14-540/14-540.pdf,1,4,4,0,Bayesian Leave-One-Out Cross-Validation Approximations for Gaussian Latent Variable Models,,false,4xuyfe,,0,,false,1473048528,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4xuyfe/bayesian_leaveoneout_crossvalidation/,t3_4xuyfe,,false,Research,
1472047279,MachineLearning,brotherrain,videolectures.net,http://videolectures.net/deeplearning2016_montreal/,13,180,180,0,"Deep Learning Summer School, Montreal 2016 - VideoLectures",,false,4zcpg3,,0,,false,1473075804,false,http://b.thumbs.redditmedia.com/3VdKpRO41-1FjYvL5vEA3FGmT9KKvWt7Z4bUEMunVKk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zcpg3/deep_learning_summer_school_montreal_2016/,t3_4zcpg3,,false,,
1470070652,MachineLearning,BadGoyWithAGun,aeon.co,https://aeon.co/essays/your-brain-does-not-process-information-and-it-is-not-a-computer,7,0,0,0,Your brain does not process information and it is not a computer – Robert Epstein | Aeon Essays,,false,4vnhyc,,0,,false,1473007713,false,http://b.thumbs.redditmedia.com/K7m0XEJqqzn61f-uLoOK-WkxKKmDC18zRbiLXy_RVxU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vnhyc/your_brain_does_not_process_information_and_it_is/,t3_4vnhyc,,false,,
1471025987,MachineLearning,julian88888888,blog.fastforwardlabs.com,http://blog.fastforwardlabs.com/post/148842796218/introducing-variational-autoencoders-in-prose-and,2,19,19,0,Introducing Variational Autoencoders (in Prose and Code),,false,4xezb0,,0,,false,1473040407,false,http://a.thumbs.redditmedia.com/vkDkEqk5cG5wquHvev4YGSsDDD2ZRUNesoZhAj0lN24.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xezb0/introducing_variational_autoencoders_in_prose_and/,t3_4xezb0,,false,,
1472581651,MachineLearning,alxndrkalinin,research.googleblog.com,https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html,0,2,2,0,TF-Slim: A high level library to define complex models in TensorFlow | Google Research Blog,,false,50cmjl,,0,,false,1473094384,false,http://b.thumbs.redditmedia.com/o_TQdhqAU2NP6kaPaMWwi3AhIu8RM9LSTNP95-2M0lE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50cmjl/tfslim_a_high_level_library_to_define_complex/,t3_50cmjl,,false,,
1471979892,MachineLearning,AreYouSherlocked,youtube.com,https://www.youtube.com/watch?v=2Yjx4SP5woU,1,0,0,0,A tribute to Alan Turing,,false,4z88ap,,0,,false,1473073528,false,http://b.thumbs.redditmedia.com/1pijDNpNTIrvpWZZ9H1ZcNDE78eQxnCQrJgOs-Kt2iE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z88ap/a_tribute_to_alan_turing/,t3_4z88ap,,false,,
1470317398,MachineLearning,[deleted],research.comma.ai,http://research.comma.ai/,0,1,1,0,comma.ai Driving Dataset,[deleted],false,4w4i3y,,0,,false,1473016614,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w4i3y/commaai_driving_dataset/,t3_4w4i3y,,false,,
1470176976,MachineLearning,[deleted],engineering.tumblr.com,https://engineering.tumblr.com/post/148350944656/categorizing-posts-on-tumblr,1,1,1,0,Categorizing Posts on Tumblr,[deleted],false,4vv8ss,,0,,false,1473011766,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vv8ss/categorizing_posts_on_tumblr/,t3_4vv8ss,,false,,
1471753746,MachineLearning,__jamworld__,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yt4lz/does_anyone_know_about_the_airbnb_dataset_for_in/,0,1,1,0,Does anyone know about the Airbnb Dataset for in door navigation task?,[removed],false,4yt4lz,,0,,false,1473065859,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yt4lz/does_anyone_know_about_the_airbnb_dataset_for_in/,t3_4yt4lz,,false,,
1472002102,MachineLearning,axtyax,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4za07o/how_do_i_calculate_the_node_delta_for_weights_in/,2,2,2,0,How do I calculate the node delta for weights in a convolution matrix (In a convolution neural network)?,"I have just finished programming a convolution neural network, and I am trying to use this tutorial(https://www.youtube.com/watch?v=p1-FiWjThs8&amp;list=PLQp3IHfXy1m_ofJlWBl_hjpvvS4Bh9_EY&amp;index=6) to figure out how to calculate the gradients of the weights in my net. I still don't understand how to calculate the node delta of the weights in my convolution kernels, because I'm not sure if there is a clear activation function to derive. As shown in the tutorial, the node delta is calculated with the derivative of an activation function (the guy in the tutorial is using a variation of a sigmoid function), but I'm not sure what the activation function(s) for my convolution kernel values is/are. I do use ReLU after each convolution, but I feel like that isn't all that I need to calculate the node delta. Since the values of each kernel are applied to the same input, will every value in a kernel have the same node delta? Please tell me if I'm missing something here- I feel like I am.

Any help would be greatly appreciated!!!",false,4za07o,,0,,false,1473074427,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4za07o/how_do_i_calculate_the_node_delta_for_weights_in/,t3_4za07o,,false,,
1472359252,MachineLearning,TheMatrixShibe,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zxz5b/amd_alternative_to_nvblas/,3,0,0,0,AMD alternative to NVBLAS?,"NVBLAS is a drop in BLAS replacement which uses your (NVidia) GPU to speed up matrix multiplication.

I was wondering if there was an AMD-supported alternative since I do not have a NVidia GPU.",false,4zxz5b,,0,,false,1473086699,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zxz5b/amd_alternative_to_nvblas/,t3_4zxz5b,,false,,
1470308684,MachineLearning,chain20,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w3zw4/implementing_timeseries_acf_from_scratch/,2,1,1,0,Implementing Timeseries ACF from scratch,"Hi,

As part of a learning exercise, I am trying to implement a routine to compute the Autocorrelation function for time series and I am comparing the result with the statsmodels.tsa.stattools.acf function. I get very similar results from my own code and the statsmodels function, except for 1 case. 

The data looks like this:
[Original Data](http://imgur.com/p6HqmHn)

I generate the autocorrelation function using something like this:

    correlations = [1.]
    for l in range(1,50):
        x = data[:-l]
        y = data[l:]
        corr,conf = pearsonr(x,y)
        correlations.append(corr)
    plt.plot(correlations)

and the generated 50-lag ACF looks like this (ranges from 1 - 0.988):    [ACF ](http://imgur.com/vYgf6d8)



and the ACF produced by statsmodels package is completely different (ranging from 1.0-0.3) [statsmodels ACF] (http://imgur.com/Qw5SWaF)

This is the only datasets I have out of many where the two ACFs are different. Is there any specific feature of this data that cause my code to produce a different ACF?",false,4w3zw4,,0,,false,1473016352,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w3zw4/implementing_timeseries_acf_from_scratch/,t3_4w3zw4,,false,,
1472208695,MachineLearning,Arech,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4znzvo/what_are_the_advantages_of_relu_over_the/,14,8,8,0,What are the advantages of ReLU over the LeakyReLU (in FFNN)?,"Hello everyone.

I was experimenting with ReLU and LeakyReLU for some time in feedforward neural networks and for me it looks like ReLU has no advantages over the LeakyReLU (besides being just very slightly faster to compute). In my experience, LeakyReLU shows at least the same or better results in most comparisons with ReLU, but moreover, it allows NN to learn in setups (architectures) where the ReLU fails. For example, it's the case where a NN architecture contains ""bottlenecks"" - very narrow layers with small neurons count. Many ReLU neurons in such bottlenecks can be and remain ""locked"" during learning which prevents gradient propagation and therefore NN can't learn to represent even a training dataset. LeakyReLU in the same scenario still propagates some gradient down the stack effectively allowing NN to learn.

So, I'm curious, did anyone investigate the question put in the subject line and what results did he or she get?

Thanks.",false,4znzvo,,0,,false,1473081567,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4znzvo/what_are_the_advantages_of_relu_over_the/,t3_4znzvo,,false,,
1470331284,MachineLearning,datasciguy-aaay,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w5ngy/in_a_multigpu_workstation_can_some_gpu_be_turned/,3,0,0,0,In a multi-GPU workstation can some GPU be turned off?,"Is there some way to turn off 3 of 4 GPU when not doing deep learning runs on a workstation?  They make too much heat in my office.  Maybe BIOS in modern computer can hide them but I don't want giant fuss from Ubuntu getting confused when I do that. 

I could make a server host separate from workstation, but I'd like to enjoy one of the GPU for games from time to time too, so I want a workstation not a GPU server host.

All new hardware ca. 2016 will be used for most modern BIOS, OS version, GPU hardware.

If you have actual experience, I would like to hear.  (SPeculators can keep quiet if you please. I can speculate plenty on my own. Thanks for your prudent silence.)",false,4w5ngy,,0,,false,1473017209,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w5ngy/in_a_multigpu_workstation_can_some_gpu_be_turned/,t3_4w5ngy,,false,,
1471895865,MachineLearning,pol4ko,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z2g39/question_how_can_i_use_unsupervised_learning_to/,2,0,0,0,Question: How can I use unsupervised learning to recommend most similar features?,"Hi fellows. If you don't mind, I would like to ask for your conceals. I'm new in this area.

In my master's degree project I would like to use Machine Learning to recommend software processes. Basically, I have N different software projects of which I extracted N different processes. Given a input (characteristics/features), I thinking in use unsupervised learning to show the **most similar** processes.

| Projects | featA | featB | ... |
|:---------|:------|:------|:----|
| proj1    | aaa   | bbb   | ... |
| proj2    | caa   | baa   | ... |
| ...      | ...   | ...   | ... |


* **input:** _aab | bbc | ..._
* **answer:** _proj1_

So:

1. Is it even possible?
2. Can you guys, please, give me some hints?

Thanks in advance.",false,4z2g39,,0,,false,1473070590,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z2g39/question_how_can_i_use_unsupervised_learning_to/,t3_4z2g39,,false,,
1470607353,MachineLearning,Akovov,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wmymq/made_a_visualization_for_confusion_matrices_what/,10,15,15,0,"Made a visualization for confusion matrices, what do you think?","I recently wrote up an article on medium about my miniature project, where I did machine learning on player deaths in Eve online. In that project I made a visualization for confusion matrices, and I haven't seen anything like it before. Do you think it's any good? Comments on the rest of my work are appreciated too. 

https://medium.com/vladprojects/mapping-destruction-in-eve-online-cc88827cf705",false,4wmymq,,0,,false,1473026091,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wmymq/made_a_visualization_for_confusion_matrices_what/,t3_4wmymq,,false,,
1470622556,MachineLearning,mainguyenmth,youtube.com,https://www.youtube.com/attribution_link?a=b8eJlsgI3q4&amp;u=%2Fwatch%3Fv%3DnfsbVudl06w%26feature%3Dshare,2,0,0,0,Máy chiết rót kem mỹ phẩm giá rẻ,,false,4wnzdo,,0,,false,1473026616,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wnzdo/máy_chiết_rót_kem_mỹ_phẩm_giá_rẻ/,t3_4wnzdo,,false,,
1472434555,MachineLearning,vvpreetham,medium.com,https://medium.com/@vvpreetham/committee-of-intelligent-machines-unity-in-diversity-of-neuralnetworks-8a6c494f089c#.rx1ftpncg,1,1,1,0,Committee of Intelligent Machines — Unity in Diversity of #NeuralNetworks - Fundamentals.,,false,502lec,,0,,false,1473089119,false,http://b.thumbs.redditmedia.com/aiNZ9ivSNKdaRdm6SE0mD1w3gjrvp9Qd788U2TY8ITE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/502lec/committee_of_intelligent_machines_unity_in/,t3_502lec,,false,,
1471332788,MachineLearning,alexjc,arxiv.org,http://arxiv.org/abs/1608.04337,9,34,34,0,[1608.04337] Factorized Convolutional Neural Networks,,false,4xybfv,,0,,false,1473050240,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4xybfv/160804337_factorized_convolutional_neural_networks/,t3_4xybfv,,false,Research,
1472540230,MachineLearning,italartworld001,hunanpipe.com,http://www.hunanpipe.com/productshow91_254.html,0,1,1,0,"Hunan Standard Steel Pipe Co.,Ltd as the professional manufacturer of steel pipe and pipe fittings, The company is located in an industrial zone in Xinsha town,Changsha city ,Standard Steel will ,taking the scientific developments view as guideline in its overall reformation and develepment.....",,false,509tq8,,0,,false,1473092923,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/509tq8/hunan_standard_steel_pipe_coltd_as_the/,t3_509tq8,,false,,
1472037342,MachineLearning,bbsome,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zc1o3/survey_any_cool_startups_you_have_recently_viewed/,4,0,0,0,"Survey: Any cool startups you have recently viewed, which do ML? Any bad ones as well?",,false,4zc1o3,,0,,false,1473075471,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zc1o3/survey_any_cool_startups_you_have_recently_viewed/,t3_4zc1o3,,false,,
1471103278,MachineLearning,michaelkepler,raduangelescu.com,http://www.raduangelescu.com/artisticmachine42.html,0,2,2,0,Artistic machine 42: neural network style transfer art with matching quotes,,false,4xjljd,,0,,false,1473042753,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xjljd/artistic_machine_42_neural_network_style_transfer/,t3_4xjljd,,false,,
1470173387,MachineLearning,amplifier_khan,gab41.lab41.org,https://gab41.lab41.org/lab41-reading-group-deep-networks-with-stochastic-depth-564321956729#.uyi262ycz,0,2,2,0,Lab41 Reading Group: Deep Networks with Stochastic Depth,,false,4vuyjz,,0,,false,1473011621,false,http://b.thumbs.redditmedia.com/II0CBWD3h3NFuD7cp-DINfUHlB1vJ-CMj3ogE9ExMfs.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vuyjz/lab41_reading_group_deep_networks_with_stochastic/,t3_4vuyjz,,false,,
1472246383,MachineLearning,oliverfromudacity,youtube.com,https://www.youtube.com/watch?v=C2c8K-qp_xo,2,13,13,0,Sebastian Thrun Self-Driving Car Talk Livestream,,false,4zqybi,,0,,false,1473083087,false,http://b.thumbs.redditmedia.com/d5OASq_wa9aggzO1yvcV2tqB9Ec6Iyq9zTV4U9A2QKo.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zqybi/sebastian_thrun_selfdriving_car_talk_livestream/,t3_4zqybi,,false,,
1471045756,MachineLearning,jdwittenauer,johnwittenauer.net,http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/,38,505,505,0,All of Andrew Ng's machine learning class in Python,,false,4xgkoa,,0,,false,1473041217,false,http://b.thumbs.redditmedia.com/YzDetJHiutWiCF6jlLMXdAonVlT5vz0YwOOqsmY31sQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xgkoa/all_of_andrew_ngs_machine_learning_class_in_python/,t3_4xgkoa,,false,,
1471309229,MachineLearning,[deleted],fortune.com,http://fortune.com/2016/08/15/elon-musk-artificial-intelligence-openai-nvidia-supercomputer/,1,0,0,0,Nvidia Just Gave A Supercomputer to Elon Musk-backed OpenAI,[deleted],false,4xwvny,,0,,false,1473049509,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xwvny/nvidia_just_gave_a_supercomputer_to_elon/,t3_4xwvny,,false,,
1470672213,MachineLearning,[deleted],research.googleblog.com,https://research.googleblog.com/2016/08/meet-parseys-cousins-syntax-for-40.html,0,1,1,0,"Meet Parseyâs Cousins: Syntax for 40 languages, plus new SyntaxNet capabilities",[deleted],false,4wqv3k,,0,,false,1473028083,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wqv3k/meet_parseyâs_cousins_syntax_for_40_languages/,t3_4wqv3k,,false,,
1471965197,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z6y4v/stochastic_networks_as_ensemble_learning/,3,1,1,0,Stochastic Networks as ensemble learning,[deleted],false,4z6y4v,,0,,false,1473072874,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z6y4v/stochastic_networks_as_ensemble_learning/,t3_4z6y4v,,false,,
1470403681,MachineLearning,chaz_it_up,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wadne/alternatives_to_grus/,0,1,1,0,Alternatives to GRUs?,[removed],false,4wadne,,0,,false,1473019624,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wadne/alternatives_to_grus/,t3_4wadne,,false,,
1472441694,MachineLearning,jeremylin_reddit,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50330c/tensorflows_high_level_api_slim_now_support_the/,15,24,24,0,tensorflow's high level api (slim) now support the following Networks,"Now, Slim has already supported the following Networks：
1、AlexNet；
2、Inception v1 v2 v3；
3、Overfeat；
4、VGG 16 VGG 19；
5、ResNet v1 v2；

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim",false,50330c,,0,,false,1473089375,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50330c/tensorflows_high_level_api_slim_now_support_the/,t3_50330c,,false,,
1470977267,MachineLearning,huyhcmut,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xbw5n/gaussian_process/,4,7,7,0,Gaussian process,"I am reading some papers about gaussian process and i wonder why many Machine learning group in England doing a lot researchs on this topic ( Cambridge with Rasmussen, Ghahramani, Mackay(in memory); Oxford; Sheffield with Neil D.Lawrence; and Edingburg; Imperial college; UCL). A noob question: Which is the main reason?",false,4xbw5n,,0,,false,1473038821,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xbw5n/gaussian_process/,t3_4xbw5n,,false,,
1471417895,MachineLearning,clemento341,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y49px/internship_as_an_undergrad/,20,3,3,0,Internship as an undergrad,"Maybe I haven't done enough research, but I got the impression that 99% of the companies are only interested in people who have a master/phd in stats/cs/ai for internships. Am I wrong about this? I'm asking because I'm an undergrad and I don't have much experience in ml besides a couple classes. Thanks.",false,4y49px,,0,,false,1473053252,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y49px/internship_as_an_undergrad/,t3_4y49px,,false,,
1471603782,MachineLearning,AlNejati,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yi8kl/cool_examples_of_ml_startups_getting_around_data/,3,5,5,0,Cool examples of ML startups getting around data problem?,"Right now a major issue in doing deep learning is the amount of data that is usually necessary for training. While a lot of problems can use pre-trained models, for a lot of other stuff you need to train from scratch. Hence, for example, what DeepMind (then a small company) did with their Atari RL system - using a simulated game environment where you can generate as much data as you want. Do you guys know of any other examples of companies using nifty tricks to get around the data problem?",false,4yi8kl,,0,,false,1473060308,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yi8kl/cool_examples_of_ml_startups_getting_around_data/,t3_4yi8kl,,false,,
1470750895,MachineLearning,nex_jeb,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wwack/how_to_perform_dirty_floor_detection_using/,5,0,0,0,How to perform Dirty floor detection using Machine Learning ?,"Are there any papers addressing the task of detecting dirty floors ?

From what I have learned from the Machine Learning space so far, I was thinking about segmenting the floor first using Deep Pixel-wise semantic segmentation, then using the original corresponding area as an input to a CNN trained on Dirty / Non Dirty binary classified Data.",false,4wwack,,0,,false,1473030849,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wwack/how_to_perform_dirty_floor_detection_using/,t3_4wwack,,false,,
1470846830,MachineLearning,__bee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x30g0/question_which_cloud_services_hosting_plans_do/,8,3,3,0,[Question] Which Cloud services (Hosting plans) do you use to train deep learning models,"Hey!

We are working on training some DL models and we need to go for one of the hosting plans as our servers can't handle it.
Based on your experience, what's the estimated budget, What do you recommend  ? 

Our main work is around using Caffe, so we need GPU. ( Note: it's an academic project for our class )",false,4x30g0,,0,,false,1473034261,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x30g0/question_which_cloud_services_hosting_plans_do/,t3_4x30g0,,false,,
1471448242,MachineLearning,JfnS,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y69um/dota_2_data_analysis_help_wanted/,9,16,16,0,Dota 2 - Data analysis - Help wanted,"Hello everyone!

I'm one of the founders of eBlur, the company responsible for creating Feedless, an AI Dota coach that talks to players in real time advising them about a bunch of different things. You can learn more about it on [our website](http://www.eblur.co.uk) and you can even download it and try it out for yourself.

We have a lot of features that we want to add to Feedless and some of the most ambitious ones require quite a bit of machine learning and data processing.

We have a system to download Dota match data at will, meaning we can put together a huge dataset of complete Dota matches.

We are looking for someone with machine learning skills to help us analysing the data

If you're skilled in machine learning and want to help developing an exciting new product with a small, agile and skilled team of engineers, just pm me so we can discuss it further.

Also, being a Dota player is obviously a plus! You don't have to be a pro but some insight of the game will go along way into figuring out what to extract from the data provided.

[EDIT] Huge thanks to everyone that pm'd me! I never thought we would have such a strong response from you.",false,4y69um,,0,,false,1473054259,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y69um/dota_2_data_analysis_help_wanted/,t3_4y69um,,false,,
1471968149,MachineLearning,Inori,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z77aq/psa_if_you_have_trouble_following_david_silvers/,9,8,8,0,"PSA: If you have trouble following David Silver's RL lectures, try Berkley's CS188","[Berkley's CS188 Intro to AI](http://ai.berkeley.edu/lecture_videos.html) lectures 8-11 cover the same material as David Silver's lectures 1-6, but in a somewhat friendlier manner.

As a bonus, you can try registering to the old [edx online course](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/), which gives you a great toy project at the end of the lectures block.",false,4z77aq,,0,,false,1473073004,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z77aq/psa_if_you_have_trouble_following_david_silvers/,t3_4z77aq,,false,,
1471364454,MachineLearning,datasciguy-aaay,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y0c9r/whats_the_best_common_size_to_scale_differentsize/,3,1,1,0,"What's the best common size to scale different-size images to: Rescale all images to match size of smallest, largest, or the mean size?","There are hundreds of thousands of different sized images in my dataset. I'm thinking the loss will be the least if I rescale them to the mean size, either up or down as needed.  What have you found works best?  I am thinking that if I took the largest or smallest, then the interpolation or extrapolation error introduced into the rescaled images will be the worst possible, hence my expectation of the middle size being the least error prone.",false,4y0c9r,,0,,false,1473051264,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y0c9r/whats_the_best_common_size_to_scale_differentsize/,t3_4y0c9r,,false,,
1470254430,MachineLearning,scionaura,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w0i88/shakeweight_a_simple_method_for_regularising/,6,11,11,0,ShakeWeight - a simple method for regularising neural networks,"Does anyone have the arxiv link for ShakeWeight? In case you missed it - it's a regularization technique for neural nets where each  weight oscillates back and forth between a min and max value (around the parameter estimate). At test time the oscillation speed is increased, proportional to a hyperparameter.

IIRC performance was competitive with dropout, but I forget the details. If someone has the link it would be much appreciated - pretty sure it came out of one of the major labs.",false,4w0i88,,0,,false,1473014553,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w0i88/shakeweight_a_simple_method_for_regularising/,t3_4w0i88,,false,,
1472169082,MachineLearning,thewhizz,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zlnkh/training_networks_with_tensorflow/,0,0,0,0,Training networks with Tensorflow,"Ok /r/MachineLearning, I've exhausted my resources and I'm looking for assistance. I'm attempting to train a VGG style network and I'm having issues with layers dropping to 0. I collect summary stats for each layer -- conv, pool, affine -- and monitor stats while training. Layer weights drop to 0 within the first ~200 mini batches. If you have any tips or suggestions, I'd really like to hear it. The link below contains code snippets and a more complete description of the problem. I haven't come across any other resources on the internet that address this issue. 

http://stackoverflow.com/questions/39070022/convnet-layers-not-showing-activity-and-dropping-to-zero-after-a-few-minibatches",false,4zlnkh,,0,,false,1473080373,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zlnkh/training_networks_with_tensorflow/,t3_4zlnkh,,false,,
1470183383,MachineLearning,amplifier_khan,medium.com,https://medium.com/@_marcos_otero/the-real-10-algorithms-that-dominate-our-world-e95fa9f16c04#.67agog79m,4,6,6,0,The real 10 algorithms that dominate our world,,false,4vvqdx,,0,,false,1473012019,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vvqdx/the_real_10_algorithms_that_dominate_our_world/,t3_4vvqdx,,false,,
1470565157,MachineLearning,perceptron01,siliconangle.com,http://siliconangle.com/blog/2016/08/05/watson-correctly-diagnoses-woman-after-doctors-were-stumped/,55,112,112,0,Watson correctly diagnoses woman after doctors were stumped,,false,4wkc7q,,0,,false,1473024748,false,http://b.thumbs.redditmedia.com/btQZfeIOXBvjdeW96Fsu8z9X25S96lTrnrZwyl7Kxlk.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wkc7q/watson_correctly_diagnoses_woman_after_doctors/,t3_4wkc7q,,false,,
1470660162,MachineLearning,manishmshiva,meetup.com,https://www.meetup.com/ML-DLC/events/233009890/,0,0,0,0,An introduction to working with Scikit,,false,4wpyd5,,0,,false,1473027618,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wpyd5/an_introduction_to_working_with_scikit/,t3_4wpyd5,,false,,
1472128279,MachineLearning,jainadi341,blog.paralleldots.com,http://blog.paralleldots.com/technology/mathematician-behind-data-scientist/,0,0,0,0,The Mathematician behind Data Scientist,,false,4zi8ik,,0,,false,1473078627,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zi8ik/the_mathematician_behind_data_scientist/,t3_4zi8ik,,false,,
1472550298,MachineLearning,italartworld001,hunapipe.blogspot.in,http://hunapipe.blogspot.in/2016/08/merits-of-seamless-steel-pipes-that.html,0,1,1,0,"Seamless Carbon Steel Pipes and fittings are made up of metal alloys which are results of carbon and iron blend. These perform well in harsh conditions, are robust and reliable choice for construction.",,false,50ab0l,,0,,false,1473093165,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50ab0l/seamless_carbon_steel_pipes_and_fittings_are_made/,t3_50ab0l,,false,,
1471477236,MachineLearning,antiprior,arxiv.org,http://arxiv.org/abs/1608.04428,0,30,30,0,[1608.04428] TerpreT: A Probabilistic Programming Language for Program Induction,,false,4y8yog,,0,,false,1473055626,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4y8yog/160804428_terpret_a_probabilistic_programming/,t3_4y8yog,,false,Research,
1470266125,MachineLearning,amitjyothie,medium.com,https://medium.com/@Mybridge/top-ten-machine-learning-articles-for-the-past-month-9c1202351144#.yf5hxh4gi,5,168,168,0,Top 10 machine learning articles from July,,false,4w1i47,,0,,false,1473015075,false,http://b.thumbs.redditmedia.com/7GOWYhta4DQxn47Fnsk4Tr23kqV3a013YEX9bOxBrus.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w1i47/top_10_machine_learning_articles_from_july/,t3_4w1i47,,false,,
1472157090,MachineLearning,[deleted],ics.uci.edu,http://www.ics.uci.edu/~pjsadows/papers/LocalLearning2016.pdf,0,1,1,0,Local Learning Rules for Neural Networks (In Press: Neural Networks 2016),[deleted],false,4zkony,,0,,false,1473079876,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zkony/local_learning_rules_for_neural_networks_in_press/,t3_4zkony,,false,,
1471564775,MachineLearning,[deleted],youtube.com,https://www.youtube.com/attribution_link?a=D3SwSfvCPZ4&amp;u=%2Fwatch%3Fv%3DeB4oSxyMCIE%26feature%3Dshare,0,1,1,0,Machine Learning Video streaming style transfer,[deleted],false,4yfobb,,0,,false,1473059016,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yfobb/machine_learning_video_streaming_style_transfer/,t3_4yfobb,,false,,
1470648559,MachineLearning,startupAlice,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wpb55/list_of_startups_in_machinelearning_robotics_and/,0,1,1,0,"List of Startups in Machinelearning, Robotics and Artificial Intelligence",[removed],false,4wpb55,,0,,false,1473027294,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wpb55/list_of_startups_in_machinelearning_robotics_and/,t3_4wpb55,,false,,
1472485801,MachineLearning,n00bzor,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/505nso/active_learning_and_images/,1,1,1,0,Active Learning and Images,I started reading about active learning and it seems like a cool concept. I'm wondering if anyone here has built any active learning for image annotation (not just classification).,false,505nso,,0,,false,1473090760,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/505nso/active_learning_and_images/,t3_505nso,,false,,
1472214741,MachineLearning,carlthome,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zocnn/question_for_scala_is_mxnet_or_dl4j_more_mature/,8,9,9,0,"[Question] For Scala, is MXNet or DL4J more mature?","Even though TensorFlow is working great I'm really sick of Python. I would love to move to Scala, but I hesitate due to a lack of mature deep learning frameworks.

DL4J and MXNet seem like the best (only?) candidates, so I'm curious if anyone has any insight into the pros/cons of the two (particularly for regression type problems with RNNs). Which should I invest in?

Last I looked at DL4J the GPU support was broken (half a year ago), and MXNet is documented so-and-so (especially compared to TensorFlow) but looks very competent, and considering XGBoost's success I'm hyped about MXNet.",false,4zocnn,,0,,false,1473081749,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zocnn/question_for_scala_is_mxnet_or_dl4j_more_mature/,t3_4zocnn,,false,,
1471956029,MachineLearning,gautamrbharadwaj,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z6908/audio_processing_using_machine_learning/,2,1,1,0,Audio processing using machine learning,"I want to know how to process a large .wav voice signal to remove noise, please help me with this and the file size is 100MB",false,4z6908,,0,,false,1473072520,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z6908/audio_processing_using_machine_learning/,t3_4z6908,,false,,
1472224200,MachineLearning,TashanValiant,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zp2f5/simpleresources_for_tempering_expectations_of_my/,6,9,9,0,SimpleResources For Tempering Expectations of my Team Lead,"It looks like I will be put on a new team that involves some aspects of bringing Machine Learning into my company. Personally, I think this is a great opportunity for me to learn and actually use my MS in Mathematics. However, I fear that the team lead has way too many grand expectations or misunderstandings about Machine Learning, namely what it can do.

I've talked to him in the past about Machine Learning and he seems to have these ideas of throwing everything at it or parsing out specific parts of the data and then throwing that in a bucket with some magic and getting answers.

From my understanding (and small amount of experience) we take the data, normalize it, cut it up into pieces to build a model, train the model, and most importantly have a clear, direct, and simple question to ask of our data. 

Are there any simple and insightful resources out there for showing the extreme basics of machine learning? I've tried looking but everything I've seen jumps right into frameworks and tools. 

For instance, I remember an article that was parsing user feedback (text and a score they picked) and trying to predict the score based of text to the feedback. It went through parsing the data, normalizing, creating a bag of words, a model, training the model, and then further methods. I can't find the article now (of course) but it illustrated the ""simple questions"" aspect I am trying to drive home.

Any help would be greatly appreciated. 
",false,4zp2f5,,0,,false,1473082118,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zp2f5/simpleresources_for_tempering_expectations_of_my/,t3_4zp2f5,,false,,
1472254782,MachineLearning,FuzziCat,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zrl5i/what_are_the_main_differences_between_energy/,1,0,0,0,"What are the main differences between ""energy based"" and ""cognitive based"" approaches to machine learning?","It seems to me that they use the same few basic equations, such as Bayes probability, softmax, negative log-likelihood, cross entropy or KL distance, but maybe with different jargon.  

What am I missing about the difference between the two approaches? ",false,4zrl5i,,0,,false,1473083413,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zrl5i/what_are_the_main_differences_between_energy/,t3_4zrl5i,,false,,
1470805500,MachineLearning,zerogravity555,stackoverflow.com,http://stackoverflow.com/questions/38824379/cannot-reshape-input-shape-in-tensorflow,3,0,0,0,How should I reshape the input tensor?,,false,4x0k53,,0,,false,1473033017,false,http://a.thumbs.redditmedia.com/qTN_mhJO0jAOM9DvK7I_s97kxvKYPdZjYtaKfj7OJb4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0k53/how_should_i_reshape_the_input_tensor/,t3_4x0k53,,false,,
1471157456,MachineLearning,tyrael71,blog.jupyter.org,http://blog.jupyter.org/2016/07/14/jupyter-lab-alpha/,23,186,186,0,JupyterLab: the next generation of the Jupyter Notebook,,false,4xn4s1,,0,,false,1473044554,false,http://b.thumbs.redditmedia.com/R0tE3dWGcJDxwBrTH80AqeKaj4Fu3lRM5tf4AXeEGkI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xn4s1/jupyterlab_the_next_generation_of_the_jupyter/,t3_4xn4s1,,false,,
1470469812,MachineLearning,dendisuhubdy,arxiv.org,http://arxiv.org/abs/1607.08583,0,1,1,0,Darknet and Deepnet Mining for Proactive Cybersecurity Threat Intelligence,,false,4wezlz,,0,,false,1473021995,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wezlz/darknet_and_deepnet_mining_for_proactive/,t3_4wezlz,,false,,
1470993421,MachineLearning,jverm,nips.cc,https://nips.cc/Conferences/2016/Schedule,4,19,19,0,NIPS 2016 Workshops,,false,4xco0b,,0,,false,1473039217,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xco0b/nips_2016_workshops/,t3_4xco0b,,false,,
1470136698,MachineLearning,harrism,devblogs.nvidia.com,https://devblogs.nvidia.com/parallelforall/ai-cat-chaser-jetson-tx1-caffe/,1,10,10,0,Build an AI Cat Chaser with Jetson TX1 and Caffe,,false,4vrx5x,,0,,false,1473010033,false,http://a.thumbs.redditmedia.com/f740xQSplsrdQdzR8XBNQeM6iJJ8fm6AF7GanAEtdb0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vrx5x/build_an_ai_cat_chaser_with_jetson_tx1_and_caffe/,t3_4vrx5x,,false,,
1471611693,MachineLearning,familylaundromatcm,familylaundromat.com,http://familylaundromat.com/,0,1,1,0,Family Laundry in West Chester,,false,4yis3a,,0,,false,1473060583,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yis3a/family_laundry_in_west_chester/,t3_4yis3a,,false,,
1470144646,MachineLearning,n00bto1337,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vsflz/why_isnt_bayesian_methods_popular_in_building/,7,6,6,0,Why isn't Bayesian methods popular in building recommendation systems?,,false,4vsflz,,0,,false,1473010295,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vsflz/why_isnt_bayesian_methods_popular_in_building/,t3_4vsflz,,false,,
1472045206,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1608.05137,4,11,11,0,[1608.05137] IM2CAD,,false,4zck0v,,0,,false,1473075728,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4zck0v/160805137_im2cad/,t3_4zck0v,,false,Research,
1471301713,MachineLearning,brockl33,arxiv.org,http://arxiv.org/abs/1602.01321,27,21,21,0,"[1602.01321] A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks",,false,4xwc8t,,0,,false,1473049235,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xwc8t/160201321_a_continuum_among_logarithmic_linear/,t3_4xwc8t,,false,,
1471636333,MachineLearning,yourbasicgeek,pcmag.com,http://www.pcmag.com/article/346018/your-new-cso-might-be-a-learning-computer-that-loves-cats,0,2,2,0,"Your New CSO Might Be a Learning Computer That Loves Cats: ""The same machine learning technologies and predictive analytic algorithms that give you useful book recommendations and power your most advanced self-serve BI and data-viz tools are being incorporated into IT security tools.""",,false,4ykwdu,,0,,false,1473061668,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ykwdu/your_new_cso_might_be_a_learning_computer_that/,t3_4ykwdu,,false,,
1470419116,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wbmry/why_not_xgboost/,13,5,5,0,Why not XGBoost?,[deleted],false,4wbmry,,0,,false,1473020271,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wbmry/why_not_xgboost/,t3_4wbmry,,false,,
1470835509,MachineLearning,thesanefreak,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x23hi/suggestions_for_final_year_project/,0,0,0,0,Suggestions for final year project,[removed],false,4x23hi,,0,,false,1473033791,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x23hi/suggestions_for_final_year_project/,t3_4x23hi,,false,,
1471453135,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y6qu5/using_tanh_activation_functions_in_the_first/,0,1,1,0,Using tanh activation functions in the first hidden layer and ReLUs in subsequent layers,[removed],false,4y6qu5,,0,,false,1473054497,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y6qu5/using_tanh_activation_functions_in_the_first/,t3_4y6qu5,,false,,
1470878041,MachineLearning,hongloumeng,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x5j09/what_role_does_causal_inference_play_in_machine/,23,18,18,0,What role does causal inference play in machine learning?,"Elias Bareinboim had two papers (one at NIPS) recently that put bandits and Markov decision processes in the context of causal inference ([1](http://ftp.cs.ucla.edu/pub/stat_ser/r460.pdf), [2](https://www.cs.purdue.edu/homes/eb/mdp-causal.pdf)).  This seems to connect causal inference to ML tasks such as active learning, online learning, and reinforcement learning.  Similarly, Jin Tian [connected causal inference to curriculum learning](http://web.cs.iastate.edu/~jtian/papers/acml-15.pdf).  Before this I imagined causal inference and machine learning as separate domains that share are a link through statistics and Bayesian networks as an intermediary -- like second degree friends on a social network.  Is this the beginning of a more direct and intimate relationship?  What is the big picture for the role of causal inference in machine learning?",false,4x5j09,,0,,false,1473035554,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4x5j09/what_role_does_causal_inference_play_in_machine/,t3_4x5j09,,false,Discusssion,
1470284211,MachineLearning,godspeed_china,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w2qj5/machine_learning_based_abstract_impact_factor/,1,0,0,0,machine learning based abstract impact factor evaluation,"www.drwang.top  
Hi All:  
I developed a web service to evaluate your abstracts based on its content via machine learning. My benchmark shows the square root of predicted IF and the square root of real IF have a correlation coefficient of 0.66.  
Ideally you can evaluate your abstract many time (it's fast ) and do manual ""gradient decent"" to optimize your abstract to maximize its potential impact factor and get published on good journal :-)  
Current trainning dataset is base on Jan 2011 to June 2016' PubMed abstracts, thus any published abstract outside the range can be used as testing set. Evaluate it by yourself.",false,4w2qj5,,0,,false,1473015709,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w2qj5/machine_learning_based_abstract_impact_factor/,t3_4w2qj5,,false,,
1472531303,MachineLearning,ml_newcomer,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/509bjz/open_source_frontend_tools_to_quickly_inspect/,1,1,1,0,Open source front-end tools to quickly inspect image classification results ?,"Hi, I'm looking for tools for displaying image, its correct labels, and its predicted labels nicely so that people can inspect the classification result quickly. E.g. something like what Karpathy has here: http://karpathy.github.io/assets/ilsvrc2.png

Did he open sourced this? or is there something similar others have created before?",false,509bjz,,0,,false,1473092666,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/509bjz/open_source_frontend_tools_to_quickly_inspect/,t3_509bjz,,false,,
1470980695,MachineLearning,fuckinghelldad,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xc2po/what_are_some_publications_explaining_how_neural/,9,6,6,0,What are some publications explaining how neural networks generalise?,I took a look and couldn't find much except for head scratching and conjectures.,false,4xc2po,,0,,false,1473038915,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xc2po/what_are_some_publications_explaining_how_neural/,t3_4xc2po,,false,,
1471001062,MachineLearning,__bee,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xd1b0/question_what_are_the_best_academic_conferences/,4,0,0,0,[Question] What are the best academic conferences in Machine Learning/Data Science/AI ?,"I am trying to collect a list of conferences that PhD students should follow while tracking the most advanced techniques in Machine Learning/Data Science/AI (Top tier conferences). 

Machine Learning  &amp; AI, Computer Vision, Text Mining and Social Media Analytics:
- NIPS
- ICLR
- IJCNN
- ICCV
- KDD
- ICDM 


What else, in case u have this list, please share it with us ?",false,4xd1b0,,0,,false,1473039405,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xd1b0/question_what_are_the_best_academic_conferences/,t3_4xd1b0,,false,,
1471621033,MachineLearning,thatindiandude78,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yjlbb/top_machine_learning_researchers_in_amazon/,14,3,3,0,Top machine learning researchers in Amazon,"Who are the top machine learning experts currently working at Amazon? After hearing about Alex Smola joining Amazon, I was curious to know what other top researchers work there. ",false,4yjlbb,,0,,false,1473060997,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yjlbb/top_machine_learning_researchers_in_amazon/,t3_4yjlbb,,false,,
1471200022,MachineLearning,satsatsat,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xpepz/neuralnets_for_count_data/,5,3,3,0,NeuralNets for Count Data?,"In the Statistics community, count data often receives special treatment (poisson, negative-binomial models etc.). 

Is there any special literature OR guidance available on using (Deep) Neural Networks for count-data? ",false,4xpepz,,0,,false,1473045707,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xpepz/neuralnets_for_count_data/,t3_4xpepz,,false,,
1472156074,MachineLearning,royalnorton,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zklom/clustering_application_sales_territory_definition/,0,0,0,0,Clustering application: sales territory definition.,"I am new to machine learning and investigating the potential application of clustering to automatically define sales territories for a company. My goal is to take m accounts and divide them into n territories based on their geographical distribution and sales potential. The challenge is to ensure that the territories are evenly distributed in terms of total sales potential with each cluster also being (relatively) geographically local. 

I have been testing k-means clustering which does a great job of clustering geographically. However, it is proving to be counterproductive when I include sales potential as an input since k-means will group objects that are similar. The resulting output tends to cluster accounts with similar potential sales value which then creates geographically scattered clusters. I have tried normalizing the sales potential values to reduce its effect but it does not help much. 

Is there a good way to break down the problem to achieve my goal? I am thinking of doing high level geographical clustering followed buy low level geographical clustering and then regrouping certain members of low level clusters to achieve an equitable distribution of sales potential but I'm not sure this is the most effective or efficient approach.

Finally, I'm not sure if I am asking the right questions. Is this a known type of optimization problem?

Thanks for your time!",false,4zklom,,0,,false,1473079834,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zklom/clustering_application_sales_territory_definition/,t3_4zklom,,false,,
1470089814,MachineLearning,Pieranha,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vp4st/best_practices_when_using_a_linux_server_for/,16,14,14,0,Best practices when using a Linux server for machine learning,"I'm managing my own Linux server that I use for machine learning experiments. Being new to Linux, I've found that not all the default settings were optimal when doing machine learning. For instance, I've experienced that the default settings provided by the university IT department made the automatic updates kill my Python scripts to reboot the server. This is fairly annoying when the server was in the midst of finding the optimal border of an SVM that takes several days to train.

I have 3 questions that I hope you can answer to make life easier for me and anyone else trying to manage a server for ML experiments:

1. What are some settings that you would always change to make the server perform better when doing ML and not cause any issues such as sudden rebooting?

2. How would you continuously check that your server is doing well and performing at its best?

3. Do you have any other advice for people new to Linux that are trying to manage their own server?",false,4vp4st,,0,,false,1473008575,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vp4st/best_practices_when_using_a_linux_server_for/,t3_4vp4st,,false,,
1471239263,MachineLearning,brilliant_ideas,arxiv.org,https://arxiv.org/ftp/arxiv/papers/1608/1608.03282.pdf,25,124,124,0,Choice of Instagram filters is a more accurate indicator of mental health than the avg. doctor's diagnosis,,false,4xs5mg,,0,,false,1473047113,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xs5mg/choice_of_instagram_filters_is_a_more_accurate/,t3_4xs5mg,,false,,
1470112451,MachineLearning,dccpt,blog.dominodatalab.com,https://blog.dominodatalab.com/video-model-based-machine-learning-webcast/,0,3,3,0,[Video] Model-based Machine Learning and Probabilistic Programming in RStan,,false,4vqqz6,,0,,false,1473009422,false,http://b.thumbs.redditmedia.com/Bx498tzg4tONorAQWRdZvNd-Bcz9qCzQukSfuI_QeYE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vqqz6/video_modelbased_machine_learning_and/,t3_4vqqz6,,false,,
1470811593,MachineLearning,tunggont,arxiv.org,http://arxiv.org/abs/1602.04938,0,16,16,0,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier",,false,4x0vdg,,0,,false,1473033173,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4x0vdg/why_should_i_trust_you_explaining_the_predictions/,t3_4x0vdg,,false,Research,
1472545524,MachineLearning,evc123,youtube.com,https://www.youtube.com/watch?v=5MdSE-N0bxs,16,58,58,0,Max Tegmark explains (via physics) why deep learning works so well,,false,50a2x0,,0,,false,1473093052,false,http://b.thumbs.redditmedia.com/vqAW80WqfkGfolLboCfsPcBr_rg6f62438mrvHD2iac.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50a2x0/max_tegmark_explains_via_physics_why_deep/,t3_50a2x0,,false,,
1471445402,MachineLearning,greymatter-analytics,wired.co.uk,http://www.wired.co.uk/article/moores-law-ending-good-ai,0,1,1,0,The death of Moore's Law could give birth to more human-like machines,,false,4y5zuo,,0,,false,1473054118,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y5zuo/the_death_of_moores_law_could_give_birth_to_more/,t3_4y5zuo,,false,,
1471884079,MachineLearning,OpenDataSciCon,opendatascience.com,https://www.opendatascience.com/blog/compare-job-skills-with-machine-learning/,0,0,0,0,"Compare Job Skills with Machine Learning By NYC Data Science Academy Contributed by Brett Amdur, Christopher Redino and Amy (Yujing) Ma.",,false,4z1f5i,,0,,false,1473070067,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z1f5i/compare_job_skills_with_machine_learning_by_nyc/,t3_4z1f5i,,false,,
1471545597,MachineLearning,shagunsodhani,ai.withthebest.com,http://ai.withthebest.com/,6,13,13,0,AI Online Conference For Developers,,false,4ydyo1,,0,,false,1473058156,false,http://b.thumbs.redditmedia.com/HV3eI6GM0v0jLMFpDfB5mylgLsAdHyy6EqAr9L233pU.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ydyo1/ai_online_conference_for_developers/,t3_4ydyo1,,false,,
1471562645,MachineLearning,coffeecoffeecoffeee,github.com,https://github.com/datamade/dedupe,0,3,3,0,GitHub - datamade/dedupe: A python library for accurate and scaleable data deduplication and entity-resolution.,,false,4yfi7e,,0,,false,1473058932,false,http://b.thumbs.redditmedia.com/FC6nZy6FYH6z2mUFMughFDWM8I7CIKiqQWGDz_GUXBM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yfi7e/github_datamadededupe_a_python_library_for/,t3_4yfi7e,,false,,
1470804186,MachineLearning,[deleted],sumve.com,http://sumve.com/ai-chatbots/relationships/relationship-bots.html,0,2,2,0,Machine Learning Relationship Bots,[deleted],false,4x0hl1,,0,,false,1473032980,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0hl1/machine_learning_relationship_bots/,t3_4x0hl1,,false,,
1470309500,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w41by/network_traffic_classification_using_machine/,3,1,1,0,Network Traffic Classification Using Machine Learning,[deleted],false,4w41by,,0,,false,1473016373,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w41by/network_traffic_classification_using_machine/,t3_4w41by,,false,,
1471544042,MachineLearning,linus_rules,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ydtay/problems_with_nvidia_gpu_also_posted_to/,3,1,1,0,Problems with NVIDIA GPU (also posted to /r/linuxadmin and /r/techsupport),[removed],false,4ydtay,,0,,false,1473058081,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4ydtay/problems_with_nvidia_gpu_also_posted_to/,t3_4ydtay,,false,,
1470096831,MachineLearning,datasciguy-aaay,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vpo7z/1400_gets_you_one_of_the_following_1_x_gtx1080_or/,5,0,0,0,"$1400 gets you one of the following: 1 x GTX1080, or 2 x GTX1070, or 3 x GTX1060. Want to actually test DNN and find the fastest?","I have $1400 for GPUs and want to buy the most DNN performance for this money. 

Heat (energy) is not a concern, and neither is anything else a concern other than what I said.

Speculation is fun especially with beer but I'm not in that business and won't listen to those in it.

GPU choice is strictly limited to the hardware I listed.

Do I have any takers?  We could agree on a DNN benchmark and anyone with one of those configs could report results so that nobody has to buy all 3 configs.",false,4vpo7z,,0,,false,1473008856,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vpo7z/1400_gets_you_one_of_the_following_1_x_gtx1080_or/,t3_4vpo7z,,false,,
1472607159,MachineLearning,liubanghoudai24,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/,46,4,4,0,"Which one should I choose: Keras, TensorLayer, TFLearn ?","Is there any detailed review or comparison for these three deep learning libraries?

Keras: https://keras.io/
https://github.com/fchollet/keras

TensorLayer: http://tensorlayer.readthedocs.io/en/latest/index.html
https://github.com/zsdonghao/tensorlayer

TFLearn: http://tflearn.org/
https://github.com/tflearn/tflearn",false,50eokb,,0,,false,1473095441,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/,t3_50eokb,,false,,
1470644310,MachineLearning,stua8992,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wp44v/isolating_different_people_in_noisy_audio/,11,0,0,0,Isolating different people in noisy audio,"Hi all,

In one of Andrew Ng's early ML lectures on youtube he mentioned and showed an example of a system that was able to take in audio of two people talking over eachother and extract the two separate audio tracks. I believe he said that the algorithm was unsupervised, but I might be wrong.

Does anyone know what this area of research is broadly called, any papers specific to the work he talked about, or any more recent work on this problem? 

Thanks very much",false,4wp44v,,0,,false,1473027196,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wp44v/isolating_different_people_in_noisy_audio/,t3_4wp44v,,false,,
1470916600,MachineLearning,cathalgarvey,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x7m1p/any_guides_on_generating_adversarial_images/,2,0,0,0,Any Guides on Generating Adversarial Images?,"Hey folks,

I'm fascinated by the potential for adversarial image generation for use in privacy preservation. For example, when uploading images of myself or others to a site, it'd be really cool to be able to make modifications un-noticeable to most humans, but blinding to a facial recognition algorithm.

I know there is a lot of research into this, including how to ""beat"" it, but I'm surprised not to be seeing tools to *assist* in generating these images.

Has anyone got any useful pointers towards guides or tools on adversarial image generation? ""The Idiot's Guide to Making CNNs Think You're a Panda""?",false,4x7m1p,,0,,false,1473036610,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x7m1p/any_guides_on_generating_adversarial_images/,t3_4x7m1p,,false,,
1470655175,MachineLearning,Rikerslash,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wpn9o/question_regarding_understanding_of_neural/,5,0,0,0,Question regarding understanding of Neural Networks,"Let us assume we have have a MLP with dimensions (5, 4, 3).
We have 100000 inputs, where the inputdata is the same all the time. The output is (1, 0, 0) in 70000 of these. (0, 1, 0) in 20000 of these and (0, 0, 1) in the rest.

After I learned the input in random order what output will I get by the Neural Net, if I would input the same dataset as the one learned.

a) Something close to (1, 0, 0).

b) Something close to (0.7, 0.2, 0.1)

c) Something else

If the answer is not b) how would I try to achieve the result b), which resembles the probabilities, which you see in the testdata.",false,4wpn9o,,0,,false,1473027463,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wpn9o/question_regarding_understanding_of_neural/,t3_4wpn9o,,false,,
1471879602,MachineLearning,newwave2k,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z11re/should_i_publish_my_paper_on_arxiv_before_the/,12,7,7,0,Should I publish my paper on ArXiv before the acceptance notification of a blind-review conference?,"I do not want to violate the essence of blind reviewing but I also want to claim my idea since I am not sure about the chance of the paper... 

Sorry if this is off-topic.",false,4z11re,,0,,false,1473069874,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4z11re/should_i_publish_my_paper_on_arxiv_before_the/,t3_4z11re,,false,Discusssion,
1471814611,MachineLearning,silvaring,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yx735/when_will_revolutionary_ai_be_used_in_video_games/,14,0,0,0,When will revolutionary AI be used in video games?,"By revolutionary I mean a massive leap from what we have had before e.g in the movie 'Her' there's a scene with what I assume is an AI buddy that is capable of actually exploring the game world with the player, responding to questions and even being a jerk to another AI assistant. Compare that to the keyboard text parser in 'Facade' or simple voice recognition commands in Kinect games and its a fundamentally more realistic experience. From what I've researched we are seeing simple game applications of voice recognition (like some games that use Amazons Alexa) and a few months back Google revealed how they had been able to achieve like 85% offline word recognition on a Nexus 5 (http://arxiv.org/pdf/1603.03185.pdf). What is keeping this tech from rolling out in high end games now though (which are running on PCs with much much more power than the Nexus 5) and what massive breakthroughs / rollouts can we expect that will usher in the era of real time AI assistants in games? Bonus question - How scalable is this tech going to be if you wanted to have like five enemies and one ally in the same play area interacting at the same time.   

Thanks for anyone who has insight and took the time to answer.",false,4yx735,,0,,false,1473067922,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yx735/when_will_revolutionary_ai_be_used_in_video_games/,t3_4yx735,,false,,
1472050512,MachineLearning,cvikasreddy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/,13,35,35,0,Machine Learning - WAYR (What Are You Reading) - Week 6,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.  

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  

[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)  
[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)  
[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)  
[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/) 
[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/) 
 
Besides that, there are no rules, have fun.",false,4zcyvk,,0,,true,1473075936,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/,t3_4zcyvk,,false,,
1471201619,MachineLearning,79c4a06fbba64629867a,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xpjb6/how_do_fully_convolutional_networks_fare_against/,26,2,2,0,How do fully convolutional networks fare against regular convolutional networks?,"From what I understand (I could be wrong), a normal CNN ends with fully connected layers before the output layer and, a fully convolutional network is made of convolutional layers end-to-end.

I haven't figured out which configuration is best for what kind of tasks. 

**Are there any good papers on fully convolutional networks?**",false,4xpjb6,,0,,false,1473045771,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xpjb6/how_do_fully_convolutional_networks_fare_against/,t3_4xpjb6,,false,,
1471021711,MachineLearning,qwertz_guy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xelce/when_does_the_deep_learning_book_bengio/,9,29,29,0,"When does the Deep Learning Book (Bengio, Goodfellow, Courville) release in print?",Is anyone involved in the process or knows people who are and can tell when it is to be expected that the deep learning book (http://www.deeplearningbook.org/) is buyable as print edition?,false,4xelce,,0,,false,1473040208,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xelce/when_does_the_deep_learning_book_bengio/,t3_4xelce,,false,,
1472418026,MachineLearning,BinaryAlgorithm,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/501e21/agents_run_with_virtual_machines_instruction_set/,0,4,4,0,Agents run with Virtual Machines - instruction set design,"I've tried a few experiments with single byte command structure and a simple VM based on this article: http://www.primaryobjects.com/2015/01/05/self-programming-artificial-intelligence-learns-to-use-functions/ (BrainPlus). It was very malleable to insertions, deletions, and changes. I am considering expanding the instruction set to two byte instructions but am not sure the best way to do that; for example if one byte value is the ""add"" command, but the operands are implied by registers or pointers, it tends to work better, while having a destination pointer byte after the add command is less likely to work with relevant data to the algorithm. In real assembly language, data values often follow the instruction code (for example, 1 byte for a load EAX and then loading the 4 following bytes to the register). This can work, as can having a different pointer to data values that increments. I am just not sure which structure will work best when randomizing, mutating, or crossing programs together. ",false,501e21,,0,,false,1473088501,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/501e21/agents_run_with_virtual_machines_instruction_set/,t3_501e21,,false,,
1470428288,MachineLearning,Greendogo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wcf2q/ml_and_gpus/,8,6,6,0,ML and GPUs,"Here's a pretty noobish question, but I was wondering, are all Machine Learning methods taking advantage of GPUs, or are Neural Networks the only one?

I've never seen it written about anywhere, but I was curious about things such as Random Forests or Support Vector Machines and any other ML category.",false,4wcf2q,,0,,false,1473020674,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wcf2q/ml_and_gpus/,t3_4wcf2q,,false,,
1470168335,MachineLearning,breakbotz,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vuin7/quick_explanation_of_gradient_boosting/,7,2,2,0,Quick explanation of Gradient Boosting?,"I understand the general concept of basic ensemble methods like random forests but don't get how Gradient Boosting provides a different approach.

I know GBM is a whole family of different algorithms but can anyone give me a quick rundown on the idea of ""Gradient Boosting"". I particularly don't understand the sequential addition of models to the ensemble.",false,4vuin7,,0,,false,1473011393,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vuin7/quick_explanation_of_gradient_boosting/,t3_4vuin7,,false,,
1470656434,MachineLearning,vladdione,vimeo.com,https://vimeo.com/170189199,2,16,16,0,"AI, Deep Learning, and Machine Learning: A Primer",,false,4wppql,,0,,false,1473027498,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wppql/ai_deep_learning_and_machine_learning_a_primer/,t3_4wppql,,false,,
1471088684,MachineLearning,dunkin1980,uk.businessinsider.com,http://uk.businessinsider.com/artificial-intelligence-in-medicine-is-promising-but-doubts-remain-2016-8,0,0,0,0,"Artificial intelligence in medicine is promising, but doubts remain",,false,4xiqvf,,0,,false,1473042319,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xiqvf/artificial_intelligence_in_medicine_is_promising/,t3_4xiqvf,,false,,
1470919531,MachineLearning,pmigdal,twitter.com,https://twitter.com/fchollet/status/753980621823750145,5,1,1,0,"The deep learning frameworks landscape (GitHub, July 2016)",,false,4x7sj3,,0,,false,1473036717,false,http://b.thumbs.redditmedia.com/Cn-NHBRMfxA20cvh2Ku4ejk8kWgWaDMd-qc4E0VoSEw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x7sj3/the_deep_learning_frameworks_landscape_github/,t3_4x7sj3,,false,,
1471611203,MachineLearning,numenta,youtube.com,https://www.youtube.com/watch?v=rHvjykCIrZM,0,2,2,0,Spatial Pooling: Learning,,false,4yiqmt,,0,,false,1473060563,false,http://b.thumbs.redditmedia.com/mcBCEDdmoMJhMt-x8sO2QPx-EUaYW4Df5QPaIEPdPTc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yiqmt/spatial_pooling_learning/,t3_4yiqmt,,false,,
1470504851,MachineLearning,sbc1906,twimlai.com,https://twimlai.com/apple-acquires-ml-startup-turi-darpa-hacker-bot-challenge-comma-ais-autonomous-driving-dataset-twiml-20160805/,0,0,0,0,"This Week in ML &amp; AI Podcast—Apple Acquires Turi, the DARPA Hacker-Bot Challenge, Comma.ai Open-Sources Driving Dataset + More",,false,4wgub5,,0,,false,1473022948,false,http://b.thumbs.redditmedia.com/_hzcOV4e0-BB_qWsGgZRkg6A3zza5Y3X08VIolLcqfI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wgub5/this_week_in_ml_ai_podcastapple_acquires_turi_the/,t3_4wgub5,,false,,
1471883626,MachineLearning,syllogism_,ines.io,https://ines.io/blog/how-front-end-can-improve-ai,0,5,5,0,How better front-end might yield better models (especially for NLP),,false,4z1dqq,,0,,false,1473070045,false,http://a.thumbs.redditmedia.com/0Zcbq1hypLgAnOQL7ktKFj6ssFHJXEUNyDWKUaAgPP8.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z1dqq/how_better_frontend_might_yield_better_models/,t3_4z1dqq,,false,,
1470155465,MachineLearning,hartator,hartator.github.io,https://hartator.github.io/animal-identifier/,29,9,9,0,Animal Identifier iOS App,,false,4vtc86,,0,,false,1473010763,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vtc86/animal_identifier_ios_app/,t3_4vtc86,,false,,
1472059850,MachineLearning,internatskymind,rubenfiszel.github.io,https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html,13,71,71,0,"Reinforcement Learning and DQN, learning to play from pixels",,false,4zdpwb,,0,,false,1473076318,false,http://b.thumbs.redditmedia.com/AerZTYn3XJPbNW1vVZXGmZUKlCsmjpsZU2wFu_ih-Oo.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zdpwb/reinforcement_learning_and_dqn_learning_to_play/,t3_4zdpwb,,false,,
1471983974,MachineLearning,adamnemecek,iai.uni-bonn.de,http://www.iai.uni-bonn.de/III/lehre/vorlesungen/IntelligentIS/WS15/,0,0,0,0,Intelligent Information Systems Class Slides,,false,4z8ko0,,0,,false,1473073704,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z8ko0/intelligent_information_systems_class_slides/,t3_4z8ko0,,false,,
1471988131,MachineLearning,smerity,arxiv.org,http://arxiv.org/abs/1608.04207,6,10,10,0,[1608.04207] Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks,,false,4z8x6d,,0,,false,1473073882,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z8x6d/160804207_finegrained_analysis_of_sentence/,t3_4z8x6d,,false,,
1472594410,MachineLearning,vanboxel,youtube.com,https://www.youtube.com/watch?v=xWuc7w9kco0,0,1,1,0,Building a self-driving car model with compressed data (livestream),,false,50dpf0,,0,,false,1473094940,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50dpf0/building_a_selfdriving_car_model_with_compressed/,t3_50dpf0,,false,,
1472409338,MachineLearning,[deleted],github.com,https://github.com/hbilen/dynamic-image-nets,1,3,3,0,This open source ML code forbids benchmarking.,[deleted],false,500pk4,,0,,false,1473088133,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/500pk4/this_open_source_ml_code_forbids_benchmarking/,t3_500pk4,,false,,
1471982678,MachineLearning,[deleted],pmirla.github.io,https://pmirla.github.io/2016/08/04/what-is-perceptron.html,0,0,0,0,"Though invented in 1960’s it is still used today, at places like Google. Why?",[deleted],false,4z8gpd,,0,,false,1473073647,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z8gpd/though_invented_in_1960s_it_is_still_used_today/,t3_4z8gpd,,false,,
1472345826,MachineLearning,potato277,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zx697/looking_for_scribe_notes_from_cmu_machine/,0,4,4,0,Looking for scribe notes from CMU Machine Learning (701),[removed],false,4zx697,,0,,false,1473086286,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zx697/looking_for_scribe_notes_from_cmu_machine/,t3_4zx697,,false,,
1470885700,MachineLearning,billyjayjohnson3416,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x6184/what_is_the_best_image_file_format_for_cnn_deep/,0,1,1,0,What is the best image file format for CNN deep learning?,[removed],false,4x6184,,0,,false,1473035812,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x6184/what_is_the_best_image_file_format_for_cnn_deep/,t3_4x6184,,false,,
1471938826,MachineLearning,mehdifarhangiann,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z5dd0/how_can_i_help_a_pizza_industry_using_machine/,0,1,1,0,How can I help a pizza industry using machine learning?,[removed],false,4z5dd0,,0,,false,1473072062,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z5dd0/how_can_i_help_a_pizza_industry_using_machine/,t3_4z5dd0,,false,,
1471958621,MachineLearning,xuanchien,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z6fg7/how_to_properly_train_and_verify_a_deep_learning/,0,1,1,0,How to properly train and verify a deep learning model on a big dataset?,[removed],false,4z6fg7,,0,,false,1473072610,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z6fg7/how_to_properly_train_and_verify_a_deep_learning/,t3_4z6fg7,,false,,
1472235774,MachineLearning,andalib_ansari,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zq2f5/big_data_made_easy_100_off_todayhurry_up/,0,1,1,0,Big Data Made Easy: 100% Off Today..Hurry up!!,[removed],false,4zq2f5,,0,,false,1473082631,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zq2f5/big_data_made_easy_100_off_todayhurry_up/,t3_4zq2f5,,false,,
1471567637,MachineLearning,BinaryAlgorithm,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yfwgt/what_are_the_correct_terms_for_describing_this/,4,0,0,0,What are the correct terms for describing this network?,"I am a hobbyist so I am not quite sure how to describe what I am working on in technical terms. My network has a series of nodes that are randomly connected to any other nodes initially. Some are mapped to inputs, and some to outputs. Each node is connected to itself and so has at least 1 input (and recurrence). Node state can be from 0 to symbolcount - 1. For display convenience I am using 26 (internal states 0-25) and mapping ""A-Z"", but it can work with arbitrary number of symbols (such as 676 ""AA-ZZ""). Each node has a weight matrix such that for each input symbol, a value is added to a running total for each potential output symbol. When all inputs are added, the symbol with the highest value becomes the next output. Each input connection is also multiplied by a connection weight. During evolution of the network nodes can be added/removed (if not mapped to I/O), connections added/removed, connection weights adjusted, or the input symbol to output symbol weight matrix adjusted. Values are typically bound to +/- 1.0.  
  
I would like to call this a sort of finite state machine, for the network can be in one of symbolcount ^ nodecount states generally and the transition rules are deterministic, but it doesn't match any definition I could find for FSM (and even if it is one, what subtype?). One major difference is that the order of inputs doesn't matter, input string ""ABCD"" might as well be ""DCBA"" because of how it is added to form the next state. I used this method to make it more robust with respect to network structural changes; it incrementally changes the result (by the weight of the added/removed node), rather than completely changing how all other inputs are mapped. I also found it easier to understand what is going on visually with the state history than with SNNs - I was able to observe cyclic and chaotic string patterns and transitions between them as well as changes from one cyclic pattern to another given an input perturbation (while being unaffected by other input symbols). It leads me to think that this network is capable of using working and long-term memory structures while being near a critical point where recognized inputs cause transitions more easily.  
  
I am sure this is unoriginal, but I have yet to find anything like it (and thus, I have no specific term to describe it). In case you need a visual: http://imgur.com/a/q04ki (top: output history of selected node, left: network; colored nodes are I/O/selected, right: agent moving along X/Y axis on grid, tiles with various ""food"" values among other values).",false,4yfwgt,,0,,false,1473059130,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yfwgt/what_are_the_correct_terms_for_describing_this/,t3_4yfwgt,,false,,
1471143743,MachineLearning,nickbomtempo,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xmeby/named_entity_recognition_in_portuguese_coursework/,1,0,0,0,Named Entity Recognition in Portuguese Coursework Conclusion Help,"Hello Guys,
Im studying in Brazil and decided to work with named entity recognition in my coursework conclusion, but I only find content using NLP libraries which do all the work for you and this is not the way I want to do my conclusion coursework.
I find that the best method to do named recognition, I think is Conditional Random Fields, but I cant understand the concept of this. I understood the concept of markov chains, but Conditional Random fields is so complex for my mind that I cant understand with the content I read.
Finaly, anyone know resources? or can do anything do help me? 
Im very frustrated with this.",false,4xmeby,,0,,false,1473044181,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xmeby/named_entity_recognition_in_portuguese_coursework/,t3_4xmeby,,false,,
1471972243,MachineLearning,damndata,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4z7k71/training_with_noisy_and_missing_labels/,2,1,1,0,Training with noisy and missing labels,"Suppose one has training data with very noisy labels. Also, the training data is heavily biased towards some classes more than others - so we have underrepresented classes and very noisy labels. What are possible ways to use this data to train a NN, say? Since the noise level is so high, standard procedures like using stratified sampling are not effective. 

I found this relevant paper: https://arxiv.org/pdf/1412.6596v3.pdf. I'm curious to hear about strategies to attack this problem and people's experiences with this setting. ",false,4z7k71,,0,,false,1473073186,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4z7k71/training_with_noisy_and_missing_labels/,t3_4z7k71,,false,,
1470809572,MachineLearning,mundher_alshabi,arxiv.org,https://arxiv.org/abs/1608.02833,0,1,1,0,Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator,,false,4x0rwr,,0,,false,1473033124,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x0rwr/facial_expression_recognition_using_a_hybrid/,t3_4x0rwr,,false,,
1471667696,MachineLearning,aaggarwall,github.com,https://github.com/ankitaggarwal011/PyCNN,5,14,14,0,Image Processing with Cellular Neural Networks in Python,,false,4yna1e,,0,,false,1473062879,false,http://b.thumbs.redditmedia.com/oPAawHcEYX0gKKPAnXyTX1Yt2qUSiqJYf1URGCyibjI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yna1e/image_processing_with_cellular_neural_networks_in/,t3_4yna1e,,false,,
1470897800,MachineLearning,mice39,github.com,https://github.com/aymericdamien/TopDeepLearning,0,1,1,0,Popular deep learning projects on Github,,false,4x6qah,,0,,false,1473036164,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x6qah/popular_deep_learning_projects_on_github/,t3_4x6qah,,false,,
1472106067,MachineLearning,ZoeyMadroxt8,cbs8.com,http://www.cbs8.com/story/32273251/30-day-performance-trial-of-renown-protocol-for-lyme-disease-at-no-cost,0,1,1,0,30-Day Performance Trial of Renown Protocol for Lyme Disease at No Cost - CBS News 8 - San Diego CA,,false,4zh5qw,,0,,false,1473078076,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zh5qw/30day_performance_trial_of_renown_protocol_for/,t3_4zh5qw,,false,,
1470120292,MachineLearning,mixmachinery,mixmachinery.com,http://www.mixmachinery.com/news/What-is-rubber-clay-mixer.html,1,1,1,0,What is rubber clay mixer in JCT?,,false,4vr6a2,,0,,false,1473009646,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vr6a2/what_is_rubber_clay_mixer_in_jct/,t3_4vr6a2,,false,,
1471268795,MachineLearning,sbc1906,twimlai.com,https://twimlai.com/another-huge-ml-acquisition-ai-olympics-win-free-ticket-oreilly-ai-conference/,1,12,12,0,"This Week in ML &amp; AI Podcast - 8/12: Intel buys Nervana, an Olympic AI bot + win a free ticket to O'Reilly's AI Conference",,false,4xtm5d,,0,,false,1473047849,false,http://b.thumbs.redditmedia.com/XWThKmq2ZvMzfcf886wYg0gKZpSpWmWJ2tH95-pTi5o.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xtm5d/this_week_in_ml_ai_podcast_812_intel_buys_nervana/,t3_4xtm5d,,false,,
1471430080,MachineLearning,T3Kaos,jbwebbdesign.com,https://jbwebbdesign.com/microsoft-ceos-10-laws-ai/,1,0,0,0,Microsoft CEO's 10 Laws of AI - JB Webb Design Online,,false,4y4uke,,0,,false,1473053543,false,http://b.thumbs.redditmedia.com/y7mkq_epD1ksNi6XFxCYafF3fW4KN7hkJVW4tN3qjSA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4y4uke/microsoft_ceos_10_laws_of_ai_jb_webb_design_online/,t3_4y4uke,,false,,
1471344832,MachineLearning,Lopelh,stats.stackexchange.com,http://stats.stackexchange.com/questions/230059/distance-preservation-measure-for-random-projection-evaluation,0,1,1,0,Distance preservation measure for Random Projection evaluation,,false,4xywet,,0,,false,1473050534,false,http://b.thumbs.redditmedia.com/B5uFOWxm5S5fw4QuRylkHK_QpJVMmGvCgISIPIuSq7w.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xywet/distance_preservation_measure_for_random/,t3_4xywet,,false,,
1470928732,MachineLearning,MaxTalanov,blog.deeprobotics.es,"http://blog.deeprobotics.es/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/08/07/deep-convolutional-q-learning/",0,7,7,0,"Deep Convolutional Q-Learning, an example",,false,4x8ibv,,0,,false,1473037081,false,http://b.thumbs.redditmedia.com/3aSlY-2vUK7phMsZNyZBM7Q9_okGp5cSzq3TUQMuczA.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4x8ibv/deep_convolutional_qlearning_an_example/,t3_4x8ibv,,false,,
1470332423,MachineLearning,compsens,matroid.com,http://matroid.com/blog/post/slides-from-scaledml-2016,0,11,11,0,All the presentation slides of the ScaledML 2016 conference,,false,4w5r1t,,0,,false,1473017259,false,http://b.thumbs.redditmedia.com/SLOOY17x0jA9CHlIDqI-4hV-O5Vt_CdQEBtXkL-ak0o.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w5r1t/all_the_presentation_slides_of_the_scaledml_2016/,t3_4w5r1t,,false,,
1472326276,MachineLearning,ivanzone,goo.gl,https://goo.gl/forms/IMbrmfZw2ZvbMsy82,10,0,0,0,"We built a crowdsourced computing resource for TensorFlow. Please fill out this survey to be one of first 100 Alpha Users. (First 100 signups get up-to $500 credits on the platform. First 1000 signups get $100 worth of credits. Signup by Sept 15th, 2016).",,false,4zvsrq,,0,,false,1473085581,false,http://b.thumbs.redditmedia.com/hTC18CKQ-xZqZvBN-NRatZsguivHmbddpFpZ5acrkRc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zvsrq/we_built_a_crowdsourced_computing_resource_for/,t3_4zvsrq,,false,,
1470434395,MachineLearning,perceptron01,arxiv.org,https://arxiv.org/abs/1608.01658v1,0,5,5,0,[1608.01658v1] Identifying Metastases in Sentinel Lymph Nodes with Deep Convolutional Neural Networks,,false,4wcwl3,,0,,false,1473020923,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wcwl3/160801658v1_identifying_metastases_in_sentinel/,t3_4wcwl3,,false,,
1471733283,MachineLearning,Staturecrane,github.com,https://github.com/staturecrane/denoising-convolutional-ae-torch,4,1,1,0,RBG Denoising Convolutional Autoencoder -- Torch,,false,4yroj6,,0,,false,1473065129,false,http://a.thumbs.redditmedia.com/qWCJwx0nMWeKwJHJ6XffqAvuzNz3xwKMZ7fkvsQ3_b4.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yroj6/rbg_denoising_convolutional_autoencoder_torch/,t3_4yroj6,,false,,
1472599609,MachineLearning,what_are_tensors,255bits.com,https://www.255bits.com/articles/hypergan-people/,11,30,30,0,Generating 256x256 faces with a GAN,,false,50e3z6,,0,,false,1473095146,false,http://b.thumbs.redditmedia.com/YT-yCmEEt9EOHyDLOaXKpGVsYX8YAnX4g7adcIBDtDM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50e3z6/generating_256x256_faces_with_a_gan/,t3_50e3z6,,false,,
1471308311,MachineLearning,[deleted],github.com,https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling,0,2,2,0,"Code+Paper for Voxel-Based VAE+GUI, along with Voxel-Based Inception-ResNets (50% relative improvement in 3D object classification SOTA)",[deleted],false,4xwtby,,0,,false,1473049476,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xwtby/codepaper_for_voxelbased_vaegui_along_with/,t3_4xwtby,,false,,
1470170148,MachineLearning,[deleted],github.com,https://github.com/KnHuq/Dynamic-Tensorflow-Tutorial,1,0,0,0,Tensorboard Tutorial,[deleted],false,4vuof5,,0,,false,1473011476,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vuof5/tensorboard_tutorial/,t3_4vuof5,,false,,
1470921053,MachineLearning,Flowx08,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x7w8i/autoencoder_with_mnist_are_these_results_normal/,0,1,1,0,Autoencoder with MNIST: are these results normal?,[removed],false,4x7w8i,,0,,false,1473036769,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x7w8i/autoencoder_with_mnist_are_these_results_normal/,t3_4x7w8i,,false,,
1472494449,MachineLearning,ibarea__redux,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/506f05/prediction_on_outofsample_data/,0,1,1,0,Prediction on out-of-sample data,[removed],false,506f05,,0,,false,1473091175,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/506f05/prediction_on_outofsample_data/,t3_506f05,,false,,
1471855469,MachineLearning,SYNA3STH3T1K,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4yzlyj/looking_for_a_specific_treelike_visualization_of/,5,15,15,0,Looking for a specific tree-like visualization of all the main Machine Learning algorithms,"A while back (maybe a year) I found an interactive visualization that had a structure similar to the tree of life expanding downwards (I'm pretty sure). This graph showed the evolution of all the popular algorithms of ML and AI which went all the way up to Deep Learning. You could click on each node/orb which would go into detail about each algorithm. Anyway, if you find it, thanks in advance. If not, thanks anyway!",false,4yzlyj,,0,,false,1473069134,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4yzlyj/looking_for_a_specific_treelike_visualization_of/,t3_4yzlyj,,false,,
1472637605,MachineLearning,muoro,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50gf8w/top_3_languages_to_build_machine_learning/,0,0,0,0,Top 3 Languages to build Machine Learning Algorithms.,"  
Here I come with yet another interesting topic to help budding data scientist, the article will help you to choose the best languages for developing machine learning algorithms. 
 
Machine learning is a process to build AI enabled algorithms with which machines are able to learn or produce codes automatically through analyzing the given data. 
 
Machine learning is the subset of Artificial Intelligence and again has the intersection with many fields including math and psychology. 
 
Now after giving a brief introduction let’s start with the tech part of the article:
 
After dong intensive research, I clustered these following languages, but please don’t be afraid to learn the other programing languages because to become a competent programmer and data science you must know a dozen of tools to stumble upon one that works the best in a particular situation, hence you can't restrict yourself to a language or two. Again to mention different jobs are best done in different languages.
 
1)   R Language:

 
This language was developed to as a modern version of S language developed in Bell labs, R language is combined with lexical scooping, which tends to provide the flexibility in producing statistical models. R is a really powerful language to start with machine learning, as there are many specified GNU packages available. One can surely choose to use R for creating powerful algorithms and plus the R studio has an easy statistical visualization of your algorithms. Though the language is widely used in academic research and gaining really well recognition in the industry use most recently. 
 
2)   Python 

 

Python language is one of the most flexible languages and can be used for various purposes. Python has gained huge popularity base of this. Python does contain special libraries for machine learning namely scipy and numpy which great for linear algebra and getting to know kernel methods of machine learning. The language is great to use when working with machine learning algorithms and has easy syntax relatively. For beginners, this is the best language to use and to start with.
3)   C language:

The mother of all language is definitely a great programming language to build your predictive algorithms. Developed at Bell Labs by Denise Ritchie (google researcher and creator of going). This language is not a cakewalk and should be only be considered when you have strong fundamentals of computer science and programming languages, however, once you are proficient in C language then there is nothing that can stop you developing your advance algorithms. One does not need Ph.D. but knows the computer programming concepts thoroughly.  You can build your own regressions analysis and time series simulation easily, which would create strong machine learning algorithms. 

 

In conclusion, I would like to add that there are many other languages that you can use after going through  the above ones. Once you get deeper you can explore the functional languages like Haskell, Erlang , Julia and Scala, these tools need you to have good knowledge of C first. As a beginner, you can start with Python and move to other languages once you get the command of that.  
To know more about machine learning please subscribe our newsletter http://www.muoro.io",false,50gf8w,,0,,false,1473096353,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50gf8w/top_3_languages_to_build_machine_learning/,t3_50gf8w,,false,,
1470918541,MachineLearning,JeezAnnoy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x7q2z/machine_learning_fingerprinting_notes_in_carnatic/,1,0,0,0,Machine Learning: Fingerprinting notes in Carnatic Music,"It's pretty difficult to process notes of rock-pop since they by default have a lot of noise and multitude frequencies. Carnatic music, in that aspect has a only a set number of frequencies repeated. Wouldn't it be easier to find process these notes (swaras), allow machines to learn these and find out the raaga of a song?",false,4x7q2z,,0,,false,1473036667,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x7q2z/machine_learning_fingerprinting_notes_in_carnatic/,t3_4x7q2z,,false,,
1471436313,MachineLearning,shiva81,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y58g4/please_suggest_project_ideas_for_under_graduate/,3,0,0,0,Please suggest project ideas for under graduate students in Machine Learning,I recently took ML class by Andrew Ng. I want to apply ML principles in realtime websites. Primarily had an idea to build a recommender system. What are the technologies should I learn to code it. I am a beginner into Machine Learning.,false,4y58g4,,0,,false,1473053736,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y58g4/please_suggest_project_ideas_for_under_graduate/,t3_4y58g4,,false,,
1471005663,MachineLearning,kneelb4darth,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xdbbi/is_someone_using_mlpack_c_library_how_good_is_it/,0,3,3,0,Is someone using MLPack c++ library ? How good is it?,"Hi, I have been learning about ML for a while but initially I started out with python based libraries like scikit, theano, tensorflow etc. I want to start using C++ from now on. The most important reason is that I am most comfortable with C++, and other than for ML, most of the coding I've ever done for almost anything has been using C++. I just want to know is if someone ever used MLPack in their project and how does it perform against other more well known tools. Also if you know other tools in C++ for ML, please suggest them . (PS: I know about dlib, so except that one)",false,4xdbbi,,0,,false,1473039548,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xdbbi/is_someone_using_mlpack_c_library_how_good_is_it/,t3_4xdbbi,,false,,
1470237923,MachineLearning,commaai,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vz1ie/commaai_for_the_people_to_experiment_with_too/,28,79,79,0,comma.ai for the people to experiment with too,"Hello, we are comma.ai

Today, we are releasing some stuff for you. Open stuff. Because we like openness. It's better than closedness. Even been stuck in an elevator? And openness makes the Machine Learning community great.

7.25 hours of driving data. From this, you should be able to replicate our initial Bloomberg experiments. Why you say? Because we know we will win. And if you do amazing things with this data, you can join the winning team and do more amazing things with even more data. We love data.

Also, our departing intern Eder Santana has a paper for you. Research. With the corresponding source code. At the real forefront of AI. Bigger than self-driving cars. We publish.

* http://research.comma.ai/
* https://github.com/commaai/research/blob/master/paper/commalds.pdf
* https://github.com/commaai/research

Enjoy,
George

PS: Zoox, Tesla, Google, and Cruise, we are waiting for your Open releases.
",false,4vz1ie,,1,,false,1473013766,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vz1ie/commaai_for_the_people_to_experiment_with_too/,t3_4vz1ie,,false,,
1471204477,MachineLearning,Sig_Luna,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xprgt/what_happened_to_andrew_ngs_machine_learning/,14,50,50,0,What happened to Andrew Ng's Machine Learning Yearning?,"As many of you might know, Andrew Ng announced a book ""Machine Learning Yearning"" which he would write over the summer. You could subscribe to a mailing list on http://www.mlyearning.org/ and would get every chapter as soon as he would've written it. The link was posted on this subreddit quite a while ago, I even remembered that someone posted a proof that this was in fact Andrew Ng's project.
I haven't got a single chapter to this day, even though (at least my ;( ) summer break is over now. Does anyone know what happened/happens with it?",false,4xprgt,,0,,false,1473045885,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xprgt/what_happened_to_andrew_ngs_machine_learning/,t3_4xprgt,,false,,
1471218357,MachineLearning,adagrad,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xqs1r/rate_of_convergence_of_the_gan_estimator/,5,2,2,0,Rate of convergence of the GAN estimator,"The original GAN paper showed if we have a perfect discriminator, then p_g is a consistent estimator for  p_data since the Jensen-Shannon divergence between p_g and p_data is minimized (and since the square root of JSD is a metric, as JSD(p_g || p_data) -&gt; 0 then p_g -&gt; p_data in probability, is that right?)

Are there any guarantees/bounds on the rate of decrease of JSD(p_g || p_data) under a perfect discriminator D*?",false,4xqs1r,,0,,false,1473046411,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xqs1r/rate_of_convergence_of_the_gan_estimator/,t3_4xqs1r,,false,,
1470424554,MachineLearning,FutureBayStreeter,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wc3q4/for_those_who_have_applied_to_data_science/,19,6,6,0,"For those who have applied to data science graduate programs, what was the process like?","This is coming from a very green undergrad student. I'm very interested in what data science masters programs (e.g. Msc in statistics with a specialization in data science at Stanford) look for in applicants. 

Is a quantitative bachelor's degree required? Or can analytics work experience make up for a less quantitative undergrad?

I would also like to know more about the process. I know for business schools, there is the GMAT and lots of essay writing. Are there equivalents tests and essays that data science program admissions look for? Or do they place a far greater value on the projects you have in your portfolio?

I would appreciate any insights anyone can provide.",false,4wc3q4,,0,,false,1473020511,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wc3q4/for_those_who_have_applied_to_data_science/,t3_4wc3q4,,false,,
1470261705,MachineLearning,timburg,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4w15b1/pooling_the_discriminator_in_dcgans/,2,0,0,0,Pooling the discriminator in DCGANs?,"Has anyone got any experience with pooling in DCGANs? Would z space representations have better spatial invariance? ",false,4w15b1,,0,,false,1473014888,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4w15b1/pooling_the_discriminator_in_dcgans/,t3_4w15b1,,false,,
1470709769,MachineLearning,petersonjc,arxiv.org,http://arxiv.org/abs/1608.02164,0,1,1,0,[1608.02164] Adapting Deep Network Features to Capture Psychological Representations,,false,4wtv6n,,0,,false,1473029614,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wtv6n/160802164_adapting_deep_network_features_to/,t3_4wtv6n,,false,,
1470620899,MachineLearning,mainguyenmth,youtube.com,https://www.youtube.com/attribution_link?a=VhZ91X2ZI68&amp;u=%2Fwatch%3Fv%3DHJEYYaQ9Zd8%26feature%3Dshare,1,0,0,0,Máy làm viên hoàn mềm chuyên nghiệp nhất,,false,4wnvrr,,0,,false,1473026566,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4wnvrr/máy_làm_viên_hoàn_mềm_chuyên_nghiệp_nhất/,t3_4wnvrr,,false,,
1470535507,MachineLearning,fruitheart,correlatesofwar.org,http://www.correlatesofwar.org/data-sets/folder_listing,2,46,46,0,[Dataset] the 'Correlates of War' datasets,,false,4wiy6z,,0,,false,1473024037,false,http://b.thumbs.redditmedia.com/WxCvEmZhOzT72dJMNuVGOUZabeA_R9ILlIhaUaXLz8A.jpg,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wiy6z/dataset_the_correlates_of_war_datasets/,t3_4wiy6z,,false,Research,
1470824427,MachineLearning,Mgordon78,gforge.se,http://gforge.se/2016/08/deep-learning-with-torch-dataframe-a-gentle-introduction-to-torch/,0,3,3,0,The new torch-dataframe v1.5 with torchnet compatibility - a gentler introduction to Torch,,false,4x1gzm,,0,,false,1473033473,false,http://a.thumbs.redditmedia.com/LsAfhw4cXvb2p8jdR0FCPgjvXsluI4HAzGXcna03u34.jpg,t5_2r3gv,false,two,,false,false,,/r/MachineLearning/comments/4x1gzm/the_new_torchdataframe_v15_with_torchnet/,t3_4x1gzm,,false,News,
1471350699,MachineLearning,john_philip,youtube.com,https://www.youtube.com/watch?v=9XZ3ihE7OjM,7,44,44,0,Scikit Learn Hyperparameter Optimization Tutorial,,false,4xz8pw,,0,,false,1473050707,false,http://a.thumbs.redditmedia.com/pRSr5MTX2qjafOsaAKKdVJoqvSFXqqoo28Mac9xCDF0.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xz8pw/scikit_learn_hyperparameter_optimization_tutorial/,t3_4xz8pw,,false,,
1472139724,MachineLearning,bdamos,arxiv.org,http://arxiv.org/abs/1608.06884,4,10,10,0,[1608.06884] Towards Bayesian Deep Learning: A Framework and Some Existing Methods,,false,4zj4rq,,0,,false,1473079086,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4zj4rq/160806884_towards_bayesian_deep_learning_a/,t3_4zj4rq,,false,Research,
1472564751,MachineLearning,ufazal,annotatemydata.com,http://annotatemydata.com,2,0,0,0,"Hey Machine Learning researchers ! We are providing service for generating ground truth data for images and videos at affordable rates. Whether its Bounding Box, Pose Estimation or Object Classification. See more info at the URL.",,false,50b6sc,,0,,false,1473093616,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50b6sc/hey_machine_learning_researchers_we_are_providing/,t3_50b6sc,,false,,
1472143481,MachineLearning,ehrenbrav,ehrenbrav.com,http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/,50,180,180,0,Fork of Google DeepMind's Atari Code to Play Super Mario Bros.,,false,4zjgwv,,0,,false,1473079256,false,http://b.thumbs.redditmedia.com/vSofg2lPGgXKr_Eu4P3ltW8Ri3ZNHTI4iaeX9VgTiJI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zjgwv/fork_of_google_deepminds_atari_code_to_play_super/,t3_4zjgwv,,false,,
1470764858,MachineLearning,cesarsalgado,arxiv.org,http://arxiv.org/abs/1608.00318,0,21,21,0,[1608.00318] A Neural Knowledge Language Model,,false,4wxh7i,,0,,false,1473031454,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4wxh7i/160800318_a_neural_knowledge_language_model/,t3_4wxh7i,,false,Research,
1472581899,MachineLearning,aulloa,github.com,https://github.com/alvarouc/polyssifier,2,24,24,0,"Python package for exploring binary classification. It runs various classifiers and reports AUC, confusion matrices, predicted probabilities, and a ranking plot",,false,50cna6,,0,,false,1473094394,false,http://b.thumbs.redditmedia.com/209jKTIYgztrOtk-UhJHwVFlV6AoTnUkeCL50xOeGTw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/50cna6/python_package_for_exploring_binary/,t3_50cna6,,false,,
1470224676,MachineLearning,[deleted],ebay.fr,http://www.ebay.fr/itm/112048536690?ssPageName=STRK:MESELX:IT&amp;_trksid=p3984.m1555.l2649,0,0,0,0,Promotion on NVIDIA Jetson TX1 for Machine Learning Algorithms,[deleted],false,4vy2ir,,0,,false,1473013235,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vy2ir/promotion_on_nvidia_jetson_tx1_for_machine/,t3_4vy2ir,,false,,
1470827446,MachineLearning,sadathanwar17,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x1m9t/what_are_the_prerequisites_to_get_into_google_rd/,0,1,1,0,What are the pre-requisites to get into google R&amp;D after bachelors in engineering?,[removed],false,4x1m9t,,0,,false,1473033548,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x1m9t/what_are_the_prerequisites_to_get_into_google_rd/,t3_4x1m9t,,false,,
1470745896,MachineLearning,2016super,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wvxlz/keeping_your_washer_fresh_and_clean/,0,1,1,0,Keeping Your Washer Fresh and Clean,[removed],false,4wvxlz,,0,,false,1473030667,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wvxlz/keeping_your_washer_fresh_and_clean/,t3_4wvxlz,,false,,
1472565027,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/50b7ko/question_about_providing_hardware_resources_for/,0,1,1,0,Question about providing hardware resources for deep learning projects.,[removed],false,50b7ko,,0,,false,1473093630,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/50b7ko/question_about_providing_hardware_resources_for/,t3_50b7ko,,false,,
1471486823,MachineLearning,Brunomars12,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y9q6y/how_longdifficult_is_it_to_build_a_cdnn_for/,2,0,0,0,How long/difficult is it to build a CDNN for facial recognition today? Where are the places to go to find the talent?,"Hey ML!, 

I used to work for a deep learning startup, and we had an amazing data scientist who did an amazing job designing the technology. I joined the team a year into development and it was amazingly efficient to work with someone of such high class. We had a great time and the technology got acquired. 

Flash forward to now: I've been working on another app idea that involves facial recognition. I have some investors now, but they want to see ""Deep learning"" in the company before they commit. 

I've got a few questions. 

1) How difficult is it to build a CDNN today? With the company I was at it took them roughly two years to design the backend. Have things changed since then? Has research in the field decreased the time/cost that it takes to build these networks? We spent over 1 million, and the AWS charges/fees were around 40k a month. While the AWS fee haven't really gone down, that's not the issue. The real issue is the time. 

2) Where are the best places to go to find recruit this type of talent? I've done the craigslist/Angel list posts, but haven't had a ton of replies from people with experience with deep learning. I've done some searches with linkedin, but a lot of  people I find have already been picked up haha. 

Any replies are greatly appreciated! 

",false,4y9q6y,,0,,false,1473056012,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y9q6y/how_longdifficult_is_it_to_build_a_cdnn_for/,t3_4y9q6y,,false,,
1472367911,MachineLearning,krchia,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zye69/are_there_any_other_classes_that_make_use_of/,11,3,3,0,Are there any other classes that make use of Octave?,"I'm just about to finish Andrew's Ng machine learning course using octave, and i don't want the time i invested in learning octave to die off. I am of course looking to use octave in a ML context, and preferably a ML course that is a step up in difficulty over an introductory one.",false,4zye69,,0,,false,1473086914,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zye69/are_there_any_other_classes_that_make_use_of/,t3_4zye69,,false,,
1472456724,MachineLearning,forwninguy,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/503wm6/correlation_vs_causation/,8,1,1,0,Correlation vs Causation,"I've been reading a bunch about SVMs lately and I cam upon something strange in many of the papers I read or skimmed over. 

That is, people finding a function to describe the relation between some inputs and outputs, then calling said function a ""causality function""... or other such terms. e.g ""X, Y, Z have a strong impact on Z"".

Surely, I thought, this must be a mistake due to the author's mother-tongue not being English, however, I've seen this type of mistake, that is implying causality when a correlation is found, so many time, that I think there might be something I'm not understanding about this whole machine learning thing.

The way I see it, there correlation and causation stand as such:

Even if a relation can be established 100% of the time between event X and event Y (e.g. y =x*w), unless one can come up with a feasible explanation (e.g. one's that certain humans will find compelling on the ground of some scientific theory or, at least, philosophical trend, one that preferably can be generalized to other sets of data which share similarities with x and y).

Stating, for example, G = m*g, is a correlation, it can be observed but there is not necessarily any humane reasoning behind it.

Explaining that the earth (and any other object), exhibit a gravitational field that can be characterized by a constant (assuming the object is perfectly round), ""g"", which defines the acceleration of an object towards the center of mass of said object. Is a correlation, even though a poor one, ignoring many important aspects that related to gravitational acceleration.

In the same way, stating that, for a right angled triangle, ""the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides"". Is a correlation, it can be observed, but simply stating it, does not prove it.

There are then various proves for said theorem, that find a ""causality"" between said right angle and the relation. Some are fairly poor, relying on finite differences, others are more robust, but still assume that the sum of all angles in a triangle is 180 degrees and other yet may change the relationship by assuming the sum of the angles in a triangle is less than 180 degrees.

Even if the prefect sine wave has a very exact relation in report to time, defining it amplitude, we can't assume a certain point in time ""causes"" the sine wave to be as such, only that it is correlated with said amplitude.

It stands to reason, then, that a machine learning algorithm would never be able to find causality, only correlation. Since none of these algorithms are able to quite ""explain"" their reasoning. Indeed, correlation seem to be a very objective and provable thing, causation is a more philosophical matter, one I would have assumed is left to human and possibly the more ""intelligent"" machines of the future to debate upon.

Am I getting something wrong here ? Mixing up terms or not understanding how a ML algorithm works ? Can one ever state that an algorithm that is able to learn to predict outputs based on inputs is doing more than finding correlations.",false,503wm6,,0,,false,1473089823,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/503wm6/correlation_vs_causation/,t3_503wm6,,false,,
1471444168,MachineLearning,binary_zeitgeist,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4y5vqh/dgcans_for_image_segmentation/,1,0,0,0,DGCANs for image segmentation?,"How can DGCANs (Deep Convolutional Generative Adversarial Networks) [1] [2] be used in segmenting specific objects from an image in a totally unsupervised format ? For example segmenting tumors from brain MRI or tissues from CT Scans.

1 : https://arxiv.org/abs/1511.06434

2 : https://www.reddit.com/r/MachineLearning/comments/3tykrw/unsupervised_representation_learning_with_deep/",false,4y5vqh,,0,,false,1473054061,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4y5vqh/dgcans_for_image_segmentation/,t3_4y5vqh,,false,,
1470526786,MachineLearning,nlpkid,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wieb7/a_dumb_question/,10,0,0,0,A dumb question,"I understand that this is a dumb question, but I'm curious why this can't be done/hasn't been done. 

Deep learning/neural networks are already roughly modeled on the principles of the human brain. To get an even more accurate picture (especially for things like spiking neural networks) why can't we take a human brain (or a rat brain or other animal brain), strap a set of electrodes on, and acquire the signals from a variety of different tasks? The results would be the discrete spikes resulting at different layers of biological neural networks. We could use linear regression or other basic statistical methods to construct a basic rule for reproducing such spikes, and we would have a (roughly) accurate neural network potentially capable of human-level performance.

Sorry if this is a dumb/amateur question, but I'm genuinely curious.",false,4wieb7,,0,,false,1473023755,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4wieb7/a_dumb_question/,t3_4wieb7,,false,Discusssion,
1470096320,MachineLearning,amplifier_khan,analyticsvidhya.com,https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/,0,4,4,0,Bayesian Statistics explained to Beginners in Simple English,,false,4vpmt5,,0,,false,1473008835,false,http://b.thumbs.redditmedia.com/ksQjcM5VG3vcjBqgTAveSpBrbtqwk8_YtX-mBechbHw.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vpmt5/bayesian_statistics_explained_to_beginners_in/,t3_4vpmt5,,false,,
1471509501,MachineLearning,pmigdal,bookspace.co,http://www.bookspace.co/search/?query_book=The+Communist+Manifesto+%28Penguin+Classics%29&amp;plus=liberty&amp;minus=communism,5,33,33,0,"doc2vec analogy search for books (e.g. ""The Communist Manifesto"" - ""communism"" + ""liberty"")",,false,4yb5wq,,0,,false,1473056744,false,http://b.thumbs.redditmedia.com/uUJDjCT4gLjrZ6VOCc4Vgf84_wbEDXcihil_mUQ4CEI.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yb5wq/doc2vec_analogy_search_for_books_eg_the_communist/,t3_4yb5wq,,false,,
1471334647,MachineLearning,slagcrusher,slagcrusher.in,http://slagcrusher.in,1,1,1,0,"Slag Crusher Plant Manufacturers, Suppliers &amp; Exporters",,false,4xyeqa,,0,,false,1473050285,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xyeqa/slag_crusher_plant_manufacturers_suppliers/,t3_4xyeqa,,false,,
1471269366,MachineLearning,j_lyf,github.com,https://github.com/joanbruna/stat212b,13,0,0,0,"Is it just me, or is the notation in this course completely inscrutable?",,false,4xtnmi,,0,,false,1473047869,false,http://b.thumbs.redditmedia.com/WuhmqkXEyQpZ4pFtedXH8eQxovJNIVJ4TeTeGmVJvus.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xtnmi/is_it_just_me_or_is_the_notation_in_this_course/,t3_4xtnmi,,false,,
1471093332,MachineLearning,iceman_121,infinityplusultra.wordpress.com,https://infinityplusultra.wordpress.com/2016/08/13/ayn-rand-would-be-proud/,0,0,0,0,Any Atlas Shrugged fans out here?,,false,4xiz64,,0,,false,1473042438,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xiz64/any_atlas_shrugged_fans_out_here/,t3_4xiz64,,false,,
1471601835,MachineLearning,xavigiro,imatge-upc.github.io,http://imatge-upc.github.io/telecombcn-2016-dlcv/,3,93,93,0,"Deep Learning for Computer Vision Barcelona, UPC 2016 (slides available)",,false,4yi4gn,,1,,false,1473060251,false,http://b.thumbs.redditmedia.com/9hkgcBTlnN7AnF_NcRoZZuzmC68n56_Ju0aNFeU7E3w.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yi4gn/deep_learning_for_computer_vision_barcelona_upc/,t3_4yi4gn,,false,,
1471277819,MachineLearning,alxndrkalinin,blog.aylien.com,http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/,1,35,35,0,Lisbon Machine Learning Summer School Highlights,,false,4xuc8s,,0,,false,1473048215,false,http://b.thumbs.redditmedia.com/rMFKikdInQ_X7toZpKgsI8jfx3O9vU1CY8kPy4EB1nc.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xuc8s/lisbon_machine_learning_summer_school_highlights/,t3_4xuc8s,,false,,
1471441128,MachineLearning,ajmooch,github.com,https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling,0,10,10,0,"Code+Paper for Voxel-Based VAE+GUI, along with Voxel Inception-Resnets (50% relative improvement on 3D Classification State of the Art)",,false,4y5m57,,0,,false,1473053927,false,http://b.thumbs.redditmedia.com/_fk9FZ7cDcvSeshUwk6OB81SO49orGNNXGqPscFm0-k.jpg,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4y5m57/codepaper_for_voxelbased_vaegui_along_with_voxel/,t3_4y5m57,,false,Research,
1471884441,MachineLearning,sbc1906,twimlai.com,https://twimlai.com/twiml-talk-2-siraj-raval-build-confidence-ml-developer/,0,0,0,0,TWiML Talk #2 - Siraj Raval - How to Build Confidence as an ML Developer - This Week in Machine Learning &amp; AI,,false,4z1g9u,,0,,false,1473070083,false,http://b.thumbs.redditmedia.com/EqJ7kwBdAykpLMVA0CscC9pgVPloXAZg3j0rMZe6xkM.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z1g9u/twiml_talk_2_siraj_raval_how_to_build_confidence/,t3_4z1g9u,,false,,
1470947847,MachineLearning,kemaswill,kemaswill.com,http://www.kemaswill.com/uncategorized/from-matrix-factorization-to-factorization-machines/,1,45,45,0,"An introduction to Matrix Factorization, Factorization Machine and Field-aware Factorization Machine",,false,4xa5yu,,0,,false,1473037935,false,http://b.thumbs.redditmedia.com/787FmWhlPApG7y5hw8v1C8H5-YcOJZGrCRKlF1TwW1w.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xa5yu/an_introduction_to_matrix_factorization/,t3_4xa5yu,,false,,
1470620268,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wnucr/why_doesnt_scikit_learn_have_an_xmeans_clustering/,1,0,0,0,"Why doesn't sci-kit learn have an ""x-means"" clustering algorithm?",[deleted],false,4wnucr,,0,,false,1473026546,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wnucr/why_doesnt_scikit_learn_have_an_xmeans_clustering/,t3_4wnucr,,false,,
1471136326,MachineLearning,[deleted],self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xlxqc/forecasting_market_share/,0,0,0,0,Forecasting Market Share,[deleted],false,4xlxqc,,0,,false,1473043943,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xlxqc/forecasting_market_share/,t3_4xlxqc,,false,,
1470793222,MachineLearning,Betunink1947,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4wzruo/how_can_i_give_tips_to_my_neural_network/,0,1,1,0,"How can i give ""tips"" to my neural network?",[removed],false,4wzruo,,0,,false,1473032619,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4wzruo/how_can_i_give_tips_to_my_neural_network/,t3_4wzruo,,false,,
1471653655,MachineLearning,ill-logical,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4ymbu2/cudnn_have_you_seen_a_big_speedup_with_3x3/,7,1,1,0,cuDNN : have you seen a big speedup with 3x3 kernels in 5.2?,"[The cuDNN download page](https://developer.nvidia.com/cudnn) mentions a 2.7-fold speed up with 3x3 kernels, but it seems like they also upgrade their hardware for the comparison, and it's unclear if the speed-up is relative to v4 or 5.0. 

I can't upgrade my version now, and [the convnet benchmark page](https://github.com/soumith/convnet-benchmarks) is still using v4.

How much speed-up do you get in v5.2 and relative to what?",false,4ymbu2,,0,,false,1473062397,false,self,t5_2r3gv,false,one,,false,true,,/r/MachineLearning/comments/4ymbu2/cudnn_have_you_seen_a_big_speedup_with_3x3/,t3_4ymbu2,,false,Discusssion,
1472125028,MachineLearning,alok29,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4zi1fr/parsey_mcparseface_online_demo/,2,0,0,0,parsey mcparseface online demo,Is there any online demo available for google's parser parsey mcparseface. I guess google did not make it available bacause people will make fun of it,false,4zi1fr,,0,,false,1473078526,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4zi1fr/parsey_mcparseface_online_demo/,t3_4zi1fr,,false,,
1470867992,MachineLearning,curryeater259,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4x4svy/difference_between_implicit_and_explicit/,5,3,3,0,Difference between Implicit and Explicit minimization?,"What's up guys,

Noob question here, but I just wanted to figure out the difference between implicit and explicit minimization. So far, I just understand that implicit minimization is found by setting the derivative to zero, while explicit minimization is something like gradient descent where it's trial and error. Is what I said accurate, and is there something I'm missing? Thanks for the help guys. ",false,4x4svy,,0,,false,1473035187,false,self,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4x4svy/difference_between_implicit_and_explicit/,t3_4x4svy,,false,,
1470293389,MachineLearning,phunter_lau,dmlc.ml,http://dmlc.ml/mxnet/2016/08/03/mxnet-titanx-benchmark.html,10,14,14,0,"New Pascal Titan X benchmark (Inception, VGG, ResNet) with MXNet, wow, up to 1.6x speedup than the old Titan X",,false,4w38sy,,0,,false,1473015970,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4w38sy/new_pascal_titan_x_benchmark_inception_vgg_resnet/,t3_4w38sy,,false,,
1471345759,MachineLearning,mixmachinery,youtube.com,https://www.youtube.com/attribution_link?a=SrresjUx4Z8&amp;u=%2Fwatch%3Fv%3DikLV0P1b6pE%26feature%3Dshare,1,1,1,0,How about the jacketed pressure vessels platform in JCT?,,false,4xyy5m,,0,,false,1473050559,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xyy5m/how_about_the_jacketed_pressure_vessels_platform/,t3_4xyy5m,,false,,
1470750134,MachineLearning,_beast__,techcrunch.com,https://techcrunch.com/2016/08/08/machine-learning-and-molecular-tinder-may-change-the-game-for-oled-screens/,0,1,1,0,New research applies neural nets and tinder-like ML techniques in an attempt to find new blue OLED molecule.,,false,4ww8c6,,0,,false,1473030820,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4ww8c6/new_research_applies_neural_nets_and_tinderlike/,t3_4ww8c6,,false,,
1471891441,MachineLearning,jehan60188,dartmouth.edu,https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf,5,11,11,0,A really good introduction to Markov Chains,,false,4z22do,,0,,false,1473070396,false,default,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z22do/a_really_good_introduction_to_markov_chains/,t3_4z22do,,false,,
1471892274,MachineLearning,alxndrkalinin,blog.fastforwardlabs.com,http://blog.fastforwardlabs.com/post/149329060653/under-the-hood-of-the-variational-autoencoder-in,1,12,12,0,Under the Hood of the Variational Autoencoder (in Prose and Code) | Fast Forward Labs,,false,4z250m,,0,,false,1473070433,false,http://b.thumbs.redditmedia.com/gJ5C1SjhguULOzOywiWB1yKJ1QWz1e_7yVytf-3bWME.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4z250m/under_the_hood_of_the_variational_autoencoder_in/,t3_4z250m,,false,,
1470160223,MachineLearning,MartianTomato,r2rt.com,http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html,13,139,139,0,"Written Memories: Understanding, Deriving and Extending the LSTM",,false,4vtrn1,,0,,false,1473011001,false,http://b.thumbs.redditmedia.com/w4S9GRQCI8krUaLvoUSwVXpwZlB-TEPgA-TCCxtcKFE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4vtrn1/written_memories_understanding_deriving_and/,t3_4vtrn1,,false,,
1471031629,MachineLearning,senecaur,devblogs.nvidia.com,https://devblogs.nvidia.com/parallelforall/detectnet-deep-neural-network-object-detection-digits/,0,3,3,0,DetectNet: Deep Neural Network for Object Detection in DIGITS,,false,4xfhdf,,0,,false,1473040661,false,http://b.thumbs.redditmedia.com/UY9EGi5oDY1UfaQeIiijW3z-MUWZl2JJzkLAorHVkTE.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4xfhdf/detectnet_deep_neural_network_for_object/,t3_4xfhdf,,false,,
1471723057,MachineLearning,pavanmirla,pmirla.github.io,https://pmirla.github.io/2016/08/16/AI-Winter.html,7,0,0,0,What is AI Winter ? A period where the promise of AI theory failed to meet practical applications Financial support dried up Researchers lost interest,,false,4yquok,,0,,false,1473064708,false,http://b.thumbs.redditmedia.com/XkexWLUtYX6ScCJQHIRmV_tZ8sUY7uyeT6SEjX1aVgQ.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4yquok/what_is_ai_winter_a_period_where_the_promise_of/,t3_4yquok,,false,,
1472076470,MachineLearning,elisebreda,blog.yhat.com,http://blog.yhat.com/posts/logistic-regression-python-rodeo.html,0,1,1,0,Logistic Regression in Python Using Rodeo,,false,4zf4cd,,0,,false,1473077033,false,http://b.thumbs.redditmedia.com/s0ETcAW7TxcSTpp9QY_BfANVL5LD7B7ig2igLUlxobY.jpg,t5_2r3gv,false,,,false,false,,/r/MachineLearning/comments/4zf4cd/logistic_regression_in_python_using_rodeo/,t3_4zf4cd,,false,,
1472163532,MachineLearning,[deleted],ics.uci.edu,http://www.ics.uci.edu/~pjsadows/papers/LocalLearning2016.pdf,0,1,1,0,flair:research Local Learning Rules for Neural Networks,[deleted],false,4zl7vi,,0,,false,1473080150,false,default,t5_2r3gv,false,three,,false,false,,/r/MachineLearning/comments/4zl7vi/flairresearch_local_learning_rules_for_neural/,t3_4zl7vi,,false,Research,
1470450927,MachineLearning,mixmachinery,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4we04j/how_about_silicon_adhesives_filling_system/,1,1,1,0,How about silicon adhesives filling system?,[removed],false,4we04j,,0,,false,1473021491,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4we04j/how_about_silicon_adhesives_filling_system/,t3_4we04j,,false,,
1470218957,MachineLearning,redditusernamed,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4vxs35/ideas_and_thoughts_on_how_to_practice_machine/,0,1,1,0,Ideas and thoughts on how to practice machine learning,[removed],false,4vxs35,,0,,false,1473013078,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4vxs35/ideas_and_thoughts_on_how_to_practice_machine/,t3_4vxs35,,false,,
1470949613,MachineLearning,AnuragHalder,self.MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4xab27/understanding_the_range_in_the_feature_set/,0,1,1,0,Understanding The Range In The Feature Set - Coursera ML EX4,[removed],false,4xab27,,0,,false,1473038008,false,default,t5_2r3gv,false,,,false,true,,/r/MachineLearning/comments/4xab27/understanding_the_range_in_the_feature_set/,t3_4xab27,,false,,
